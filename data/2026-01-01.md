<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 18]
- [cs.CR](#cs.CR) [Total: 20]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models](https://arxiv.org/abs/2512.23850)
*Rahul Baxi*

Main category: cs.AI

TL;DR: DDFT 测试通过对模型在语义压缩和对抗性生成下维持事实准确性的能力进行评估，揭示了语言模型的元认知稳健性与设计规模、架构类型无关，而可能与训练方法和验证机制有关。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估方法只衡量模型在理想条件下的知识水平，无法评估其在实际情况下的稳健性。为此，提出了 DDFT 测试来评估模型在语义压缩和对抗性生成下的事实准确性。

Method: DDFT 测试引入了一种新的认知模型框架，包括语义系统和知识验证系统，并在多种模型和知识领域进行了大规模实验。

Result: 研究发现模型的元认知稳健性与参数数量和架构类型无关，而是与训练方法和验证机制有关。旗舰模型表现出色，而小型模型可以实现稳健性能，挑战了模型规模与可靠性之间的关系。

Conclusion: DDFT 测试为评估语言模型的元认知稳健性提供了理论基础和实践工具。

Abstract: Current language model evaluations measure what models know under ideal conditions but not how robustly they know it under realistic stress. Static benchmarks like MMLU and TruthfulQA cannot distinguish a model that lacks knowledge from one whose verification mechanisms collapse when information degrades or adversaries probe for weaknesses. We introduce the Drill-Down and Fabricate Test (DDFT), a protocol that measures epistemic robustness: a model's ability to maintain factual accuracy under progressive semantic compression and adversarial fabrication. We propose a two-system cognitive model comprising a Semantic System that generates fluent text and an Epistemic Verifier that validates factual accuracy. Our findings, based on evaluating 9 frontier models across 8 knowledge domains at 5 compression levels (1,800 turn-level evaluations), reveal that epistemic robustness is orthogonal to conventional design paradigms. Neither parameter count (r=0.083, p=0.832) nor architectural type (r=0.153, p=0.695) significantly predicts robustness, suggesting it emerges from training methodology and verification mechanisms distinct from current approaches. Error detection capability strongly predicts overall robustness (rho=-0.817, p=0.007), indicating this is the critical bottleneck. We find that flagship models exhibit brittleness despite their scale, while smaller models can achieve robust performance, challenging assumptions about the relationship between model size and reliability. The DDFT framework provides both theoretical foundation and practical tools for assessing epistemic robustness before deployment in critical applications.

</details>


### [2] [A Proof-of-Concept for Explainable Disease Diagnosis Using Large Language Models and Answer Set Programming](https://arxiv.org/abs/2512.23932)
*Ioanna Gemou,Evangelos Lamprou*

Main category: cs.AI

TL;DR: McCoy 是一种框架，它结合了大型语言模型和回答集编程（ASP），以生成健壮且可解释的疾病预测模型。


<details>
  <summary>Details</summary>
Motivation: 现有的符号AI在医疗保健中的应用受到构建高质量知识库所需的大量努力的限制，McCoy旨在通过结合大型语言模型和ASP来克服这一障碍。

Method: McCoy 框架通过使用大型语言模型将医学文献转化为ASP代码，并结合患者数据，利用ASP求解器进行处理，以达成最终的诊断。

Result: McCoy 在小型疾病诊断任务中展示了强大的性能。

Conclusion: McCoy 提供了一种新颖的方法，可以有效地将大型语言模型和ASP的优势结合起来，以实现健壮且可解释的疾病预测。

Abstract: Accurate disease prediction is vital for timely intervention, effective treatment, and reducing medical complications. While symbolic AI has been applied in healthcare, its adoption remains limited due to the effort required for constructing high-quality knowledge bases. This work introduces McCoy, a framework that combines Large Language Models (LLMs) with Answer Set Programming (ASP) to overcome this barrier. McCoy orchestrates an LLM to translate medical literature into ASP code, combines it with patient data, and processes it using an ASP solver to arrive at the final diagnosis. This integration yields a robust, interpretable prediction framework that leverages the strengths of both paradigms. Preliminary results show McCoy has strong performance on small-scale disease diagnosis tasks.

</details>


### [3] [SCP: Accelerating Discovery with a Global Web of Autonomous Scientific Agents](https://arxiv.org/abs/2512.24189)
*Yankai Jiang,Wenjie Lou,Lilong Wang,Zhenyu Tang,Shiyang Feng,Jiaxuan Lu,Haoran Sun,Yaning Pan,Shuang Gu,Haoyang Su,Feng Liu,Wangxu Wei,Pan Tan,Dongzhan Zhou,Fenghua Ling,Cheng Tan,Bo Zhang,Xiaosong Wang,Lei Bai,Bowen Zhou*

Main category: cs.AI

TL;DR: SCP 是一种开源标准，旨在通过统一资源集成和管控行星实验生命周期来加速科学发现，构建了一个包含 1,600 多个工具资源的生态系统。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在通过提供统一的科学资源描述和调用方法，以及全面的实验生命周期管理，来加速全球范围内的科学发现并支持跨机构和异构系统的合作。

Method: SCP 通过定义统一的资源描述规范和构建中央 SCP Hub 以及联邦 SCP 服务器的安全服务平台架构来实现其目标。

Result: SCP 构建了一个包含 1,600 多个工具资源的平台，促进了安全的大规模跨领域合作，显著降低了集成成本，提高了可重复性。

Conclusion: SCP 提供了必要的基础设施，支持跨机构、跨机构的自主科学研究，并为科学发现提供了一个大规模生态系统。

Abstract: We introduce SCP: the Science Context Protocol, an open-source standard designed to accelerate discovery by enabling a global network of autonomous scientific agents. SCP is built on two foundational pillars: (1) Unified Resource Integration: At its core, SCP provides a universal specification for describing and invoking scientific resources, spanning software tools, models, datasets, and physical instruments. This protocol-level standardization enables AI agents and applications to discover, call, and compose capabilities seamlessly across disparate platforms and institutional boundaries. (2) Orchestrated Experiment Lifecycle Management: SCP complements the protocol with a secure service architecture, which comprises a centralized SCP Hub and federated SCP Servers. This architecture manages the complete experiment lifecycle (registration, planning, execution, monitoring, and archival), enforces fine-grained authentication and authorization, and orchestrates traceable, end-to-end workflows that bridge computational and physical laboratories. Based on SCP, we have constructed a scientific discovery platform that offers researchers and agents a large-scale ecosystem of more than 1,600 tool resources. Across diverse use cases, SCP facilitates secure, large-scale collaboration between heterogeneous AI systems and human researchers while significantly reducing integration overhead and enhancing reproducibility. By standardizing scientific context and tool orchestration at the protocol level, SCP establishes essential infrastructure for scalable, multi-institution, agent-driven science.

</details>


### [4] [Constrained Language Model Policy Optimization via Risk-aware Stepwise Alignment](https://arxiv.org/abs/2512.24263)
*Lijun Zhang,Lin Li,Wei Wei,Yajie Qi,Huizhong Song,Jun Wang,Yaodong Yang,Jiye Liang*

Main category: cs.AI

TL;DR: RSA 提出了一种新的风险意识分步对齐方法，通过引入嵌套风险度量，将安全对齐问题转化为带风险约束的分词级别策略优化问题，从而避免模型过度偏离参考策略并抑制低概率但高影响的有害行为。


<details>
  <summary>Details</summary>
Motivation: 现有的安全对齐方法通常假设风险中立，不能有效应对偏离参考策略的风险和潜在严重有害行为，因此需要一种新的方法来增强对齐的安全性。

Method: RSA 方法通过引入嵌套风险度量，将安全对齐问题转化为一个带风险约束的分词级别策略优化问题，实现了风险意识下的步骤对齐。

Result: RSA 方法在实验中表现出高度的帮助性，同时确保了强大的安全性，并且有效地抑制了尾部风险，即低概率但高影响的不安全响应。

Conclusion: RSA 为细调预训练语言模型以实现所需行为提供了一种风险管理方法，能够有效缓解模型偏离并抑制潜在严重危害。

Abstract: When fine-tuning pre-trained Language Models (LMs) to exhibit desired behaviors, maintaining control over risk is critical for ensuring both safety and trustworthiness. Most existing safety alignment methods, such as Safe RLHF and SACPO, typically operate under a risk-neutral paradigm that is insufficient to address the risks arising from deviations from the reference policy and offers limited robustness against rare but potentially catastrophic harmful behaviors. To address this limitation, we propose Risk-aware Stepwise Alignment (RSA), a novel alignment method that explicitly incorporates risk awareness into the policy optimization process by leveraging a class of nested risk measures. Specifically, RSA formulates safety alignment as a token-level risk-aware constrained policy optimization problem and solves it through a stepwise alignment procedure that yields token-level policy updates derived from the nested risk measures. This design offers two key benefits: (1) it mitigates risks induced by excessive model shift away from a reference policy, and (2) it explicitly suppresses low-probability yet high-impact harmful behaviors. Moreover, we provide theoretical analysis on policy optimality under mild assumptions. Experimental results demonstrate that our method achieves high levels of helpfulness while ensuring strong safety and significantly suppresses tail risks, namely low-probability yet high-impact unsafe responses.

</details>


### [5] [What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?](https://arxiv.org/abs/2512.24497)
*Basile Terver,Tsung-Yen Yang,Jean Ponce,Adrien Bardes,Yann LeCun*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A long-standing challenge in AI is to develop agents capable of solving a wide range of physical tasks and generalizing to new, unseen tasks and environments. A popular recent approach involves training a world model from state-action trajectories and subsequently use it with a planning algorithm to solve new tasks. Planning is commonly performed in the input space, but a recent family of methods has introduced planning algorithms that optimize in the learned representation space of the world model, with the promise that abstracting irrelevant details yields more efficient planning. In this work, we characterize models from this family as JEPA-WMs and investigate the technical choices that make algorithms from this class work. We propose a comprehensive study of several key components with the objective of finding the optimal approach within the family. We conducted experiments using both simulated environments and real-world robotic data, and studied how the model architecture, the training objective, and the planning algorithm affect planning success. We combine our findings to propose a model that outperforms two established baselines, DINO-WM and V-JEPA-2-AC, in both navigation and manipulation tasks. Code, data and checkpoints are available at https://github.com/facebookresearch/jepa-wms.

</details>


### [6] [Thinking on Maps: How Foundation Model Agents Explore, Remember, and Reason Map Environments](https://arxiv.org/abs/2512.24504)
*Zhiwei Wei,Yuxing Liu,Hua Liao,Wenjia Xu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Map environments provide a fundamental medium for representing spatial structure. Understanding how foundation model (FM) agents understand and act in such environments is therefore critical for enabling reliable map-based reasoning and applications. However, most existing evaluations of spatial ability in FMs rely on static map inputs or text-based queries, overlooking the interactive and experience-driven nature of spatial understanding.In this paper, we propose an interactive evaluation framework to analyze how FM agents explore, remember, and reason in symbolic map environments. Agents incrementally explore partially observable grid-based maps consisting of roads, intersections, and points of interest (POIs), receiving only local observations at each step. Spatial understanding is then evaluated using six kinds of spatial tasks. By systematically varying exploration strategies, memory representations, and reasoning schemes across multiple foundation models, we reveal distinct functional roles of these components. Exploration primarily affects experience acquisition but has a limited impact on final reasoning accuracy. In contrast, memory representation plays a central role in consolidating spatial experience, with structured memories particularly sequential and graph-based representations, substantially improving performance on structure-intensive tasks such as path planning. Reasoning schemes further shape how stored spatial knowledge is used, with advanced prompts supporting more effective multi-step inference. We further observe that spatial reasoning performance saturates across model versions and scales beyond a certain capability threshold, indicating that improvements in map-based spatial understanding require mechanisms tailored to spatial representation and reasoning rather than scaling alone.

</details>


### [7] [Evaluating the Reasoning Abilities of LLMs on Underrepresented Mathematics Competition Problems](https://arxiv.org/abs/2512.24505)
*Samuel Golladay,Majid Bani-Yaghoub*

Main category: cs.AI

TL;DR: 本研究通过将三大语言模型应用于较少研究的数学竞赛问题，评估语言模型在数学推理中的表现，特别是在几何领域的特定错误类型和结构推理方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究多用相同数据集评估大型语言模型在数学推理中的局限性，本研究旨在探讨不同模型在处理特定数学问题时的不同表现。

Method: 研究选取了三种领头的语言模型GPT-4o-mini、Gemini-2.0-Flash和DeepSeek-V3，针对密苏里大学生数学竞赛问题中的微积分、解析几何及离散数学部分进行测试，对比模型解题结果，并分析它们的解题过程。

Result: DeepSeek-V3在微积分、解析几何和离散数学三个类别中表现最佳，特别是在推理和最终答案上。GPT-4o-mini和Gemini-2.0-Flash大多数错误分别集中在逻辑错误和推理不完整。DeepSeek-V3的错误主要由计算和逻辑错误引起，GPT-4o-mini则是逻辑和方法上的错误，而Gemini倾向于提前下结论。

Conclusion: 该研究发现，对较少研究的数学竞赛题目进行评估，可以揭示语言模型在问题解决时的特定错误模式，并指出在结构化推理方面，尤其是在几何领域，仍存在的挑战。

Abstract: Understanding the limitations of Large Language Models, or LLMs, in mathematical reasoning has been the focus of several recent studies. However, the majority of these studies use the same datasets for benchmarking, which limits the generalizability of their findings and may not fully capture the diverse challenges present in mathematical tasks. The purpose of the present study is to analyze the performance of LLMs on underrepresented mathematics competition problems. We prompted three leading LLMs, namely GPT-4o-mini, Gemini-2.0-Flash, and DeepSeek-V3, with the Missouri Collegiate Mathematics Competition problems in the areas of Calculus, Analytic Geometry, and Discrete Mathematics. The LLMs responses were then compared to the known correct solutions in order to determine the accuracy of the LLM for each problem domain. We also analyzed the LLMs reasoning to explore patterns in errors across problem types and models. DeepSeek-V3 has the best performance in all three categories of Calculus, Analytic Geometry, and Discrete Mathematics, both in reasoning and correct final answers. All three LLMs exhibited notably weak performance in Geometry. The majority of errors made by DeepSeek-V3 were attributed to computational and logical mistakes, whereas GPT-4o-mini frequently exhibited logical and approach-related errors. Gemini, on the other hand, tended to struggle with incomplete reasoning and drawing rushed conclusions. In conclusion, evaluating LLMs on underrepresented mathematics competition datasets can provide deeper insights into their distinct error patterns and highlight ongoing challenges in structured reasoning, particularly within the domain of Geometry.

</details>


### [8] [From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning](https://arxiv.org/abs/2512.24532)
*Amir Tahmasbi,Sadegh Majidi,Kazem Taram,Aniket Bera*

Main category: cs.AI

TL;DR: 本文提出了一种两阶段的方法，将空间推理分解为基本的原子构建块及其组合，首先通过监督微调赋予模型基本的空间物理学知识，然后在封闭环框架中训练轻量级LoRA适配器，使其能够组合这些构建块进行多步规划。该方法在动态和静态环境中均优于基线模型，并且训练过程更快且更稳定。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在导航和规划等应用中的需求增加，提高模型的空间推理能力变得尤为重要。尽管大型语言模型具有强大的语言能力，但在特定结构环境下的空间变换和多步规划方面仍存在挑战。

Method: 本文方法分为两阶段：第一阶段通过监督微调赋予模型基本的空间物理学知识，第二阶段在封闭环框架中训练轻量级LoRA适配器，让模型学习如何组合这些基本的空间构建块进行复杂的多步规划。为此，还构建了一个基于ASCII的艺术数据集和对应的强化学习环境。

Result: 该方法在动态和静态环境中均优于基线模型，包括通用的模型基础、具有空间物理意识的模型和从头开始的端到端强化学习模型。此外，该方法的训练过程更快且更稳定。

Conclusion: 研究表明，通过这种方法可以提升大型语言模型在空间推理方面的性能，并通过分析注意力模式证实了微调可以带来对空间理解的有意义改进。

Abstract: Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.

</details>


### [9] [Recursive Language Models](https://arxiv.org/abs/2512.24601)
*Alex L. Zhang,Tim Kraska,Omar Khattab*

Main category: cs.AI

TL;DR: 本文提出了一种递归语言模型（RLMs），允许大语言模型处理任意长的提示。RLMs 能够处理超出模型上下文窗口两个数量级的输入，并在多种长期上下文任务中显著优于基线大模型和常见的长期上下文支架，成本相似或更低。


<details>
  <summary>Details</summary>
Motivation: 由于实际情况中的输入可能非常长，直接处理这些输入可能会导致性能下降或超出模型上下文窗口。因此，探索如何有效处理任意长度输入成为了一个关键问题。

Method: 研究人员提出了 Recursive Language Models，这种方法将长输入视为外部环境的一部分，并允许大语言模型对输入进行程序化地拆分和递归处理。RLMs 通过将模型分成多个子阶段，并递归地处理输入的部分片段来工作。

Result: 实验结果表明，RLMs 能够成功处理输入数据的长度为其上下文窗口的两倍多，同时在各种任务中显著提高了输出的质量，且每查询的成本与普通基线模型相当甚至更低。

Conclusion: 该研究提出的方法为解决大语言模型处理长提示的问题提供了一种新的思路，验证了在长输入处理上的有效性和经济性。

Abstract: We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.

</details>


### [10] [Multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis under unseen working conditions](https://arxiv.org/abs/2512.24679)
*Pengcheng Xia,Yixiang Huang,Chengjin Qin,Chengliang Liu*

Main category: cs.AI

TL;DR: 本文提出了一种双解耦机制的多模态跨域混合融合模型，旨在解决现有故障诊断方法在面对未见过的工作条件时性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法在面对未见过的工作条件时性能下降，依赖目标域样本的域适应方法能力受限，且大多数研究仅依赖单一模态感知信号，未能充分利用多模态信息以提升模型泛化能力。

Method: 本文提出了一个双解耦框架来解耦不变模态特征和特定模态特征，以及不变域特征和特定域特征的表示，旨在实现全面的多模态表示学习和稳健的域泛化。同时，设计了跨域混合融合策略以增加模态和域多样性，并引入了三模态融合机制以适配性地整合多模态异质信息。

Result: 在不同未见过的恒定和时变工作条件下对感应电机故障诊断进行大量实验，结果表明所提出的方法在性能上优于先进方法，并且通过全面的消融研究进一步验证了每个提出部件和多模态融合的有效性。

Conclusion: 本文提出的方法可实现故障诊断的稳健性，增强了模型在未见过的工作条件下的泛化能力，同时为未来该领域的工作提供了新的视角和方法。

Abstract: Intelligent fault diagnosis has become an indispensable technique for ensuring machinery reliability. However, existing methods suffer significant performance decline in real-world scenarios where models are tested under unseen working conditions, while domain adaptation approaches are limited to their reliance on target domain samples. Moreover, most existing studies rely on single-modal sensing signals, overlooking the complementary nature of multi-modal information for improving model generalization. To address these limitations, this paper proposes a multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis. A dual disentanglement framework is developed to decouple modality-invariant and modality-specific features, as well as domain-invariant and domain-specific representations, enabling both comprehensive multi-modal representation learning and robust domain generalization. A cross-domain mixed fusion strategy is designed to randomly mix modality information across domains for modality and domain diversity augmentation. Furthermore, a triple-modal fusion mechanism is introduced to adaptively integrate multi-modal heterogeneous information. Extensive experiments are conducted on induction motor fault diagnosis under both unseen constant and time-varying working conditions. The results demonstrate that the proposed method consistently outperforms advanced methods and comprehensive ablation studies further verify the effectiveness of each proposed component and multi-modal fusion. The code is available at: https://github.com/xiapc1996/MMDG.

</details>


### [11] [BatteryAgent: Synergizing Physics-Informed Interpretation with LLM Reasoning for Intelligent Battery Fault Diagnosis](https://arxiv.org/abs/2512.24686)
*Songqi Zhou,Ruixue Liu,Boman Su,Jiazhou Wang,Yixing Wang,Benben Jiang*

Main category: cs.AI

TL;DR: BatteryAgent 是一种分层框架，通过融合物理知识特征和大型语言模型的推理能力，解决了电池故障诊断中‘黑盒’性质和二元分类局限性的问题，实现了具有解释性的多类型诊断，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法虽然在故障检测方面表现出色，但其‘黑盒’性质限制了其解释性；另外，它们受限于二元分类框架，难以进行根本原因分析和维护建议。

Method: BatteryAgent 确立了三个核心模块，包括物理感知层、检测与归因层、以及推理与诊断层。这些模块结合了机理特征、梯度提升决策树及 SHAP 方法来量化特征贡献，并利用大型语言模型构建数字与语义桥梁，综合 SHAP 归因与机理知识库生成全面报告。

Result: 实验结果表明，BatteryAgent 在硬边样本上的误分类纠正效果明显，AUC-ROC 达到 0.986，优于现有先进方法，还扩展了传统二元检测到多类型可解释诊断，推动了电池安全管理从‘被动检测’向‘智能诊断’转变。

Conclusion: BatteryAgent 的提出，为电池故障诊断提供了一个兼具解释性和先进性的新框架，展示了未来在电池安全管理中的应用潜力。

Abstract: Fault diagnosis of lithium-ion batteries is critical for system safety. While existing deep learning methods exhibit superior detection accuracy, their "black-box" nature hinders interpretability. Furthermore, restricted by binary classification paradigms, they struggle to provide root cause analysis and maintenance recommendations. To address these limitations, this paper proposes BatteryAgent, a hierarchical framework that integrates physical knowledge features with the reasoning capabilities of Large Language Models (LLMs). The framework comprises three core modules: (1) A Physical Perception Layer that utilizes 10 mechanism-based features derived from electrochemical principles, balancing dimensionality reduction with physical fidelity; (2) A Detection and Attribution Layer that employs Gradient Boosting Decision Trees and SHAP to quantify feature contributions; and (3) A Reasoning and Diagnosis Layer that leverages an LLM as the agent core. This layer constructs a "numerical-semantic" bridge, combining SHAP attributions with a mechanism knowledge base to generate comprehensive reports containing fault types, root cause analysis, and maintenance suggestions. Experimental results demonstrate that BatteryAgent effectively corrects misclassifications on hard boundary samples, achieving an AUROC of 0.986, which significantly outperforms current state-of-the-art methods. Moreover, the framework extends traditional binary detection to multi-type interpretable diagnosis, offering a new paradigm shift from "passive detection" to "intelligent diagnosis" for battery safety management.

</details>


### [12] [Explaining Why Things Go Where They Go: Interpretable Constructs of Human Organizational Preferences](https://arxiv.org/abs/2512.24829)
*Emmanuel Fashae,Michael Burke,Leimin Tian,Lingheng Meng,Pamela Carreno-Medrano*

Main category: cs.AI

TL;DR: 该研究通过设计和验证一份自我报告问卷，提出了室内物体排列的四个可解释要素，并将其应用于Monte Carlo Tree Search (MCTS) 动态规划器，结果显示这种规划器能够生成与参与者相似的合理排列。


<details>
  <summary>Details</summary>
Motivation: 当前的机器人系统依赖于从人类演示中推断出的潜在偏好模型，虽然这些模型在预测方面有效，但对于引导人类决策的可解释因素缺乏洞察。因此，作者们试图明确定义物体排列偏好，以提高机器人的理解能力，使它们能更接近人的自然需求。

Method: 研究设计并通过63名参与者的在线研究验证了一份自我报告问卷，进而定义了四个可解释的物体排列偏好：空间实用性、习惯便利、语义连贯性和常识适宜性。利用这些偏好，将它们集成到Monte Carlo Tree Search (MCTS) 策略中，使算法可以根据受访者的偏好生成合理的物体布置。

Result: 针对厨房和客厅两种场景的研究结果表明，这些定义的偏好对于这些场景是明确且具有解释力的，并且使用这些偏好指导的Monte Carlo Tree Search (MCTS) 计划器生成的安排与参与者生成的相似度很高。

Conclusion: 研究提出了一种简化且可解释的物体排列偏好定义，并展示了如何将其应用于机器人规划中。这一贡献为提高机器人系统对人类物体布置偏好的理解提供了新的方法。

Abstract: Robotic systems for household object rearrangement often rely on latent preference models inferred from human demonstrations. While effective at prediction, these models offer limited insight into the interpretable factors that guide human decisions. We introduce an explicit formulation of object arrangement preferences along four interpretable constructs: spatial practicality (putting items where they naturally fit best in the space), habitual convenience (making frequently used items easy to reach), semantic coherence (placing items together if they are used for the same task or are contextually related), and commonsense appropriateness (putting things where people would usually expect to find them). To capture these constructs, we designed and validated a self-report questionnaire through a 63-participant online study. Results confirm the psychological distinctiveness of these constructs and their explanatory power across two scenarios (kitchen and living room). We demonstrate the utility of these constructs by integrating them into a Monte Carlo Tree Search (MCTS) planner and show that when guided by participant-derived preferences, our planner can generate reasonable arrangements that closely align with those generated by participants. This work contributes a compact, interpretable formulation of object arrangement preferences and a demonstration of how it can be operationalized for robot planning.

</details>


### [13] [GenZ: Foundational models as latent variable generators within traditional statistical models](https://arxiv.org/abs/2512.24834)
*Marko Jojic,Nebojsa Jojic*

Main category: cs.AI

TL;DR: GenZ 是一种结合了基础模型和统计建模的混合模型，通过可解释的语义特征来捕捉特定数据集模式，从而提高预测任务的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然具有广泛的领域知识，但在捕捉特定数据集的模式方面通常表现不佳，这影响了预测任务的准确性。GenZ 通过发现基于统计建模误差的语义特征描述来解决这一问题，引入了一种结合语义特征和统计模型参数优化的通用 EM 算法，以提高模型在特定数据集上的预测能力。

Method: GenZ 通过迭代过程发现语义特征描述，利用通用 EM 算法联合优化了语义特征描述和统计模型参数。模型利用未冻结的基础模型根据发现的特征进行分类，将其分类判断视为求解潜在二元特征的噪声观察，潜在二元特征通过学习的统计关系预测真实值目标。

Result: GenZ 在两个领域展示了优越性能：房屋价格预测中，使用从多方列表数据中发现的语义特征，模型的中位相对误差为12%，显著低于依赖于大语言模型通用领域知识的GPT-5baseline（38%误差）。在Netflix电影嵌入中，利用语义描述预测协作过滤表示，相似度达到0.59，相当于大约需要4000个用户评分的传统协作过滤方法。

Conclusion: GenZ 实现了基础模型和统计建模的有效结合，发现了数据集特异性模式，提高了模型在预测任务中的表现。

Abstract: We present GenZ, a hybrid model that bridges foundational models and statistical modeling through interpretable semantic features. While large language models possess broad domain knowledge, they often fail to capture dataset-specific patterns critical for prediction tasks. Our approach addresses this by discovering semantic feature descriptions through an iterative process that contrasts groups of items identified via statistical modeling errors, rather than relying solely on the foundational model's domain understanding. We formulate this as a generalized EM algorithm that jointly optimizes semantic feature descriptors and statistical model parameters. The method prompts a frozen foundational model to classify items based on discovered features, treating these judgments as noisy observations of latent binary features that predict real-valued targets through learned statistical relationships. We demonstrate the approach on two domains: house price prediction (hedonic regression) and cold-start collaborative filtering for movie recommendations. On house prices, our model achieves 12\% median relative error using discovered semantic features from multimodal listing data, substantially outperforming a GPT-5 baseline (38\% error) that relies on the LLM's general domain knowledge. For Netflix movie embeddings, our model predicts collaborative filtering representations with 0.59 cosine similarity purely from semantic descriptions -- matching the performance that would require approximately 4000 user ratings through traditional collaborative filtering. The discovered features reveal dataset-specific patterns (e.g., architectural details predicting local housing markets, franchise membership predicting user preferences) that diverge from the model's domain knowledge alone.

</details>


### [14] [A study on constraint extraction and exception exclusion in care worker scheduling](https://arxiv.org/abs/2512.24853)
*Koki Suenaga,Tomohiro Furuta,Satoshi Ono*

Main category: cs.AI

TL;DR: 本文提出了一种针对长期护理设施定制工作排班的方法，利用约束模板提取多种工作安排的限制条件，并减少软约束的违反次数。


<details>
  <summary>Details</summary>
Motivation: 由于长期护理设施之间的条件差异性，需要针对不同设施设计特定条件。现有的排班方法未必适用于所有设施，因此提出了一种新的方法来设计设施特定的约束条件。

Method: 该方法使用约束模板提取不同的工作排班模式和工作人员组合。这些模板可以通过调整关注的天数和人员数量来提取不同类型的约束条件。此外，该方法还包含排除异常约束的机制。

Result: 实验表明，本文提出的方法成功地创建了满足所有硬约束的排班，同时减少了软约束的违反次数。

Conclusion: 本文提出的方法通过约束模板提取适用于特定护理设施的约束条件，利用约束编程求解器生成护理人员的工作排班，证明了其在实际应用中的有效性和优越性。

Abstract: Technologies for automatically generating work schedules have been extensively studied; however, in long-term care facilities, the conditions vary between facilities, making it essential to interview the managers who create shift schedules to design facility-specific constraint conditions. The proposed method utilizes constraint templates to extract combinations of various components, such as shift patterns for consecutive days or staff combinations. The templates can extract a variety of constraints by changing the number of days and the number of staff members to focus on and changing the extraction focus to patterns or frequency. In addition, unlike existing constraint extraction techniques, this study incorporates mechanisms to exclude exceptional constraints. The extracted constraints can be employed by a constraint programming solver to create care worker schedules. Experiments demonstrated that our proposed method successfully created schedules that satisfied all hard constraints and reduced the number of violations for soft constraints by circumventing the extraction of exceptional constraints.

</details>


### [15] [Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem](https://arxiv.org/abs/2512.24873)
*Weixun Wang,XiaoXiao Xu,Wanhe An,Fangwen Dai,Wei Gao,Yancheng He,Ju Huang,Qiang Ji,Hanqi Jin,Xiaoyang Li,Yang Li,Zhongwen Li,Shirong Lin,Jiashun Liu,Zenan Liu,Tao Luo,Dilxat Muhtar,Yuanbin Qu,Jiaqiang Shi,Qinghui Sun,Yingshui Tan,Hao Tang,Runze Wang,Yi Wang,Zhaoguo Wang,Yanan Wu,Shaopan Xiong,Binchen Xu,Xander Xu,Yuchi Xu,Qipeng Zhang,Xixia Zhang,Haizhou Zhao,Jie Zhao,Shuaibing Zhao,Baihui Zheng,Jianhui Zheng,Suhang Zheng,Yanni Zhu,Mengze Cai,Kerui Cao,Xitong Chen,Yue Dai,Lifan Du,Tao Feng,Tao He,Jin Hu,Yijie Hu,Ziyu Jiang,Cheng Li,Xiang Li,Jing Liang,Chonghuan Liu,ZhenDong Liu,Haodong Mi,Yanhu Mo,Junjia Ni,Shixin Pei,Jingyu Shen,XiaoShuai Song,Cecilia Wang,Chaofan Wang,Kangyu Wang,Pei Wang,Tao Wang,Wei Wang,Ke Xiao,Mingyu Xu,Tiange Xu,Nan Ya,Siran Yang,Jianan Ye,Yaxing Zang,Duo Zhang,Junbo Zhang,Boren Zheng,Wanxi Deng,Ling Pan,Lin Qu,Wenbo Su,Jiamang Wang,Wei Wang,Hu Wei,Minggang Wu,Cheng Yu,Bing Zhao,Zhicheng Zheng,Bo Zheng*

Main category: cs.AI

TL;DR: 该研究介绍了Agentic Learning Ecosystem (ALE)，一种优化智能体LLM生产流程的基础架构，其中包括ROLL、ROCK和iFlow CLI三个组件。ALE通过合成复杂行为的数据编排协议和基于交互的策略对齐（IPA）算法提高了长期训练的稳定性。研究还展示了基于ALE的开源智能体ROME在多个基准测试中的出色表现。


<details>
  <summary>Details</summary>
Motivation: 当前开源社区缺乏一个系统化的智能体开发环境，为了解决这一问题，作者提出了Agentic Learning Ecosystem (ALE)，以简化智能体的开发流程。

Method: 该方法包括三个主要组件：RILL (Rollout Optimizer) 用于模型权重优化；ROCK (Rollout Control Kit) 用于轨迹生成的沙盒环境管理；以及iFlow CLI，一种用于智能体开发上下文工程的框架。

Result: 通过ALE，开发了一种被称为ROME的开源智能体，它基于超过一百万条轨迹的数据训练，并使用了数据编排协议和IPA (Interaction-based Policy Alignment) 算法。研究人员进一步通过引入Terminal Bench Pro基准测试，增强了规模和污染控制。

Conclusion: 实验结果表明，基于ALE的智能体ROME在多个基准测试中表现出色，验证了ALE基础设施的有效性。

Abstract: Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.

</details>


### [16] [Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing](https://arxiv.org/abs/2512.24896)
*Andrii Gamalii,Daniel Górniak,Robert Nowak,Bartłomiej Olber,Krystian Radlak,Jakub Winter*

Main category: cs.AI

TL;DR: 本报告介绍了DARTS项目中开发的半自动化数据标注管道的设计与实施，目标是在波兰条件下创建大规模的多模态驾驶场景数据集。该解决方案通过结合人工智能与人类专业知识，有效降低了标注成本和时间。


<details>
  <summary>Details</summary>
Motivation: 手动标注这种异构数据既昂贵又耗时。为解决这一挑战，本研究提出了一种人机协作的方法，结合了人工智能与人类专业知识，以减少标注成本和时间。

Method: 系统自动生成初步标注，支持迭代模型重新训练，并集成数据匿名化和领域自适应技术。核心工具依赖于三维物体检测算法产生初步标注。

Result: 该开发的工具和方法在保持不同传感器模态之间一致性和高质量标注的同时，节省了大量时间。该解决方案直接支持DARTS项目，加速了标准格式的大规模标注数据集的准备，增强了波兰自主车辆研究的技术基础。

Conclusion: 本研究提出的半自动化数据标注管道显著提高了数据标注效率，确保了高质量的多模态数据标注，支持了DARTS项目的进展。

Abstract: This report presents the design and implementation of a semi-automated data annotation pipeline developed within the DARTS project, whose goal is to create a large-scale, multimodal dataset of driving scenarios recorded in Polish conditions. Manual annotation of such heterogeneous data is both costly and time-consuming. To address this challenge, the proposed solution adopts a human-in-the-loop approach that combines artificial intelligence with human expertise to reduce annotation cost and duration. The system automatically generates initial annotations, enables iterative model retraining, and incorporates data anonymization and domain adaptation techniques. At its core, the tool relies on 3D object detection algorithms to produce preliminary annotations. Overall, the developed tools and methodology result in substantial time savings while ensuring consistent, high-quality annotations across different sensor modalities. The solution directly supports the DARTS project by accelerating the preparation of large annotated dataset in the project's standardized format, strengthening the technological base for autonomous vehicle research in Poland.

</details>


### [17] [Iterative Deployment Improves Planning Skills in LLMs](https://arxiv.org/abs/2512.24940)
*Augusto B. Corrêa,Yoav Gelberg,Luckeciano C. Melo,Ilia Shumailov,André G. Pereira,Yarin Gal*

Main category: cs.AI

TL;DR: 该研究展示了通过迭代部署大型语言模型并根据前一模型的部署数据精心选择数据进行微调，可以显著改变模型的性质。通过在多个规划领域进行测试，发现这一机制可以使模型获得更好的规划能力，并且能够发现更长的解决方案。研究还证明这种迭代部署实际上是在外环中实现了强化学习的训练，具有潜在的安全性和新颖性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，如何通过优化训练数据来增强模型的能力成为了一个重要的研究方向。该研究旨在探索通过迭代部署和精心挑选的数据Fine-tuning来提升模型性能的方法。

Method: 该研究设计了一种基于迭代部署和精心挑选数据微调的机制，通过对多个规划领域进行实验，验证了这种方法的有效性。

Result: 实验结果表明，通过这一机制，模型的规划能力得到了显著提升，尤其是在发现更长的规划解决方案方面表现出色。这种方法还被证明能够在外环中实现强化学习的训练。

Conclusion: 研究认为，未来的大型语言模型构建可以通过这种方式进一步优化数据和方法以实现更好的性能，同时也为AI安全研究提供了新的视角。

Abstract: We show that iterative deployment of large language models (LLMs), each fine-tuned on data carefully curated by users from the previous models' deployment, can significantly change the properties of the resultant models. By testing this mechanism on various planning domains, we observe substantial improvements in planning skills, with later models displaying emergent generalization by discovering much longer plans than the initial models. We then provide theoretical analysis showing that iterative deployment effectively implements reinforcement learning (RL) training in the outer-loop (i.e. not as part of intentional model training), with an implicit reward function. The connection to RL has two important implications: first, for the field of AI safety, as the reward function entailed by repeated deployment is not defined explicitly, and could have unexpected implications to the properties of future model deployments. Second, the mechanism highlighted here can be viewed as an alternative training regime to explicit RL, relying on data curation rather than explicit rewards.

</details>


### [18] [AMAP Agentic Planning Technical Report](https://arxiv.org/abs/2512.24957)
*Yulan Hu,Xiangwen Zhang,Sheng Ouyang,Hao Yi,Lu Xu,Qinglin Lang,Lide Tan,Xiang Cheng,Tianchen Ye,Zhicong Li,Ge Chen,Wenjin Yang,Zheng Pan,Shaopan Xiong,Siran Yang,Ju Huang,Yan Zhang,Jiamang Wang,Yong Liu,Yinfeng Huang,Tucheng Lin,Xin Li,Ning Guo*

Main category: cs.AI

TL;DR: STAgent 是一种专为时空理解设计的大规模语言模型，通过稳定工具环境、高级数据整理框架和分层训练方法提升了其在复杂任务如有约束的兴趣点探索和行程计划中的表现。


<details>
  <summary>Details</summary>
Motivation: STAgent 被设计为解决复杂的时空任务，特别是在限制性点兴趣发现和行程规划方面。开发者希望通过增强模型的时空理解能力和与其他工具的交互，提高其在这些领域的应用效果。

Method: STAgent 通过以下三个关键贡献提升其能力：1. 一种稳定的工具环境，支持了超过十种领域特异工具的异步部署和训练；2. 一种分层数据整理框架，该框架能高效识别高质量数据，过滤比率为1:10,000，同时涵盖了多样性和难度；3. 一种分层训练方案，从一个种子样品FMT阶段开始评估查询难度，接着是对高确定性查询的FMT再训练，最后是使用低确定性数据的RL阶段。这些技术手段帮助模型保持通用性的同时提高了特定情境下的表现。

Result: STAgent 显示出了在 TravelBench 上的卓越表现，并在广泛的通用基准测试中保持了其通用性，这表明了该机构模型的有效性。

Conclusion: 这项研究证明了 STAgent 在时空任务上的应用潜力，提出了有效的模型设计和训练策略，吸引了对时空数据处理和大模型应用感兴趣的社区成员。

Abstract: We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning. STAgent is a specialized model capable of interacting with ten distinct tools within spatio-temporal scenarios, enabling it to explore, verify, and refine intermediate steps during complex reasoning. Notably, STAgent effectively preserves its general capabilities. We empower STAgent with these capabilities through three key contributions: (1) a stable tool environment that supports over ten domain-specific tools, enabling asynchronous rollout and training; (2) a hierarchical data curation framework that identifies high-quality data like a needle in a haystack, curating high-quality queries with a filter ratio of 1:10,000, emphasizing both diversity and difficulty; and (3) a cascaded training recipe that starts with a seed SFT stage acting as a guardian to measure query difficulty, followed by a second SFT stage fine-tuned on queries with high certainty, and an ultimate RL stage that leverages data of low certainty. Initialized with Qwen3-30B-A3B to establish a strong SFT foundation and leverage insights into sample difficulty, STAgent yields promising performance on TravelBench while maintaining its general capabilities across a wide range of general benchmarks, thereby demonstrating the effectiveness of our proposed agentic model.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [19] [Secure and Governed API Gateway Architectures for Multi-Cluster Cloud Environments](https://arxiv.org/abs/2512.23774)
*Vinoth Punniyamoorthy,Kabilan Kannan,Akshay Deshpande,Lokesh Butra,Akash Kumar Agarwal,Adithya Parthasarathy,Suhas Malempati,Bikesh Kumar*

Main category: cs.CR

TL;DR: 本文提出了一种治理感知且意图驱动的API网关管理架构，适用于多集群云环境。通过将安全、治理和性能目标表达为高级声明性意图，持续验证策略并借助遥测反馈进行制导，确保治理保证和服务级目标不受损害。


<details>
  <summary>Details</summary>
Motivation: 随着组织越来越多地采用多集群和混合云部署，需要一种治理感知的API网关管理架构，以保持跨异构网关环境的一致性策略执行、可预测性能和操作稳定性。

Method: 提出了一种治理感知且意图驱动的架构，通过将安全、治理和性能目标表达为高级声明性意图，确保治理保证和服务级目标不受损害；通过策略验证和基于遥测反馈的持续验证来实施和验证这些意图；实现了一个跨多个Kubernetes集群的原型。

Result: 试验结果表明，与手动和声明性基线方法相比，此设计具有以下优势：策略漂移减少高达42%，策略传播时间改善31%，且在可变负载下持续p95延迟开销低于6%。

Conclusion: 治理感知且意图驱动的API网关编排为安全、一致且具有可预测性能的云原生平台提供了一个可扩展且可靠的基石。

Abstract: API gateways serve as critical enforcement points for security, governance, and traffic management in cloud-native systems. As organizations increasingly adopt multi-cluster and hybrid cloud deployments, maintaining consistent policy enforcement, predictable performance, and operational stability across heterogeneous gateway environments becomes challenging. Existing approaches typically manage security, governance, and performance as loosely coupled concerns, leading to configuration drift, delayed policy propagation, and unstable runtime behavior under dynamic workloads. This paper presents a governance-aware, intent-driven architecture for coordinated API gateway management in multi-cluster cloud environments. The proposed approach expresses security, governance, and performance objectives as high-level declarative intents, which are systematically translated into enforceable gateway configurations and continuously validated through policy verification and telemetry-driven feedback. By decoupling intent specification from enforcement while enabling bounded, policy-compliant adaptation, the architecture supports heterogeneous gateway implementations without compromising governance guarantees or service-level objectives. A prototype implementation across multiple Kubernetes clusters demonstrates the effectiveness of the proposed design. Experimental results show up to a 42% reduction in policy drift, a 31% improvement in configuration propagation time, and sustained p95 latency overhead below 6% under variable workloads, compared to manual and declarative baseline approaches. These results indicate that governance-aware, intent-driven gateway orchestration provides a scalable and reliable foundation for secure, consistent, and performance-predictable cloud-native platforms.

</details>


### [20] [SyncGait: Robust Long-Distance Authentication for Drone Delivery via Implicit Gait Behaviors](https://arxiv.org/abs/2512.23778)
*Zijian Ling,Man Zhou,Hongda Zhai,Yating Huang,Lingchen Zhao,Qi Li,Chao Shen,Qian Wang*

Main category: cs.CR

TL;DR: SyncGait 是一种利用行人独有行走摆臂模式实现无硬件交互的无人机身份验证系统，远距离认证准确率达 99.84%，并在多种欺骗攻击下表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有无人机认证方案在认证距离有限且对抗高级攻击能力不足的问题。

Method: 利用行人行走时的臂摆动特征进行身份验证，无需额外硬件和特定动作。

Result: 在超过 18 米的距离上，SyncGait 达到了 99.84% 的平均认证准确率，并在各种欺骗攻击下表现出色。

Conclusion: SyncGait 提供了一种可靠的、安全的且用户友好的无人机认证解决方案。

Abstract: In recent years, drone delivery, which utilizes unmanned aerial vehicles (UAVs) for package delivery and pickup, has gradually emerged as a crucial method in logistics. Since delivery drones are expensive and may carry valuable packages, they must maintain a safe distance from individuals until user-drone mutual authentication is confirmed. Despite numerous authentication schemes being developed, existing solutions are limited in authentication distance and lack resilience against sophisticated attacks. To this end, we introduce SyncGait, an implicit gait-based mutual authentication system for drone delivery. SyncGait leverages the user's unique arm swing as he walks toward the drone to achieve mutual authentication without requiring additional hardware or specific authentication actions. We conducted extensive experiments on 14 datasets collected from 31 subjects. The results demonstrate that SyncGait achieves an average accuracy of 99.84\% at a long distance ($>18m$) and exhibits strong resilience against various spoofing attacks, making it a robust, secure, and user-friendly solution in real-world scenarios.

</details>


### [21] [Prompt-Induced Over-Generation as Denial-of-Service: A Black-Box Attack-Side Benchmark](https://arxiv.org/abs/2512.23779)
*Manu,Yi Guo,Jo Plested,Tim Lynar,Kanchana Thilakarathna,Nirhoshan Sivaroopan,Jack Yang,Wangli Yang*

Main category: cs.CR

TL;DR: 该研究引入了一种基准测试来评估基于提示的DoS攻击，并研究了两个仅基于提示的攻击者。进化型过生成提示搜索（EOGen）和目标条件强化学习攻击者（RL-GOAL）。攻击者的目标是生成大量令牌以降低成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 大语言模型容易产生过量生成，这个问题会影响答案质量，增加延迟和成本，甚至可以作为拒绝服务攻击。现有研究侧重于单一攻击算法或假设白盒访问，缺乏一个黑盒、只查询的基准测试来比较基于提示的攻击者。

Method: 研究者引入了一种新的基准测试方法，评估了两种基于提示的攻击者：进化型过生成提示搜索（EOGen）和目标条件强化学习攻击者（RL-GOAL）。攻击者利用进化算法或强化学习来生成能够抑制EOS并产生长续发展缀的前缀。

Result: 研究表明，EOGen在Phi-3上的成功率较低，平均过生成因子（OGF）为1.38 +/- 1.15；而RL-GOAL表现更优，具有较高的平均过生成因子，最高可达2.81 +/- 1.38。

Conclusion: 研究结果表明，目标条件强化学习攻击者具有更强的攻击能力，能够在多种受害者模型下实现更高的过生成效果，为对抗过生成问题提供了新的思路。

Abstract: Large language models (LLMs) can be driven into over-generation, emitting thousands of tokens before producing an end-of-sequence (EOS) token. This degrades answer quality, inflates latency and cost, and can be weaponized as a denial-of-service (DoS) attack. Recent work has begun to study DoS-style prompt attacks, but typically focuses on a single attack algorithm or assumes white-box access, without an attack-side benchmark that compares prompt-based attackers in a black-box, query-only regime with a known tokenizer. We introduce such a benchmark and study two prompt-only attackers. The first is Evolutionary Over-Generation Prompt Search (EOGen), which searches the token space for prefixes that suppress EOS and induce long continuations. The second is a goal-conditioned reinforcement learning attacker (RL-GOAL) that trains a network to generate prefixes conditioned on a target length. To characterize behavior, we introduce Over-Generation Factor (OGF), the ratio of produced tokens to a model's context window, along with stall and latency summaries. Our evolutionary attacker achieves mean OGF = 1.38 +/- 1.15 and Success@OGF >= 2 of 24.5 percent on Phi-3. RL-GOAL is stronger: across victims it achieves higher mean OGF (up to 2.81 +/- 1.38).

</details>


### [22] [Application-Specific Power Side-Channel Attacks and Countermeasures: A Survey](https://arxiv.org/abs/2512.23785)
*Sahan Sanjaya,Aruna Jayasena,Prabhat Mishra*

Main category: cs.CR

TL;DR: 本文提供了一篇关于不同应用场景下的电源侧信道攻击及其对策的全面综述，旨在分类最近的电源侧信道攻击并根据应用场景进行详细比较。


<details>
  <summary>Details</summary>
Motivation: 鉴于近年来电源侧信道攻击在不同应用领域（如加密实现、机器学习模型反向工程、用户行为数据利用和指令级反汇编）中的探索日益增多，本文旨在进行系统性的研究，以便更好地理解这些攻击及其防御措施。

Method: 本文通过回顾现已发表的相关研究，识别和分类电源侧信道攻击的不同类型，并基于应用领域对这些攻击进行比较分析，以此构建一个全面的综述框架。

Result: 本文提供了一个关于电源侧信道攻击在不同应用领域的分类框架和详细比较，这有助于研究人员和安全专家更好地理解和应对这些攻击。

Conclusion: 本文通过提供一个系统的综述，强调了在不同应用领域内进...

Abstract: Side-channel attacks try to extract secret information from a system by analyzing different side-channel signatures, such as power consumption, electromagnetic emanation, thermal dissipation, acoustics, time, etc. Power-based side-channel attack is one of the most prominent side-channel attacks in cybersecurity, which rely on data-dependent power variations in a system to extract sensitive information. While there are related surveys, they primarily focus on power side-channel attacks on cryptographic implementations. In recent years, power-side channel attacks have been explored in diverse application domains, including key extraction from cryptographic implementations, reverse engineering of machine learning models, user behavior data exploitation, and instruction-level disassembly. In this paper, we provide a comprehensive survey of power side-channel attacks and their countermeasures in different application domains. Specifically, this survey aims to classify recent power side-channel attacks and provide a comprehensive comparison based on application-specific considerations.

</details>


### [23] [Security Without Detection: Economic Denial as a Primitive for Edge and IoT Defense](https://arxiv.org/abs/2512.23849)
*Samaresh Kumar Singh,Joyjit Roy*

Main category: cs.CR

TL;DR: EDS 提供了一种基于经济原理的安全框架，通过增加攻击成本来保护受限资源环境下的 IoT/边缘设备，即使不依赖传统检测方法也能显著提升安全防护。


<details>
  <summary>Details</summary>
Motivation: 传统的基于检测的安全机制在对抗加密、隐身及低速率攻击时失效，特别是在资源受限的 IoT/边缘环境中。因此，需要一种无需依赖检测的方法来保证安全。

Method: EDS 采用四个机制：自适应计算难题、诱饵驱动的交互熵、时间拉伸和带宽征税，这些机制组合成框架以证明超线性成本放大效果。该框架被形式化为 Stackelberg 游戏，用于优化参数选择。

Result: EDS 在 20 台异构 IoT 设备上进行了评估，结果显示攻击速度降低了 32-560 倍，成本比单独机制高出 2.1 倍，攻击成功率减少了 8-62%，影响小于 20ms，且几乎无误报率。与其他方法结合使用时，EDS 可提供 94% 的防护效果，对比单独使用 IDS 提升了 27%。

Conclusion: EDS 是一种面向资源受限环境的经济驱动安全框架，即使不依赖传统的入侵检测系统也能提供有效的防护，为物联网和边缘系统的安全保护提供了可行方法。

Abstract: Detection-based security fails against sophisticated attackers using encryption, stealth, and low-rate techniques, particularly in IoT/edge environments where resource constraints preclude ML-based intrusion detection. We present Economic Denial Security (EDS), a detection-independent framework that makes attacks economically infeasible by exploiting a fundamental asymmetry: defenders control their environment while attackers cannot. EDS composes four mechanisms adaptive computational puzzles, decoy-driven interaction entropy, temporal stretching, and bandwidth taxation achieving provably superlinear cost amplification. We formalize EDS as a Stackelberg game, deriving closed-form equilibria for optimal parameter selection (Theorem 1) and proving that mechanism composition yields 2.1x greater costs than the sum of individual mechanisms (Theorem 2). EDS requires < 12KB memory, enabling deployment on ESP32 class microcontrollers. Evaluation on a 20-device heterogeneous IoT testbed across four attack scenarios (n = 30 trials, p < 0.001) demonstrates: 32-560x attack slowdown, 85-520:1 cost asymmetry, 8-62% attack success reduction, < 20ms latency overhead, and close to 0% false positives. Validation against IoT-23 malware (Mirai, Torii, Hajime) shows 88% standalone mitigation; combined with ML-IDS, EDS achieves 94% mitigation versus 67% for IDS alone a 27% improvement. EDS provides detection-independent protection suitable for resource-constrained environments where traditional approaches fail. The ability to detect and mitigate the malware samples tested was enhanced; however, the benefits provided by EDS were realized even without the inclusion of an IDS. Overall, the implementation of EDS serves to shift the economic balance in favor of the defender and provides a viable method to protect IoT and edge systems methodologies.

</details>


### [24] [RepetitionCurse: Measuring and Understanding Router Imbalance in Mixture-of-Experts LLMs under DoS Stress](https://arxiv.org/abs/2512.23995)
*Ruixuan Huang,Qingyue Wang,Hantao Huang,Yudong Gao,Dong Chen,Shuai Wang,Wei Wang*

Main category: cs.CR

TL;DR: 研究指出，混合专家架构在处理大规模语言模型时存在隐式负载均衡问题，可通过构造特定模式的提示利用这一缺陷，导致严重的路由集中和资源分配不均，从而影响服务可用性和响应时间。


<details>
  <summary>Details</summary>
Motivation: 混合专家架构因其参数效率被广泛用于扩展大型语言模型。本文揭示了一个隐含的负载均衡问题，当使用此架构的推理系统遇到特定类型的出-of-distribution提示时，会导致严重的资源分配不均，从而引起服务级别协议中的时间到首次响应时间的违反。

Method: 作者通过构建简单的重复令牌模式，设计了一种低成本的黑盒策略（称为RepetitionCurse），以利用混合专家路由器的行为缺陷。该策略不依赖于特定模型，可以在广泛部署的混合专家模型上进行测试。

Result: 实验结果表明，通过使用RepetitionCurse构造的对抗性提示，可以在Mixtral-8x7B等广泛部署的混合专家模型中，将端到端的推理延迟增加3.063倍，显著降低服务可用性。

Conclusion: 本文强调了混合专家路由器行为中的潜在安全性和性能风险，提出了一种新的威胁模式，并展示了这种模式导致的严重后果，为未来的系统设计和安全防护提供了重要参考。

Abstract: Mixture-of-Experts architectures have become the standard for scaling large language models due to their superior parameter efficiency. To accommodate the growing number of experts in practice, modern inference systems commonly adopt expert parallelism to distribute experts across devices. However, the absence of explicit load balancing constraints during inference allows adversarial inputs to trigger severe routing concentration. We demonstrate that out-of-distribution prompts can manipulate the routing strategy such that all tokens are consistently routed to the same set of top-$k$ experts, which creates computational bottlenecks on certain devices while forcing others to idle. This converts an efficiency mechanism into a denial-of-service attack vector, leading to violations of service-level agreements for time to first token. We propose RepetitionCurse, a low-cost black-box strategy to exploit this vulnerability. By identifying a universal flaw in MoE router behavior, RepetitionCurse constructs adversarial prompts using simple repetitive token patterns in a model-agnostic manner. On widely deployed MoE models like Mixtral-8x7B, our method increases end-to-end inference latency by 3.063x, degrading service availability significantly.

</details>


### [25] [Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?](https://arxiv.org/abs/2512.24044)
*Yuan Xin,Dingfan Chen,Linyi Yang,Michael Backes,Xiao Zhang*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As large language models (LLMs) are increasingly deployed, ensuring their safe use is paramount. Jailbreaking, adversarial prompts that bypass model alignment to trigger harmful outputs, present significant risks, with existing studies reporting high success rates in evading common LLMs. However, previous evaluations have focused solely on the models, neglecting the full deployment pipeline, which typically incorporates additional safety mechanisms like content moderation filters. To address this gap, we present the first systematic evaluation of jailbreak attacks targeting LLM safety alignment, assessing their success across the full inference pipeline, including both input and output filtering stages. Our findings yield two key insights: first, nearly all evaluated jailbreak techniques can be detected by at least one safety filter, suggesting that prior assessments may have overestimated the practical success of these attacks; second, while safety filters are effective in detection, there remains room to better balance recall and precision to further optimize protection and user experience. We highlight critical gaps and call for further refinement of detection accuracy and usability in LLM safety systems.

</details>


### [26] [FedLiTeCAN : A Federated Lightweight Transformer for Fast and Robust CAN Bus Intrusion Detection](https://arxiv.org/abs/2512.24088)
*Devika S,Pratik Narang,Tejasvi Alladi*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work implements a lightweight Transformer model for IDS in the domain of Connected and Autonomous Vehicles

</details>


### [27] [Spatial Discretization for Fine-Grain Zone Checks with STARKs](https://arxiv.org/abs/2512.24238)
*Sungmin Lee,Kichang Lee,Gyeongmin Han,JeongGil Ko*

Main category: cs.CR

TL;DR: 该研究利用基于网格的查找表，在固定STARK执行模型下，探讨了不同区域编码方式对精确度和证明成本的影响。研究发现，基于距离的编码方法在粗网格上具有更高的准确度，并且验证成本相对较低。


<details>
  <summary>Details</summary>
Motivation: 许多基于位置的服务依赖于点在多边形内的测试（PiP），而如何私密地执行PiP测试是一个挑战。因此，研究如何不同编码区域的方式影响准确性及证明成本变得尤为重要。

Method: 研究使用基于网格的查找表，并探索了基于距离的编码方式来提高PiP测试的准确性。

Result: 实验结果表明，在粗网格上基于距离的编码方法具有较高的准确度（最多60%的准确性提升），同时验证成本相对较低（大约1.4倍增加），使得区域编码成为高效零知识空间检查的关键。

Conclusion: 基于距离感知编码方法为高效执行隐私PiP测试提供了一种可能性，通过优化编码方式提高隐私保护下的位置服务应用效率。

Abstract: Many location-based services rely on a point-in-polygon test (PiP), checking whether a point or a trajectory lies inside a geographic zone. Since geometric operations are expensive in zero-knowledge proofs, privately performing the PiP test is challenging. In this paper, we answer the research questions of how different ways of encoding zones affect accuracy and proof cost by exploiting gridbased lookup tables under a fixed STARK execution model. Beyond a Boolean grid-based baseline that marks cells as in- or outside, we explore a distance-aware encoding approach that stores how far each cell is from a zone boundary and uses interpolation to reason within a cell. Our experiments on real-world data demonstrate that the proposed distance-aware approach achieves higher accuracy on coarse grids (max. 60%p accuracy gain) with only a moderate verification overhead (approximately 1.4x), making zone encoding the key lever for efficient zero-knowledge spatial checks.

</details>


### [28] [How Would Oblivious Memory Boost Graph Analytics on Trusted Processors?](https://arxiv.org/abs/2512.24255)
*Jiping Yu,Xiaowei Zhu,Kun Chen,Guanyu Feng,Yunyi Chen,Xiaoyu Fan,Wenguang Chen*

Main category: cs.CR

TL;DR: 该研究通过在处理器中集成隐蔽内存（OM），提高了图分析等易受访问模式攻击的应用性能，比基准系统快100倍。


<details>
  <summary>Details</summary>
Motivation: 描述了当前数据不可知论算法导致性能下降的问题，以图分析为例，提出使用OM来增强隐私保护。

Method: 通过与存储结构和算法共同设计的方式，实现了OM集成的处理器原型。

Result: 所提出的系统在同规模的OM下，性能比基线快100倍。

Conclusion: 本文研究结果表明，集成OM的处理器为其提供了增强隐私保护的新途径。

Abstract: Trusted processors provide a way to perform joint computations while preserving data privacy. To overcome the performance degradation caused by data-oblivious algorithms to prevent information leakage, we explore the benefits of oblivious memory (OM) integrated in processors, to which the accesses are unobservable by adversaries. We focus on graph analytics, an important application vulnerable to access-pattern attacks. With a co-design between storage structure and algorithms, our prototype system is 100x faster than baselines given an OM sized around the per-core cache which can be implemented on existing processors with negligible overhead. This gives insights into equipping trusted processors with OM.

</details>


### [29] [FedSecureFormer: A Fast, Federated and Secure Transformer Framework for Lightweight Intrusion Detection in Connected and Autonomous Vehicles](https://arxiv.org/abs/2512.24345)
*Devika S,Vishnu Hari,Pratik Narang,Tejasvi Alladi,F. Richard Yu*

Main category: cs.CR

TL;DR: 该研究提出了一种基于联邦学习的最少层Transformer编码器模型，用于连接和自主车辆领域的入侵检测。


<details>
  <summary>Details</summary>
Motivation: 随着无人驾驶汽车的发展，车辆安全成为重要议题，而现有的入侵检测模型往往较为复杂，本文旨在设计一种轻量级模型应用于车联网环境。

Method: 研究使用Transformer架构，构建了一种最小层的编码器模型，并结合联邦学习方法，在保持模型性能的同时，优化模型复杂度。

Result: 实验结果显示，该模型在保持较高准确率的情况下有效减少了模型复杂性，且能够适应多人参与的车联网场景下的数据分布。

Conclusion: 本文提出的方法为车联网下的轻量级、高效的入侵检测系统设计提供了新思路，有望在资源受限的设备中部署。

Abstract: This works presents an encoder-only transformer built with minimum layers for intrusion detection in the domain of Connected and Autonomous Vehicles using Federated Learning.

</details>


### [30] [FAST-IDS: A Fast Two-Stage Intrusion Detection System with Hybrid Compression for Real-Time Threat Detection in Connected and Autonomous Vehicles](https://arxiv.org/abs/2512.24391)
*Devika S,Vishnu Hari,Pratik Narang,Tejasvi Alladi,Vinay Chamola*

Main category: cs.CR

TL;DR: 该研究实现了一个多阶段的基于CAVs的IDS系统，并通过混合模型压缩技术适用于资源受限的环境，但缺乏具体细节和实验结果。


<details>
  <summary>Details</summary>
Motivation: 鉴于车联网在现实应用中的资源约束问题，本文旨在开发一种能在资源受限环境下有效运行的入侵检测系统（IDS），以提升车联网的安全性。

Method: 研究采用了混合模型压缩技术来优化和部署多阶段IDS，以适配CAVs（车联网）不同环境下的资源限制。

Result: 实现了一种适用于资源受限的CAVs多阶段IDS系统，但具体性能指标和验证实验未详细给出。

Conclusion: 未来的工作需要验证该IDS在不同资源受限环境下的实际性能，并进一步优化部署策略。

Abstract: We have implemented a multi-stage IDS for CAVs that can be deployed to resourec-constrained environments after hybrid model compression.

</details>


### [31] [SourceBroken: A large-scale analysis on the (un)reliability of SourceRank in the PyPI ecosystem](https://arxiv.org/abs/2512.24400)
*Biagio Montaruli,Serena Elisa Ponta,Luca Compagna,Davide Balzarotti*

Main category: cs.CR

TL;DR: 本文提出了一种针对SourceRank评分系统的可靠性分析，特别是针对恶意包伪装为可信包的欺骗攻击。研究结果显示，SourceRank在现实世界场景中不能可靠地区分良性包和恶意包，主要原因是它无法及时反映包的删除情况。


<details>
  <summary>Details</summary>
Motivation: 鉴于源代码托管平台中恶意包可能伪装成可信包的风险显著增加，本文旨在评估SourceRank评分系统的可靠性，并识别其在应对欺骗攻击方面的不足。

Method: 本文首先构建了一个威胁模型，以识别SourceRank中的每一项指标可能面临的规避方法。随后通过分析先进且权威的MalwareBench数据集以及实际的122,398个包数据集，研究器对比了良性包和恶意包的SourceRank分布。

Result: 研究发现，尽管历史数据表明良性包与恶意包之间有明显的区别，但实际世界数据中两者的分布重叠严重，主要是由SourceRank无法及时反映包的删除情况导致的。此外，URL混淆技术被发现在现实世界数据集中的使用率增加，并常与其它规避技术结合使用，从而显著提高了恶意包的SourceRank评分。

Conclusion: 本文得出了SourceRank在现实世界使用场景中不能可靠地区分良性包和恶意包的结论，并指出了URL混淆技术作为新兴威胁的重要性。

Abstract: SourceRank is a scoring system made of 18 metrics that assess the popularity and quality of open-source packages. Despite being used in several recent studies, none has thoroughly analyzed its reliability against evasion attacks aimed at inflating the score of malicious packages, thereby masquerading them as trustworthy. To fill this gap, we first propose a threat model that identifies potential evasion approaches for each metric, including the URL confusion technique, which can affect 5 out of the 18 metrics by leveraging a URL pointing to a legitimate repository potentially unrelated to the malicious package.
  Furthermore, we study the reliability of SourceRank in the PyPI ecosystem by analyzing the SourceRank distributions of benign and malicious packages in the state-of-the-art MalwareBench dataset, as well as in a real-world dataset of 122,398 packages. Our analysis reveals that, while historical data suggests a clear distinction between benign and malicious packages, the real-world distributions overlap significantly, mainly due to SourceRank's failure to timely reflect package removals. As a result, SourceRank cannot be reliably used to discriminate between benign and malicious packages in real-world scenarios, nor to select benign packages among those available on PyPI.
  Finally, our analysis reveals that URL confusion represents an emerging attack vector, with its prevalence increasing from 4.2% in MalwareBench to 7.0% in our real-world dataset. Moreover, this technique is often used alongside other evasion techniques and can significantly inflate the SourceRank metrics of malicious packages.

</details>


### [32] [GateChain: A Blockchain Based Application for Country Entry Exit Registry Management](https://arxiv.org/abs/2512.24416)
*Mohamad Akkad,Hüseyin Bodur*

Main category: cs.CR

TL;DR: GateChain是一种基于区块链的应用，通过分布式、不可变和加密可验证的账本来记录进出境事件，增强了数据的完整性和透明性，为授权机构提供了实时的访问控制和验证。


<details>
  <summary>Details</summary>
Motivation: 传统边境控制系统依赖集中式数据库，容易受到数据操纵并具有机构之间的低互通性。因此，需要一种新的方法来提高数据的完整性和透明性。

Method: GateChain采用了区块链技术，提供了一种分布式账本解决方案，确保了记录的不可篡改性和透明性。通过智能合约实现访问控制和验证机制。

Result: 研究表明，GateChain能够有效地增强数据完整性、可靠性和透明性，同时提供高效的实时访问控制和验证功能。

Conclusion: GateChain解决了现有边境控制系统的脆弱性和互通性问题，为增强边境管理提供了创新的解决方案。

Abstract: Recording entry and exit records for a country, with properties such as confidentiality, integrity, and auditability, is increasingly important due to rising international mobility and security requirements. Traditional border control systems, which rely on centralised databases, are vulnerable to data manipulation and have limited interoperability between institutions. This study presents GateChain, a blockchain-based application that addresses these vulnerabilities. GateChain aims to enhance data integrity, reliability, and transparency by recording entry and exit events on a distributed, immutable, and cryptographically verifiable ledger. The application provides real-time access control and verification for authorised institutions. This paper describes the architecture and security components of GateChain and evaluates its performance and security features.

</details>


### [33] [Document Data Matching for Blockchain-Supported Real Estate](https://arxiv.org/abs/2512.24457)
*Henrique Lin,Tiago Dias,Miguel Correia*

Main category: cs.CR

TL;DR: 该系统通过OCR、NLP和可验证凭证技术实现了房地产文档的自动提取、验证和管理，结合区块链技术增强了透明性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有房地产领域依赖大量手工文档处理与验证，导致效率低下且易受欺诈。本研究旨在开发一种自动化系统以提高文档处理效率，增强数据安全与透明度。

Method: 系统采用OCR、NLP技术进行文档提取和标准化处理，并使用验证凭证和区块链技术确保数据的一致性和可信性。包括一个基于合成数据集训练的OCR-NLP提取管道、一个用于凭证发放和管理的后端系统，以及一个支持发放者、持有者和验证者交互的前端。

Result: 实验结果显示，模型在多种文档类型上达到了具有竞争力的准确率，且整体流程可以显著缩短验证时间并保持可靠性。

Conclusion: 所提出的框架展示了在房地产交易领域简化流程、加强利益相关者信任以及实现高效可靠的数字过程的潜力。

Abstract: The real estate sector remains highly dependent on manual document handling and verification, making processes inefficient and prone to fraud. This work presents a system that integrates optical character recognition (OCR), natural language processing (NLP), and verifiable credentials (VCs) to automate document extraction, verification, and management. The approach standardizes heterogeneous document formats into VCs and applies automated data matching to detect inconsistencies, while the blockchain provides a decentralized trust layer that reinforces transparency and integrity. A prototype was developed that comprises (i) an OCR-NLP extraction pipeline trained on synthetic datasets, (ii) a backend for credential issuance and management, and (iii) a frontend supporting issuer, holder, and verifier interactions. Experimental results show that the models achieve competitive accuracy across multiple document types and that the end-to-end pipeline reduces verification time while preserving reliability. The proposed framework demonstrates the potential to streamline real estate transactions, strengthen stakeholder trust, and enable scalable, secure digital processes.

</details>


### [34] [Correctness of Extended RSA Public Key Cryptosystem](https://arxiv.org/abs/2512.24531)
*Dar-jen Chang,Suranjan Gautam*

Main category: cs.CR

TL;DR: 本文提出了一种新的方法，用于正式验证RSA公钥密码系统的正确性。


<details>
  <summary>Details</summary>
Motivation: 由于现有的证明方法在形式上验证RSA的正确性时存在局限，因此本文尝试提出一种替代方法。

Method: 本文通过探索N值的选择标准，推导出了特定条件下某些N值的有效性和哪些值可能不满足正确性要求的具体条件。

Result: 作者通过数学证明了在特定条件下，某些N值可以用于RSA加密方案，但并未涉及RSA的加密安全性。

Conclusion: 本文的主要结论是，N的选择在RSA中需要满足特定条件才能保证其正确性。

Abstract: This paper proposes an alternative approach to formally establishing the correctness of the RSA public key cryptosystem. The methodology presented herein deviates slightly from conventional proofs found in existing literature. Specifically, this study explores the conditions under which the choice of the positive integer N, a fundamental component of RSA, can be extended beyond the standard selection criteria. We derive explicit conditions that determine when certain values of N are valid for the encryption scheme and explain why others may fail to satisfy the correctness requirements. The scope of this paper is limited to the mathematical proof of correctness for RSA-like schemes, deliberately omitting issues related to the cryptographic security of RSA.

</details>


### [35] [SynRAG: A Large Language Model Framework for Executable Query Generation in Heterogeneous SIEM System](https://arxiv.org/abs/2512.24571)
*Md Hasan Saju,Austin Page,Akramul Azim,Jeff Gardiner,Farzaneh Abazari,Frank Eargle*

Main category: cs.CR

TL;DR: SynRAG是一种统一框架，能够从单一的平台无关规范中自动生成针对不同SIEM平台的具体查询，从而改善威胁检测和事件调查的效率，特别是在应对不同SIEM平台多样性的挑战时。


<details>
  <summary>Details</summary>
Motivation: 目前大规模企业中，安全操作中心（SOC）分析师需要监控和分析大量的日志和事件以识别潜在威胁。然而，SIEM平台的多样性（如Palo Alto Networks Qradar、Google SecOps、Splunk、Microsoft Sentinel和Elastic Stack）使得分析师难以全面监控这些平台并进行有效的威胁检测。

Method: SynRAG通过从一个通用的高层规范中自动生成特定于各个SIEM平台的查询，消除了分析师需要了解每种SIEM平台的具体查询语言的需求。该框架使用一种与平台无关的规范来统一生成针对不同SIEM平台的具体查询。

Result: 我们评估SynRAG时，相比于最先进的语言模型（包括GPT、Llama、DeepSeek、Gemma和Claude），SynRAG在跨SIEM威胁检测和事件调查中生成了质量更高的查询。

Conclusion: SynRAG为不同SIEM平台提供了一种统一的查询生成方法，从而提高了威胁检测和事件调查的效率和效果。

Abstract: Security Information and Event Management (SIEM) systems are essential for large enterprises to monitor their IT infrastructure by ingesting and analyzing millions of logs and events daily. Security Operations Center (SOC) analysts are tasked with monitoring and analyzing this vast data to identify potential threats and take preventive actions to protect enterprise assets. However, the diversity among SIEM platforms, such as Palo Alto Networks Qradar, Google SecOps, Splunk, Microsoft Sentinel and the Elastic Stack, poses significant challenges. As these systems differ in attributes, architecture, and query languages, making it difficult for analysts to effectively monitor multiple platforms without undergoing extensive training or forcing enterprises to expand their workforce. To address this issue, we introduce SynRAG, a unified framework that automatically generates threat detection or incident investigation queries for multiple SIEM platforms from a platform-agnostic specification. SynRAG can generate platformspecific queries from a single high-level specification written by analysts. Without SynRAG, analysts would need to manually write separate queries for each SIEM platform, since query languages vary significantly across systems. This framework enables seamless threat detection and incident investigation across heterogeneous SIEM environments, reducing the need for specialized training and manual query translation. We evaluate SynRAG against state-of-the-art language models, including GPT, Llama, DeepSeek, Gemma, and Claude, using Qradar and SecOps as representative SIEM systems. Our results demonstrate that SynRAG generates significantly better queries for crossSIEM threat detection and incident investigation compared to the state-of-the-art base models.

</details>


### [36] [Practical Traceable Over-Threshold Multi-Party Private Set Intersection](https://arxiv.org/abs/2512.24652)
*Le Yang,Weijing You,Huiyang He,Kailiang Ji,Jingqiang Lin*

Main category: cs.CR

TL;DR: 本文提出了一种新的阈值多方私有集合交集协议，能够提高效率，并增强了安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的多方私有集合交集（MP-PSI）协议在涉及多个参与者时存在计算成本高且容忍恶意行为者的数量有限的问题。

Method: 作者提出了两种新型的带有追踪性的阈值多方私有集合交集协议（T-OT-MP-PSI）：一种是高效带有追踪性的阈值多方私有集合交集协议（ET-OT-MP-PSI），结合了Shamir秘密共享和公开编程伪随机函数；另一种是增强安全性带有追踪性的阈值多方私有集合交集协议（ST-OT-MP-PSI），利用了公开线性评估协议，增强了安全性。

Result: ET-OT-MP-PSI 协议在 $n=5$，$t=3$，集合大小为 $2^{14}$ 的情况下，比起Mahdavi等人的协议实现了 $15056$ 倍的效率提升；ST-OT-MP-PSI 协议实现了 $505$ 倍的效率提升。

Conclusion: 这些新型协议通过减少假设并结合更有效的加密技术，大幅提高了效率，并增强了 MPC-PSI 协议的安全性。

Abstract: Multi-Party Private Set Intersection (MP-PSI) with threshold enhances the flexibility of MP-PSI by disclosing elements present in at least $t$ participants' sets, rather than requiring elements to appear in all $n$ sets. In scenarios where each participant is responsible for its dataset, e.g., digital forensics, MP-PSI with threshold should disclose both intersection elements and corresponding holders such that elements are traceable and the reliability of intersection is guaranteed. We refer to MP-PSI with threshold supporting traceability as Traceable Over-Threshold MP-PSI (T-OT-MP-PSI). However, research on such protocols remains limited, and existing work tolerates at most $t-2$ semi-honest participants at considerable computational cost. We propose two novel Traceable OT-MP-PSI protocols. The first, Efficient Traceable OT-MP-PSI (ET-OT-MP-PSI), combines Shamir's secret sharing with an oblivious programmable pseudorandom function, achieving significantly improved efficiency with resistance to at most $t-2$ semi-honest participants. The second, Security-enhanced Traceable OT-MP-PSI (ST-OT-MP-PSI), achieves security against up to $n-1$ semi-honest participants by further leveraging the oblivious linear evaluation protocol. Compared to Mahdavi et al.'s protocol, ours eliminate the assumption that certain special parties do not collude. Experimental results demonstrate significant improvements: for $n=5$, $t=3$, and sets of size $2^{14}$, ET-OT-MP-PSI achieves $15056\times$ speedup and ST-OT-MP-PSI achieves $505\times$ speedup over Mahdavi et al.'s protocol.

</details>


### [37] [MTSP-LDP: A Framework for Multi-Task Streaming Data Publication under Local Differential Privacy](https://arxiv.org/abs/2512.24899)
*Chang Liu,Junzhou Zhao*

Main category: cs.CR

TL;DR: MTSP-LDP 通过动态分配隐私预算和构建自适应的私有二叉树结构来处理复杂查询和多任务场景，同时通过预算免费机制提升整体实用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效处理复杂的查询并捕捉时间关联性，大多数数据保护机制依赖于可信第三方，MTSP-LDP旨在克服这些限制。

Method: MTSP-LDP 框架结合了时序关联分析的最优隐私预算分配算法、数据自适应的私有二叉树结构、跨时间戳的分组和平滑操作以及预算免费的多任务处理机制。

Result: MTSP-LDP 在多个实际数据集上的实验表明，其在多种流处理任务中保持了高实用性，远超现有方法。

Conclusion: MTSP-LDP 提出了一种新的隐私保护机制，能够有效处理复杂流查询并在不额外耗用隐私预算的情况下支持多种任务。

Abstract: The proliferation of streaming data analytics in data-driven applications raises critical privacy concerns, as directly collecting user data may compromise personal privacy. Although existing $w$-event local differential privacy (LDP) mechanisms provide formal guarantees without relying on trusted third parties, their practical deployment is hindered by two key limitations. First, these methods are designed primarily for publishing simple statistics at each timestamp, making them inherently unsuitable for complex queries. Second, they handle data at each timestamp independently, failing to capture temporal correlations and consequently degrading the overall utility. To address these issues, we propose MTSP-LDP, a novel framework for \textbf{M}ulti-\textbf{T}ask \textbf{S}treaming data \textbf{P}ublication under $w$-event LDP. MTSP-LDP adopts an \emph{Optimal Privacy Budget Allocation} algorithm to dynamically allocate privacy budgets by analyzing temporal correlations within each window. It then constructs a \emph{data-adaptive private binary tree structure} to support complex queries, which is further refined by cross-timestamp grouping and smoothing operations to enhance estimation accuracy. Furthermore, a unified \emph{Budget-Free Multi-Task Processing} mechanism is introduced to support a variety of streaming queries without consuming additional privacy budget. Extensive experiments on real-world datasets demonstrate that MTSP-LDP consistently achieves high utility across various streaming tasks, significantly outperforming existing methods.

</details>


### [38] [Towards Provably Secure Generative AI: Reliable Consensus Sampling](https://arxiv.org/abs/2512.24925)
*Yu Cui,Hang Fu,Sicheng Pan,Zhuoyu Sun,Yifei Liu,Yuhong Nie,Bo Ran,Baohan Huang,Xufeng Zhang,Haibin Zhang,Cong Zuo,Licheng Wang*

Main category: cs.CR

TL;DR: 本文提出了一种新的共识采样方法（RCS），旨在提供一种理论可控风险的生成式AI。经过实验验证，RCS在增强鲁棒性和实用性方面远超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的生成AI安全研究受到攻防动态驱动，导致了未知攻击的产生，为此研究探索了具有可证明安全性的生成AI以控制风险。然而，现有的Consensus Sampling（CS）算法依赖频繁的避免措施来防止不安全的输出，降低了其使用效率，且在不安全模型被恶意操控时会变得十分脆弱。因此，作者提出了一种名为Reliable Consensus Sampling（RCS）的新方法。

Method: RCS算法通过追溯接受概率来接受极端对抗行为，从而提高其鲁棒性。同时，该算法去除了CS算法中的避免措施需求。

Result: 理论证明，RCS可以维持可控的风险阈值。从而在鲁棒性和实用性方面都大幅超越了CS算法。

Conclusion: RCS为生成式AI带来了脱胎换骨的提升，同时有望促进生成式AI的可证明安全性发展。

Abstract: Existing research on generative AI security is primarily driven by mutually reinforcing attack and defense methodologies grounded in empirical experience. This dynamic frequently gives rise to previously unknown attacks that can circumvent current detection and prevention. This necessitates the continual updating of security mechanisms. Constructing generative AI with provable security and theoretically controllable risk is therefore necessary. Consensus Sampling (CS) is a promising algorithm toward provably secure AI. It controls risk by leveraging overlap in model output probabilities. However, we find that CS relies on frequent abstention to avoid unsafe outputs, which reduces utility. Moreover, CS becomes highly vulnerable when unsafe models are maliciously manipulated. To address these issues, we propose a new primitive called Reliable Consensus Sampling (RCS), that traces acceptance probability to tolerate extreme adversarial behaviors, improving robustness. RCS also eliminates the need for abstention entirely. We further develop a feedback algorithm to continuously and dynamically enhance the safety of RCS. We provide theoretical guarantees that RCS maintains a controllable risk threshold. Extensive experiments show that RCS significantly improves robustness and utility while maintaining latency comparable to CS. We hope this work contributes to the development of provably secure generative AI.

</details>
