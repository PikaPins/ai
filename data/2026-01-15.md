<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 11]
- [cs.AI](#cs.AI) [Total: 15]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [ABE-VVS: Attribute-Based Encrypted Volumetric Video Streaming](https://arxiv.org/abs/2601.08987)
*Mohammad Waquas Usmani,Susmit Shannigrahi,Michael Zink*

Main category: cs.CR

TL;DR: ABE-VVS框架通过选择性地对点云基于体积视频流进行坐标加密，实现了轻量级且有效的数字版权管理，减少了计算开销和延迟，同时提供了强烈的视觉扭曲以防止未经授权的观看。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决点云视频流中的数字版权管理问题，传统的全帧加密方法会增加计算开销，影响视频流服务的性能。ABE-VVS框架通过仅加密部分坐标的加密策略，减轻了这些问题，提供了更加高效和安全的解决方案。

Method: ABE-VVS采用基于属性的加选择性坐标加密方法，加密X,Y,Z中的一个或多个坐标。这种方法能够在保持视频流的实际观看体验的同时，显著降低加密和解密的时间，并且能够有效防止未经授权的查看。

Result: 实验结果显示，仅加密X坐标即可实现有效的数据混淆，同时将加密和解密时间分别缩短了50%和80%。另外，在云实验室测试床中对基于ABE的加密方案进行了评估，结果显示相较于传统的HTTP/HTTPS安全流加密方案和HTTP未加密方案，ABE-XYZ和ABE-XY方案减少了服务器和缓存的CPU负担，并保持了相似的缓存命中率。

Conclusion: ABE-VVS框架为点云视频流提供了有效的数字版权管理机制，能够显著减少计算开销和提升流媒体服务的效率。虽然引入了客户端CPU使用量的增加，但并不影响最终用户体验，并在版权管理、减少加密开销以及降低服务器和缓存负载方面提供了多项优势。

Abstract: This work introduces ABE-VVS, a framework that performs attribute based selective coordinate encryption for point cloud based volumetric video streaming, enabling lightweight yet effective digital rights management (DRM). Rather than encrypting entire point cloud frames, our approach encrypts only selected subsets of coordinates ($X, Y, Z$, or combinations), lowering computational overhead and latency while still producing strong visual distortion that prevents meaningful unauthorized viewing. Our experiments show that encrypting only the $X$ coordinates achieves effective obfuscation while reducing encryption and decryption times by up to 50% and 80%, respectively, compared to full-frame encryption.
  To our knowledge, this is the first work to provide a novel end-to-end evaluation of a DRM-enabled secure point cloud streaming system. We deployed a point cloud video streaming setup on the CloudLab testbed and evaluated three HTTP-based Attribute-Based Encryption (ABE) granularities - ABE-XYZ (encrypting all $X,Y,Z$ coordinates), ABE-XY, and ABE-X against conventional HTTPS/TLS secure streaming as well as an HTTP-only baseline without any security. Our streaming evaluation demonstrates that ABE-based schemes reduce server-side CPU load by up to 80% and cache CPU load by up to 63%, comparable to HTTP-only, while maintaining similar cache hit rates. Moreover, ABE-XYZ and ABE-XY exhibit lower client-side rebuffering than HTTPS, and ABE-X achieves zero rebuffering comparable to HTTP-only. Although ABE-VVS increases client-side CPU usage, the overhead is not large enough to affect streaming quality and is offset by its broader benefits, including simplified key revocation, elimination of per-client encryption, and reduced server and cache load.

</details>


### [2] [StegoStylo: Squelching Stylometric Scrutiny through Steganographic Stitching](https://arxiv.org/abs/2601.09056)
*Robert Dilworth*

Main category: cs.CR

TL;DR: 本文探讨了如何通过增强对抗样式攻击和优化隐写术，来对抗样式分析，以保护隐私并增强文本匿名性。


<details>
  <summary>Details</summary>
Motivation: 随着样式分析（stylometry）技术在多个领域中的应用逐渐广泛，它也需要面对各种潜在的滥用风险，尤其是在作者身份验证（authorship verification）方面。因此，研究者们探索了如何通过结合对抗样式攻击和隐写术（steganography）来增强文本的匿名性。

Method: 研究者首先改进了他们的对抗攻击工具$	extit{TraceTarnish}$，并展示了它可以更好地混淆样式分析系统。其次，他们研究了如何通过隐写术来掩盖作者的风格指纹，并量化了在改变一定程度的单词为零宽度Unicode字符的情况下，作者身份混淆的程度。最后，基于研究成果，研究者讨论了样式分析可能带来的隐私风险，并呼吁开发防御工具来应对这些风险。

Result: 研究者改进了$	extit{TraceTarnish}$，并展示了锐意的高压集成方法可减少33%以上的单词混淆。他们量化了隐写术对作者身份混淆的影响，并提出33%的覆盖率为一个明显的界限，证明了通过隐写术进行作者身份遮蔽的有效性。

Conclusion: 本文强调了对抗样式攻击和隐写术在保护隐私和匿名性方面的重要性。研究者认为，开发相应的防御工具（如$	extit{TraceTarnish}$）对于防止样式分析带来的潜在威胁至关重要。

Abstract: Stylometry--the identification of an author through analysis of a text's style (i.e., authorship attribution)--serves many constructive purposes: it supports copyright and plagiarism investigations, aids detection of harmful content, offers exploratory cues for certain medical conditions (e.g., early signs of dementia or depression), provides historical context for literary works, and helps uncover misinformation and disinformation. In contrast, when stylometry is employed as a tool for authorship verification--confirming whether a text truly originates from a claimed author--it can also be weaponized for malicious purposes. Techniques such as de-anonymization, re-identification, tracking, profiling, and downstream effects like censorship illustrate the privacy threats that stylometric analysis can enable. Building on these concerns, this paper further explores how adversarial stylometry combined with steganography can counteract stylometric analysis. We first present enhancements to our adversarial attack, $\textit{TraceTarnish}$, providing stronger evidence of its capacity to confound stylometric systems and reduce their attribution and verification accuracy. Next, we examine how steganographic embedding can be fine-tuned to mask an author's stylistic fingerprint, quantifying the level of authorship obfuscation achievable as a function of the proportion of words altered with zero-width Unicode characters. Based on our findings, steganographic coverage of 33% or higher seemingly ensures authorship obfuscation. Finally, we reflect on the ways stylometry can be used to undermine privacy and argue for the necessity of defensive tools like $\textit{TraceTarnish}$.

</details>


### [3] [KryptoPilot: An Open-World Knowledge-Augmented LLM Agent for Automated Cryptographic Exploitation](https://arxiv.org/abs/2601.09129)
*Xiaonan Liu,Zhihao Li,Xiao Lan,Hao Ren,Haizhou Wang,Xingshu Chen*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Capture-the-Flag (CTF) competitions play a central role in modern cybersecurity as a platform for training practitioners and evaluating offensive and defensive techniques derived from real-world vulnerabilities. Despite recent advances in large language models (LLMs), existing LLM-based agents remain ineffective on high-difficulty cryptographic CTF challenges, which require precise cryptanalytic knowledge, stable long-horizon reasoning, and disciplined interaction with specialized toolchains. Through a systematic exploratory study, we show that insufficient knowledge granularity, rather than model reasoning capacity, is a primary factor limiting successful cryptographic exploitation: coarse or abstracted external knowledge often fails to support correct attack modeling and implementation. Motivated by this observation, we propose KryptoPilot, an open-world knowledge-augmented LLM agent for automated cryptographic exploitation. KryptoPilot integrates dynamic open-world knowledge acquisition via a Deep Research pipeline, a persistent workspace for structured knowledge reuse, and a governance subsystem that stabilizes reasoning through behavioral constraints and cost-aware model routing. This design enables precise knowledge alignment while maintaining efficient reasoning across heterogeneous subtasks. We evaluate KryptoPilot on two established CTF benchmarks and in six real-world CTF competitions. KryptoPilot achieves a complete solve rate on InterCode-CTF, solves between 56 and 60 percent of cryptographic challenges on the NYU-CTF benchmark, and successfully solves 26 out of 33 cryptographic challenges in live competitions, including multiple earliest-solved and uniquely-solved instances. These results demonstrate the necessity of open-world, fine-grained knowledge augmentation and governed reasoning for scaling LLM-based agents to real-world cryptographic exploitation.

</details>


### [4] [Deep Learning-based Binary Analysis for Vulnerability Detection in x86-64 Machine Code](https://arxiv.org/abs/2601.09157)
*Mitchell Petingola*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While much of the current research in deep learning-based vulnerability detection relies on disassembled binaries, this paper explores the feasibility of extracting features directly from raw x86-64 machine code. Although assembly language is more interpretable for humans, it requires more complex models to capture token-level context. In contrast, machine code may enable more efficient, lightweight models and preserve all information that might be lost in disassembly. This paper approaches the task of vulnerability detection through an exploratory study on two specific deep learning model architectures and aims to systematically evaluate their performance across three vulnerability types. The results demonstrate that graph-based models consistently outperform sequential models, emphasizing the importance of control flow relationships, and that machine code contains sufficient information for effective vulnerability discovery.

</details>


### [5] [The Real Menace of Cloning Attacks on SGX Applications](https://arxiv.org/abs/2601.09273)
*Annika Wilde,Samira Briongos,Claudio Soriente,Ghassan Karame*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Trusted Execution Environments (TEEs) are gaining popularity as an effective means to provide confidentiality in the cloud. TEEs, such as Intel SGX, suffer from so-called rollback and cloning attacks (often referred to as forking attacks). Rollback attacks are enabled by the lack of freshness guarantees for sealed data; cloning attacks stem from the inability to determine if other instances of an enclave are running on the same platform. While rollback attacks have been extensively studied by the community, cloning attacks have been, unfortunately, less investigated. To address this gap, we extensively study and thoroughly analyze the susceptibility of 72 SGX-based proposals to cloning attacks. Our results show that roughly 20% of the analyzed proposals are insecure against cloning attacks-including those applications that rely on monotonic counters and are, therefore, secure against rollback attacks.

</details>


### [6] [Explainable Autoencoder-Based Anomaly Detection in IEC 61850 GOOSE Networks](https://arxiv.org/abs/2601.09287)
*Dafne Lozano-Paredes,Luis Bote-Curiel,Juan Ramón Feijóo-Martínez,Ismael Gómez-Talal,José Luis Rojo-Álvarez*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The IEC 61850 Generic Object-Oriented Substation Event (GOOSE) protocol plays a critical role in real-time protection and automation of digital substations, yet its lack of native security mechanisms can expose power systems to sophisticated cyberattacks. Traditional rule-based and supervised intrusion detection techniques struggle to detect protocol-compliant and zero-day attacks under significant class imbalance and limited availability of labeled data. This paper proposes an explainable, unsupervised multi-view anomaly detection framework for IEC 61850 GOOSE networks that explicitly separates semantic integrity and temporal availability. The approach employs asymmetric autoencoders trained only on real operational GOOSE traffic to learn distinct latent representations of sequence-based protocol semantics and timing-related transmission dynamics in normal traffic. Anomaly detection is implemented using reconstruction errors mixed with statistically grounded thresholds, enabling robust detection without specified attack types. Feature-level reconstruction analysis provides intrinsic explainability by directly linking detection outcomes to IEC 61850 protocol characteristics. The proposed framework is evaluated using real substation traffic for training and a public dataset containing normal traffic and message suppression, data manipulation, and denial-of-service attacks for testing. Experimental results show attack detection rates above 99% with false positives remaining below 5% of total traffic, demonstrating strong generalization across environments and effective operation under extreme class imbalance and interpretable anomaly attribution.

</details>


### [7] [Blue Teaming Function-Calling Agents](https://arxiv.org/abs/2601.09292)
*Greta Dolcetti,Giulio Zizzo,Sergio Maffeis*

Main category: cs.CR

TL;DR: 研究评估了四个开源LLM在三种不同攻击下的鲁棒性，并测量了八种不同防御措施的有效性。结果显示，这些模型默认情况下并不安全，且目前的防御措施在现实场景中亦不可行。


<details>
  <summary>Details</summary>
Motivation: 为了提高大型语言模型在实际应用中的安全性，研究者们进行了一次实验评估，以了解现有的开源LLM在遭受攻击时的表现，以及当前已有的防御措施的效果。

Method: 研究者选择了四个声称具备函数调用能力的开源LLM，并对它们进行了三种不同类型的攻击评估，同时测试了八种不同的防御措施。

Result: 研究结果表明，这些开源LLM在默认状态下并不具备安全性，现有的防御措施也远未达到实际应用的标准。

Conclusion: 研究结论强调了在实际应用大型语言模型之前，需要进一步提升模型的安全性，并完善有效的防御策略。

Abstract: We present an experimental evaluation that assesses the robustness of four open source LLMs claiming function-calling capabilities against three different attacks, and we measure the effectiveness of eight different defences. Our results show how these models are not safe by default, and how the defences are not yet employable in real-world scenarios.

</details>


### [8] [SpatialJB: How Text Distribution Art Becomes the "Jailbreak Key" for LLM Guardrails](https://arxiv.org/abs/2601.09321)
*Zhiyi Mou,Jingyuan Yang,Zeheng Qian,Wangze Ni,Tianfang Xiao,Ning Liu,Chen Zhang,Zhan Qin,Kui Ren*

Main category: cs.CR

TL;DR: 研究提出了名为SpatialJB的新攻击技术，能够绕过现有的大型语言模型防护措施，有效地生成有害内容。尽管增加了防御机制，SpatialJB依然保持了较高的成功率，揭示了现有防护的漏洞。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的依赖于自回归的逐词推理机制，它们在面对空间布局变化时缺乏鲁棒性，这造成了模型容易受到旨在扰乱其输出生成过程的攻击。为了填补这一安全漏洞，研究者提出了SpatialJB，以此提升大型语言模型的安全性。

Method: 研究者通过重排Token的位置来改变语言模型的输入结构，以此来触发L语言模型未能正确预测的空间布局漏洞，并利用这一特性来生成有害内容。

Result: 实验表明，SpatialJB攻击在对一系列领先的大语言模型测试中达到了近乎100%的成功率。即使在加入OpenAI Moderation API等高级防护措施后，其成功的概率仍然超过了75%，显著优于现有的营救技术。

Conclusion: SpatialJB的提出揭示了大型语言模型在处理空间布局方面的重要弱点，并强调了在大型语言模型安全研究中空间语义的重要性。此外，还提供了基本的防御策略以抵御SpatialJB的攻击，并评估了这些防御措施的有效性。

Abstract: While Large Language Models (LLMs) have powerful capabilities, they remain vulnerable to jailbreak attacks, which is a critical barrier to their safe web real-time application. Current commercial LLM providers deploy output guardrails to filter harmful outputs, yet these defenses are not impenetrable. Due to LLMs' reliance on autoregressive, token-by-token inference, their semantic representations lack robustness to spatially structured perturbations, such as redistributing tokens across different rows, columns, or diagonals. Exploiting the Transformer's spatial weakness, we propose SpatialJB to disrupt the model's output generation process, allowing harmful content to bypass guardrails without detection. Comprehensive experiments conducted on leading LLMs get nearly 100% ASR, demonstrating the high effectiveness of SpatialJB. Even after adding advanced output guardrails, like the OpenAI Moderation API, SpatialJB consistently maintains a success rate exceeding 75%, outperforming current jailbreak techniques by a significant margin. The proposal of SpatialJB exposes a key weakness in current guardrails and emphasizes the importance of spatial semantics, offering new insights to advance LLM safety research. To prevent potential misuse, we also present baseline defense strategies against SpatialJB and evaluate their effectiveness in mitigating such attacks. The code for the attack, baseline defenses, and a demo are available at https://anonymous.4open.science/r/SpatialJailbreak-8E63.

</details>


### [9] [CallShield: Secure Caller Authentication over Real-Time Audio Channels](https://arxiv.org/abs/2601.09327)
*Mouna Rabh,Yazan Boshmaf,Mashael Alsabah,Shammur Chowdhury,Mohamed Hefeeda,Issa Khalil*

Main category: cs.CR

TL;DR: CallShield 是一种无需依赖语音转录、互联网连接或受信任基础设施的新系统，能够在实时音频流中嵌入数据进行认证。


<details>
  <summary>Details</summary>
Motivation: 在当今通信安全日益重要的时代，CallShield 的目标是提供一种能够在音频层实时工作的认证系统，以确保通话安全。

Method: CallShield 使用实时神经水印技术，在 8 kHz 语音的 40 毫秒帧中嵌入和提取位数据。它采用低比特率的数据链路协议，实现基本的帧同步，并提供错误检测、纠正和恢复功能。

Result: CallShield 在实际电信环境中表现出色，在清晰音频中达到 99.2% 的认证成功率，即使在常见的失真情况下也能保持 95% 的成功率。系统在平均不到 63 秒的时间内完成认证过程，包括三次重传尝试。

Conclusion: CallShield 展现了其在实际应用中的高性能和可靠性，为安全实时通话认证提供了新的解决方案，确保了音频质量的同时也实现了信息的高效传输。

Abstract: We present CallShield, the first caller identity authentication system that operates entirely at the audio layer, without relying on speech transcription, internet connectivity, or trusted infrastructure. CallShield introduces a real-time neural watermarking technique that enables per-bit embedding and recovery within 40-millisecond frames of live 8 kHz speech. This capability allows CallShield to transform the real-time audio channel into a noisy serial communication medium. To ensure reliable data transmission, CallShield implements a low-bitrate data link protocol that provides basic frame synchronization along with error detection, correction, and recovery. For caller authentication, CallShield adopts a secure and lightweight symmetric-key protocol that relies on pairwise shared secrets among trusted contacts. The system completes the full authentication process in an average of 63 seconds, including up to three retransmission attempts, making it suitable for real-time deployment. Extensive experiments under realistic telephony conditions demonstrate that CallShield achieves an overall authentication success rates exceeding 99.2% on clean audio and over 95% under common distortions, aided by selective retransmission of failed messages. Additionally, CallShield maintains high audio quality, achieving PESQ scores above 4.2 and STOI scores above 0.94 on clean speech, and exhibits robustness across a wide range of channel distortions, validating its practical viability for secure, real-time caller authentication.

</details>


### [10] [A Systematic Security Analysis for Path-based Traceability Systems in RFID-Enabled Supply Chains](https://arxiv.org/abs/2601.09407)
*Fokke Heikamp,Lei Pan,Robin Doss,Rolando Trujillo-Rasua,Sushmita Ruj*

Main category: cs.CR

TL;DR: 本文分析了现有可追溯性解决方案的安全要求往往结构不良或不完整，导致关键漏洞未被解决。通过统一的安全框架，对比了17种可追溯性解决方案的安全声明并发现了一些缺陷，这是首次对可追溯性解决方案进行大规模的安全评估。


<details>
  <summary>Details</summary>
Motivation: 由于RFID和物联网技术的发展导致产品召回和防伪、篡改和偷盗的需求增加，因此提高了对可追溯性系统的重视，但同时这些系统也成为了黑客攻击的目标，本文为了解决现有可追溯性解决方案安全要求不明确的问题，提出了一个统一的安全框架进行大规模评估。

Method: 本文通过将现有的17种可追溯性解决方案的安全特性纳入一个统一的安全框架中，客观地比较了他们的安全声明，并识别了其中的缺陷和漏洞。

Result: 论文揭示了在现有可追溯性解决方案中存在的缺陷，实施了一种用于分析和比较其安全声明的方法，并进行了首次大规模的安全评估。

Conclusion: 本文标志着对可追溯性解决方案的大规模安全评估的开端，为改进这些系统提供了有价值的见解。

Abstract: Traceability systems have become prevalent in supply chains because of the rapid development of RFID and IoT technologies. These systems facilitate product recall and mitigate problems such as counterfeiting, tampering, and theft by tracking the manufacturing and distribution life-cycle of a product. Therefore, traceability systems are a defense mechanism against supply chain attacks and, consequently, have become a target for attackers to circumvent. For example, a counterfeiter may change the trace of a fake product for the trace of an authentic product, fooling the system into accepting a counterfeit product as legit and thereby giving a false sense of security.
  This systematic analysis starts with the observation that security requirements in existing traceability solutions are often unstructured or incomplete, leaving critical vulnerabilities unaddressed. We synthesized the properties of current state-of-the-art traceability solutions within a single security framework that allows us to analyze and compare their security claims. Using this framework, we objectively compared the security of $17$ traceability solutions and identified several weaknesses and vulnerabilities. This article reports on these flaws, the methodology we used to identify them, and the first security evaluation of traceability solutions on a large scale.

</details>


### [11] [SoK: Enhancing Cryptographic Collaborative Learning with Differential Privacy](https://arxiv.org/abs/2601.09460)
*Francesco Capano,Jonas Böhler,Benjamin Weggenmann*

Main category: cs.CR

TL;DR: 本文系统梳理了在多方计算和差分隐私结合的协作学习中的隐私、准确性和性能之间的权衡，提出了一个统一框架，分析了不同安全噪声采样技术的权衡，并建议了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题，多方在协作学习中不能直接共享数据。尽管多方计算和差分隐私可以分别提供输入和输出隐私保护，但如何高效结合二者以平衡隐私、准确性和性能是一个挑战。

Method: 作者提出一个统一框架，涵盖了CPCL常用阶段，并分析了不同安全噪声采样技术的权衡。此外，作者还实现了被识别的噪声采样技术并在广域网和局域网中评估了其计算和通信成本。

Result: 研究工作分析了不同噪声采样技术的实施挑战，并评估了它们在CPCL框架下的准确性和加密开销，为CPCL中的技术选型提供了重要参考。

Conclusion: 本文提出了一个统一的CPCL框架，对不同噪声采样技术进行了详细的分析，并指出了未来的研究方向，为更好的实现CPCL提供了指导。

Abstract: In collaborative learning (CL), multiple parties jointly train a machine learning model on their private datasets. However, data can not be shared directly due to privacy concerns. To ensure input confidentiality, cryptographic techniques, e.g., multi-party computation (MPC), enable training on encrypted data. Yet, even securely trained models are vulnerable to inference attacks aiming to extract memorized data from model outputs. To ensure output privacy and mitigate inference attacks, differential privacy (DP) injects calibrated noise during training. While cryptography and DP offer complementary guarantees, combining them efficiently for cryptographic and differentially private CL (CPCL) is challenging. Cryptography incurs performance overheads, while DP degrades accuracy, creating a privacy-accuracy-performance trade-off that needs careful design considerations. This work systematizes the CPCL landscape. We introduce a unified framework that generalizes common phases across CPCL paradigms, and identify secure noise sampling as the foundational phase to achieve CPCL. We analyze trade-offs of different secure noise sampling techniques, noise types, and DP mechanisms discussing their implementation challenges and evaluating their accuracy and cryptographic overhead across CPCL paradigms. Additionally, we implement identified secure noise sampling options in MPC and evaluate their computation and communication costs in WAN and LAN. Finally, we propose future research directions based on identified key observations, gaps and possible enhancements in the literature.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue](https://arxiv.org/abs/2601.08950)
*Mayank Sharma,Roy Pea,Hari Subramonyam*

Main category: cs.AI

TL;DR: 该研究介绍了基于知识构建理论的ConvoLearn数据集，该数据集包含中学地球科学的1250对双边对话，旨在改进LLM的对话行为，以促进知识构建。经过细调后的Mistral 7B在人类评估中显著优于基线模型和Claude Sonnet 4.5。


<details>
  <summary>Details</summary>
Motivation: 研究指出，现有的LLM在教育应用中存在一些基本的教育学限制，如倾向于揭示解决方案而不是支持对话式学习。为了改善这一点，作者提出了一种新的数据集ConvoLearn，旨在通过引入更符合知识构建理论的对话模式，促使LLM采用更多的知识构建策略。

Method: 研究人员构建了一个包含1250对15轮对话的半合成数据集，通过控制人类教师与模拟学生之间的互动生成。他们利用QLoRA方法对Mistral 7B进行了微调，并通过人类教师的评估来衡量其效果。

Result: 人类教师的评估结果显示，经过ConvoLearn数据集微调后的Mistral 7B在对话质量上显著优于基线版本和Claude Sonnet 4.5。

Conclusion: 这项工作为未来构建支持性AI导师提供了潜在框架，通过改善LLM的对话策略，使其更适于知识构建和对话式学习方法。

Abstract: In educational applications, LLMs exhibit several fundamental pedagogical limitations, such as their tendency to reveal solutions rather than support dialogic learning. We introduce ConvoLearn (https://huggingface.co/datasets/masharma/convolearn ), a dataset grounded in knowledge building theory that operationalizes six core pedagogical dimensions: cognitive engagement, formative assessment, accountability, cultural responsiveness, metacognition, and power dynamics. We construct a semi-synthetic dataset of 1250 tutor-student dialogues (20 turns each) in middle school Earth Science through controlled interactions between human teachers and a simulated student. Using QLoRA, we demonstrate that training on this dataset meaningfully shifts LLM behavior toward knowledge-building strategies. Human evaluation by 31 teachers shows our fine-tuned Mistral 7B (M = 4.10, SD = 1.03) significantly outperforms both its base version (M = 2.59, SD = 1.11) and Claude Sonnet 4.5 (M = 2.87, SD = 1.29) overall. This work establishes a potential framework to guide future development and evaluation of constructivist AI tutors.

</details>


### [13] [ART: Action-based Reasoning Task Benchmarking for Medical AI Agents](https://arxiv.org/abs/2601.08988)
*Ananya Mantravadi,Shivali Dalmia,Abhishek Mukherji*

Main category: cs.AI

TL;DR: 该研究引入ART基准测试，针对医疗AI在EHR上的行动导向推理能力进行评估，发现现有基准测试的局限性并在 retrieval、aggregation 和 threshold reasoning 方面暴露出GPT-4o-mini和Claude 3.5 Sonnet的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的医疗AI评估基准测试无法全面评估医疗AI在临床上的表现，特别是在EHR上的行动导向推理能力，因此研究者提出了ART基准测试来填补这一空白。

Method: 研究者通过实证研究方法，分析已有的基准测试，识别出主要的错误类别，设计了一个四阶段的管线来生成具有挑战性的任务：情景识别、任务生成、质量审计和评估。

Result: 使用ART基准测试评估GPT-4o-mini和Claude 3.5 Sonnet的表现，发现这些模型在数据检索方面有接近完美的表现，但在聚合和阈值推理方面存在较大差距。

Conclusion: ART基准测试有助于识别医疗AI行动导向推理的弱点，为开发更可靠、更有效的医疗AI系统提供了重要依据，以减轻认知负担和行政负担，支持高需求护理环境中的劳动力容量。

Abstract: Reliable clinical decision support requires medical AI agents capable of safe, multi-step reasoning over structured electronic health records (EHRs). While large language models (LLMs) show promise in healthcare, existing benchmarks inadequately assess performance on action-based tasks involving threshold evaluation, temporal aggregation, and conditional logic. We introduce ART, an Action-based Reasoning clinical Task benchmark for medical AI agents, which mines real-world EHR data to create challenging tasks targeting known reasoning weaknesses. Through analysis of existing benchmarks, we identify three dominant error categories: retrieval failures, aggregation errors, and conditional logic misjudgments. Our four-stage pipeline -- scenario identification, task generation, quality audit, and evaluation -- produces diverse, clinically validated tasks grounded in real patient data. Evaluating GPT-4o-mini and Claude 3.5 Sonnet on 600 tasks shows near-perfect retrieval after prompt refinement, but substantial gaps in aggregation (28--64%) and threshold reasoning (32--38%). By exposing failure modes in action-oriented EHR reasoning, ART advances toward more reliable clinical agents, an essential step for AI systems that reduce cognitive load and administrative burden, supporting workforce capacity in high-demand care settings

</details>


### [14] [Human-AI Co-design for Clinical Prediction Models](https://arxiv.org/abs/2601.09072)
*Jean Feng,Avni Kothari,Patrick Vossler,Andrew Bishara,Lucas Zier,Newton Addo,Aaron Kornblith,Yan Shuo Tan,Chandan Singh*

Main category: cs.AI

TL;DR: HACHI 提出了一种迭代的人工智能辅助框架，通过自动化处理临床记录中的概念探索与人工反馈相结合，加速了临床预测模型的开发，同时保持了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统方法开发临床预测模型效率低，人力物力消耗大，难以广泛应用于临床实践，尤其是在处理复杂且信息量巨大的非结构化临床记录时。

Method: HACHI 采用了一种迭代的 AI 助手框架，其过程中交替进行 AI 探索候选概念和人类专家反馈的过程。AI 从临床记录中自动发现潜在有意义的概念，然后再由专业人员评估，从而加速概念的选择过程。

Result: 在两个实际的预测任务（急性肾损伤和创伤性脑损伤）中，HACHI 比现有方法表现出更好性能，发现了传统 CPM 中未包含的新临床相关概念，并提高了模型在不同临床环境和时间范围的普遍适用性。

Conclusion: HACHI 证实了临床 AI 团队在促进 CPM 开发中不可或缺的作用，不仅加快了模型的开发过程，而且还能有效提高模型的质量和解释性。

Abstract: Developing safe, effective, and practically useful clinical prediction models (CPMs) traditionally requires iterative collaboration between clinical experts, data scientists, and informaticists. This process refines the often small but critical details of the model building process, such as which features/patients to include and how clinical categories should be defined. However, this traditional collaboration process is extremely time- and resource-intensive, resulting in only a small fraction of CPMs reaching clinical practice. This challenge intensifies when teams attempt to incorporate unstructured clinical notes, which can contain an enormous number of concepts. To address this challenge, we introduce HACHI, an iterative human-in-the-loop framework that uses AI agents to accelerate the development of fully interpretable CPMs by enabling the exploration of concepts in clinical notes. HACHI alternates between (i) an AI agent rapidly exploring and evaluating candidate concepts in clinical notes and (ii) clinical and domain experts providing feedback to improve the CPM learning process. HACHI defines concepts as simple yes-no questions that are used in linear models, allowing the clinical AI team to transparently review, refine, and validate the CPM learned in each round. In two real-world prediction tasks (acute kidney injury and traumatic brain injury), HACHI outperforms existing approaches, surfaces new clinically relevant concepts not included in commonly-used CPMs, and improves model generalizability across clinical sites and time periods. Furthermore, HACHI reveals the critical role of the clinical AI team, such as directing the AI agent to explore concepts that it had not previously considered, adjusting the granularity of concepts it considers, changing the objective function to better align with the clinical objectives, and identifying issues of data bias and leakage.

</details>


### [15] [Programming over Thinking: Efficient and Robust Multi-Constraint Planning](https://arxiv.org/abs/2601.09097)
*Derrick Goh Xin Deik,Quanyu Long,Zhengyuan Liu,Nancy F. Chen,Wenya Wang*

Main category: cs.AI

TL;DR: SCOPE 引入了一种新的框架，通过将查询特定的推理与通用代码执行分离，解决了多约束规划中的挑战，实现了高成功率和成本降低。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多约束规划问题时存在不足，SCOPE 旨在通过分离推理与执行，提高逻辑的一致性、确定性和复用性，同时减少成本和延迟。

Method: SCOPE 使用了分离查询特定推理和通用代码执行的方法，生成可重用的求解器函数，同时减少对输入参数的修改。

Result: SCOPE 在 TravelPlanner 测试中，使用 GPT-4o 达到了 93.1% 的成功率，比最佳基线 CoT 高出 61.6%，同时降低了 1.4 倍的推理成本和近 4.67 倍的时间。

Conclusion: SCOPE 为多约束规划提供了有效的解决方案，提高了性能并降低了成本和延迟。

Abstract: Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.

</details>


### [16] [DScheLLM: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large language Model](https://arxiv.org/abs/2601.09100)
*Lixiang Zhang,Chenggong Zhao,Qing Gao,Xiaoke Zhao,Gengyi Bai,Jinhu Lv*

Main category: cs.AI

TL;DR: 该研究提出了一种名为 DScheLLM 的动态调度方法，利用细调的大规模语言模型在快慢双系统架构下应对不同规模的干扰，通过华为 OpenPangu Embedded-7B 模型在混合推理模式下进行训练，实验证明了该方法在生成高质量调度方案方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的生产调度方法在面对动态干扰（如加工时间变化、机器可用性变化和任务插入）时存在固有的局限性，例如适应性较差和缺乏泛化能力。因此，研究提出 DScheLLM 方法，利用大规模语言模型来增强调度的智能性和适应性。

Method: 该方法通过构建一个基于大规模语言模型的统一框架来处理动态事件，利用来自运筹学求解器的确切调度数据生成快慢两种推理模式的训练数据。模型采用 LoRA 在混合推理模式下进行微调。

Result: 实验结果表明，快速推理模式可以高效地生成高质量的调度方案，而慢速推理模式可以产生求解器兼容且格式良好的决策输入。

Conclusion: 该研究被认为是最早将大规模语言模型应用于动态环境中的作业车间调度研究之一，展示了其在智能和自适应调度优化中的巨大潜力。

Abstract: Production scheduling is highly susceptible to dynamic disruptions, such as variations in processing times, machine availability, and unexpected task insertions. Conventional approaches typically rely on event-specific models and explicit analytical formulations, which limits their adaptability and generalization across previously unseen disturbances. To overcome these limitations, this paper proposes DScheLLM, a dynamic scheduling approach that leverages fine-tuned large language models within a dual-system (fast-slow) reasoning architecture to address disturbances of different scales. A unified large language model-based framework is constructed to handle dynamic events, where training datasets for both fast and slow reasoning modes are generated using exact schedules obtained from an operations research solver. The Huawei OpenPangu Embedded-7B model is subsequently fine-tuned under the hybrid reasoning paradigms using LoRA. Experimental evaluations on standard job shop scheduling benchmarks demonstrate that the fast-thinking mode can efficiently generate high-quality schedules and the slow-thinking mode can produce solver-compatible and well-formatted decision inputs. To the best of our knowledge, this work represents one of the earliest studies applying large language models to job shop scheduling in dynamic environments, highlighting their considerable potential for intelligent and adaptive scheduling optimization.

</details>


### [17] [AviationLMM: A Large Multimodal Foundation Model for Civil Aviation](https://arxiv.org/abs/2601.09105)
*Wenbin Li,Jingling Wu,Xiaoyong Lin. Jing Chen,Cong Chen*

Main category: cs.AI

TL;DR: 该论文提出了一种针对民航的大型多模态基础模型（AviationLMM），旨在整合民航领域的异构数据流，实现理解、推理、生成和主动应用。论文明确了现有AI解决方案与需求之间的差距，并描述了该模型的架构，同时还指出了进一步研究的关键机会，如数据获取、对齐和融合、预训练、推理、可信性、隐私保护以及面对缺失模态的鲁棒性等。


<details>
  <summary>Details</summary>
Motivation: 现有的民航人工智能解决方案存在问题，如数据孤岛、单一模态和局限性，难以整合异构数据，导致航空安全、效率和客户满意度受损。

Method: 该论文提出了一种新的人工智能模型，即大型多模态基础模型（AviationLMM），能够整合多种数据源，并通过跨模态对齐和融合来提升情境感知、适应性和实时决策支持能力。同时，指出了一系列研究机会以解决当前问题。

Result: 该模型可以根据输入的数据类型生产从情况概述、风险预警到预测诊断及多模态事件重建等形式多样的输出。论文还指出了为实现这一愿景需要解决的关键研究机会。

Conclusion: 通过描述AviationLMM的设计和挑战，旨在促进民航基础模型的进步，推动协调的研究努力，构建一个综合、可信且隐私保护的航空AI生态系统。

Abstract: Civil aviation is a cornerstone of global transportation and commerce, and ensuring its safety, efficiency and customer satisfaction is paramount. Yet conventional Artificial Intelligence (AI) solutions in aviation remain siloed and narrow, focusing on isolated tasks or single modalities. They struggle to integrate heterogeneous data such as voice communications, radar tracks, sensor streams and textual reports, which limits situational awareness, adaptability, and real-time decision support. This paper introduces the vision of AviationLMM, a Large Multimodal foundation Model for civil aviation, designed to unify the heterogeneous data streams of civil aviation and enable understanding, reasoning, generation and agentic applications. We firstly identify the gaps between existing AI solutions and requirements. Secondly, we describe the model architecture that ingests multimodal inputs such as air-ground voice, surveillance, on-board telemetry, video and structured texts, and performs cross-modal alignment and fusion, and produces flexible outputs ranging from situation summaries and risk alerts to predictive diagnostics and multimodal incident reconstructions. In order to fully realize this vision, we identify key research opportunities to address, including data acquisition, alignment and fusion, pretraining, reasoning, trustworthiness, privacy, robustness to missing modalities, and synthetic scenario generation. By articulating the design and challenges of AviationLMM, we aim to boost the civil aviation foundation model progress and catalyze coordinated research efforts toward an integrated, trustworthy and privacy-preserving aviation AI ecosystem.

</details>


### [18] [The AI Hippocampus: How Far are We From Human Memory?](https://arxiv.org/abs/2601.09113)
*Zixia Jia,Jiaqi Li,Yipeng Kang,Yuxuan Wang,Tong Wu,Quansen Wang,Xiaobo Wang,Shuyi Zhang,Junzhe Shen,Qing Li,Siyuan Qi,Yitao Liang,Di He,Zilong Zheng,Song-Chun Zhu*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型和多模态语言模型中记忆机制的架构和功能进化，分为隐式记忆、显式记忆和能动记忆三个主要框架。详细探讨了这些框架下的记忆整合、外部存储、以及在多模态设置中的应用。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）和多模态语言模型（MLLM）从静态预测器转变为能够实现持续学习和个人化推理的交互式系统，记忆机制的融入成为了它们架构和功能演进的核心主题。

Method: 作者通过梳理相关文献，将记忆机制分类为隐式记忆、显式记忆和能动记忆，并详细分析了每种记忆框架下的机制，讨论了相关的关键架构进步、基准任务和开放挑战。

Result: 研究构建了一个关于记忆机制的综合和结构化分类，并探讨了各种维度上的近期进展，包括记忆容量、对齐、事实一致性以及跨系统互操作性等问题。

Conclusion: 该综述为理解和设计更具适应性和和上下文相关的LLM和MLLM提供了新的视角，并指出了未来的研究方向和挑战。

Abstract: Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.

</details>


### [19] [Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback](https://arxiv.org/abs/2601.09182)
*JungMin Yun,JuneHyoung Kwon,MiHyeon Kim,YoungBin Kim*

Main category: cs.AI

TL;DR: 本文批评了现有的基于LLM的自动化评审，并提出了一个以人为核心的方法，通过LLM辅助的教育和反馈系统提升评审人的技能，进而增强评审质量，支持更可持续的学术生态系统。


<details>
  <summary>Details</summary>
Motivation: 现有的自动评审系统可能导致评审质量低下，本文旨在通过引入LLM辅助的教育和反馈机制，改进评审流程，提高评审质量，从而保障评审的可持续性。

Method: 本文提出了两个基于LLM的系统：一个为培养评审人长期能力的导师系统，另一个为帮助改进评审质量的反馈系统。通过这两个系统，旨在提升评审人的技能。

Result: 本文为增强学术评审的质量和可持续性提供了一种新颖的、以人为核心的方法，强调了LLM工具在支持评审教育和反馈中的潜力。

Conclusion: 本文认为，通过LLM辅助的教育与反馈机制，能够在维护高质量评审的基础上，增强评审人的专业能力，从而加强评审的可持续性。

Abstract: The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. We define the core principles of high-quality peer review and propose two complementary systems grounded in these foundations: (i) an LLM-assisted mentoring system that cultivates reviewers' long-term competencies, and (ii) an LLM-assisted feedback system that helps reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.

</details>


### [20] [Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models](https://arxiv.org/abs/2601.09260)
*Yan Liu,Feng Zhang,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Han Liu,Yangdong Deng*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: High-quality chain-of-thought has demonstrated strong potential for unlocking the reasoning capabilities of large language models. However, current paradigms typically treat the reasoning process as an indivisible sequence, lacking an intrinsic mechanism to quantify step-wise information gain. This granularity gap manifests in two limitations: inference inefficiency from redundant exploration without explicit guidance, and optimization difficulty due to sparse outcome supervision or costly external verifiers. In this work, we propose CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, quantifying the contribution of each step toward the ground-truth answer. Built on this formulation, CoT-Flow enables two complementary methodologies: flow-guided decoding, which employs a greedy flow-based decoding strategy to extract information-efficient reasoning paths, and flow-based reinforcement learning, which constructs a verifier-free dense reward function. Experiments on challenging benchmarks demonstrate that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance.

</details>


### [21] [Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants](https://arxiv.org/abs/2601.09264)
*Ziyi Shi,Xusen Guo,Hongliang Lu,Mingxing Peng,Haotian Wang,Zheng Zhu,Zhenning Li,Yuxuan Liang,Xinhu Zheng,Hai Yang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...

</details>


### [22] [RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering](https://arxiv.org/abs/2601.09269)
*Wencheng Ye,Liang Peng,Xiaoyang Yuan,Yi Bin,Pengpeng Zeng,Hengyu Jin,Heng Tao Shen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.

</details>


### [23] [$A^3$-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation](https://arxiv.org/abs/2601.09274)
*Jian Zhang,Yu He,Zhiyuan Wang,Zhangqi Wang,Kai He,Fangzhi Xu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the \textit{memory-driven} mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose $A^3$-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate $A^3$-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.

</details>


### [24] [STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models](https://arxiv.org/abs/2601.09281)
*Jingjing Zhou,Gaoxiang Cong,Li Su,Liang Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为STaR的参数自由方法，用于推理过程中的隐私保护。该方法通过语义感知检测敏感内容，安全提示前缀注入全局安全约束，轨迹感知抑制以动态阻断敏感内容，以及分层过滤以防止生成的敏感和同义敏感标记。这些机制共同实现了强隐私保护，同时保持小的有用性损失。


<details>
  <summary>Details</summary>
Motivation: 由于大型推理模型（LRMs）中的复杂推理链可能导致敏感信息的持续隐私泄露，现有的基于最终答案的大语言模型（LLMs）遗忘方法不足以保护隐私。因此，本文提出了一种新型的在线遗忘框架STaR，以确保推理过程中的全面隐私保护。

Method: STaR框架包括四部分：(1) 通过语义感知技术识别敏感内容；(2) 通过安全提示前缀注入全局安全约束；(3) 实施轨迹感知抑制以动态阻断敏感信息；(4) 使用 token 级自适应过滤来防止生成的敏感和同义敏感标记。此外，还提出了两种评估指标： Multi-Decoding Consistency Assessment (MCS) 和 Multi-Granularity Membership Inference Attack (MIA)，以衡量在不同密级和回答层面的隐私保护。

Result: 在R-TOFU基准测试上进行的实验表明，STaR方法能够在保持最小有用性损失的情况下实现全面和稳定的遗忘，为LRM中的隐私保留推理设立了新的标准。

Conclusion: STaR框架提供了一种实用的方法，能够在不牺牲准确性的情况下为复杂推理提供强隐私保护。该研究对大型推理模型的隐私保护和安全性具有重要意义。

Abstract: Large Reasoning Models (LRMs) have advanced automated multi-step reasoning, but their ability to generate complex Chain-of-Thought (CoT) trajectories introduces severe privacy risks, as sensitive information may be deeply embedded throughout the reasoning process. Existing Large Language Models (LLMs) unlearning approaches that typically focus on modifying only final answers are insufficient for LRMs, as they fail to remove sensitive content from intermediate steps, leading to persistent privacy leakage and degraded security. To address these challenges, we propose Sensitive Trajectory Regulation (STaR), a parameter-free, inference-time unlearning framework that achieves robust privacy protection throughout the reasoning process. Specifically, we first identify sensitive content via semantic-aware detection. Then, we inject global safety constraints through secure prompt prefix. Next, we perform trajectory-aware suppression to dynamically block sensitive content across the entire reasoning chain. Finally, we apply token-level adaptive filtering to prevent both exact and paraphrased sensitive tokens during generation. Furthermore, to overcome the inadequacies of existing evaluation protocols, we introduce two metrics: Multi-Decoding Consistency Assessment (MCS), which measures the consistency of unlearning across diverse decoding strategies, and Multi-Granularity Membership Inference Attack (MIA) Evaluation, which quantifies privacy protection at both answer and reasoning-chain levels. Experiments on the R-TOFU benchmark demonstrate that STaR achieves comprehensive and stable unlearning with minimal utility loss, setting a new standard for privacy-preserving reasoning in LRMs.

</details>


### [25] [Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing](https://arxiv.org/abs/2601.09282)
*Leszek Sliwko,Jolanta Mizeria-Pietraszko*

Main category: cs.AI

TL;DR: 该论文提出了一种基于自然语言处理的语义意图驱动调度范例，旨在简化集群工作负载的分配。通过结合大型语言模型和Kubernetes调度扩展器，实现对软亲和偏好注释的理解，展示了出色的准确性和调度质量。


<details>
  <summary>Details</summary>
Motivation: 目前，集群工作负载分配需要复杂的配置，造成了使用上的不便。为了解决这一问题，本文提出了一个语义意图驱动的调度框架，通过自然语言处理技术来理解和执行用户意图，从而提高调度的灵活性和可操作性。

Method: 论文利用了大型语言模型（LLM）并通过Kubernetes调度扩展器集成，开发了原型系统。该原型系统包括一个集群状态缓存和一个意图分析器（使用AWS Bedrock），用于解析自然语言中的调度提示注释，实现对软亲和偏好的理解和执行。

Result: 实证研究表明，该系统具有高LLM解析准确率（约95%的子集准确率），尤其是在顶级模型如Amazon Nova Pro/Premier和Mistral Pixtral Large的情况下，明显优于基线引擎。在六种场景的调度质量测试中，原型系统在复杂性和定量场景中表现优秀，并能妥善处理软偏好冲突。

Conclusion: 该论文的研究结果验证了使用大型语言模型进行可访问式调度的可行性，虽然面对同步LLM延迟的限制，建议采用异步处理机制以提高系统适应性，同时确认了语义软亲和性在简化工作负载编排中的有效性。

Abstract: Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.

</details>


### [26] [Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning](https://arxiv.org/abs/2601.09536)
*Dongjie Cheng,Yongqi Li,Zhixin Ma,Hongru Cai,Yupeng Hu,Wenjie Wang,Liqiang Nie,Wenjie Li*

Main category: cs.AI

TL;DR: 该研究提出了一种统一的生成式多模态推理方法，通过在推理过程中生成中间图像来统一多种多模态推理技能，并通过Omni-R1和Omni-R1-Zero模型验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态推理方法存在通用性有限的问题，主要通过单一任务特定的推理模式实现，难以适应多样化的多模态任务。为了解决这一问题，研究针对统一的多模态推理技能提出了生成性统一多模态推理方法。

Method: 该方法通过一个两阶段的SFT+RL框架（Omni-R1）实现多模态推理技能的统一。包括感知对齐损失和感知奖励等多项技术，并引入了不依赖多模态标注数据的Omni-R1-Zero模型。

Result: 实验表明，Omni-R1能够在多种多模态任务中实现统一的生成性推理，并且Omni-R1-Zero在某些任务上甚至超过了Omni-R1，展示了生成性多模态推理的有效性。

Conclusion: 此项研究为生成性多模态推理提供了一种新的思路，即从纯文本推理中逐步生成视觉化解释，从而解决了通用性问题，并证明了该方法在实际应用中的潜力。

Abstract: Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.

</details>
