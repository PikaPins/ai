<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 8]
- [cs.CR](#cs.CR) [Total: 12]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [SemanticALLI: Caching Reasoning, Not Just Responses, in Agentic Systems](https://arxiv.org/abs/2601.16286)
*Varun Chillara,Dylan Kline,Christopher Alvares,Evan Wooten,Huan Yang,Shlok Khetan,Cade Bauer,Tré Guillory,Tanishka Shah,Yashodhara Dhariwal,Volodymyr Pavlov,George Popstefanov*

Main category: cs.AI

TL;DR: 该研究提出了一种名为SemanticALLI的新架构，通过将生成过程分解为Analytic Intent Resolution和Visualization Synthesis，实现在AI管道中的冗余推理的缓存操作，显著降低了LLM的调用次数和延迟，提供了AI系统设计的一个实际案例。


<details>
  <summary>Details</summary>
Motivation: 现有AI管道在处理自然语言指令时，频繁地重新生成相同的中间逻辑，导致资源浪费。传统边界缓存方法无法有效解决这一问题。

Method: 引入SemanticALLI架构，将生成过程按分解为两次阶段处理：Analytic Intent Resolution和Visualization Synthesis，并通过优化缓存策略提升了中间表示的复用性。

Result: 该方法在实际应用中展示了显著的效果，相比传统的单阶段缓存方法，多阶段缓存可以达到83.10%的命中率，减少了4023次LLM调用，每调用的平均延迟仅为2.66ms。

Conclusion: 该研究表明，在特定的结构化检查点处实现缓存能够有效地提升AI管道的效率。

Abstract: Agentic AI pipelines suffer from a hidden inefficiency: they frequently reconstruct identical intermediate logic, such as metric normalization or chart scaffolding, even when the user's natural language phrasing is entirely novel. Conventional boundary caching fails to capture this inefficiency because it treats inference as a monolithic black box.
  We introduce SemanticALLI, a pipeline-aware architecture within Alli (PMG's marketing intelligence platform), designed to operationalize redundant reasoning. By decomposing generation into Analytic Intent Resolution (AIR) and Visualization Synthesis (VS), SemanticALLI elevates structured intermediate representations (IRs) to first-class, cacheable artifacts.
  The impact of caching within the agentic loop is substantial. In our evaluation, baseline monolithic caching caps at a 38.7% hit rate due to linguistic variance. In contrast, our structured approach allows for an additional stage, the Visualization Synthesis stage, to achieve an 83.10% hit rate, bypassing 4,023 LLM calls with a median latency of just 2.66 ms. This internal reuse reduces total token consumption, offering a practical lesson for AI system design: even when users rarely repeat themselves, the pipeline often does, at stable, structured checkpoints where caching is most reliable.

</details>


### [2] [SycoEval-EM: Sycophancy Evaluation of Large Language Models in Simulated Clinical Encounters for Emergency Care](https://arxiv.org/abs/2601.16529)
*Dongshen Peng,Yi Wang,Carl Preiksaitis,Christian Rose*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) show promise in clinical decision support yet risk acquiescing to patient pressure for inappropriate care. We introduce SycoEval-EM, a multi-agent simulation framework evaluating LLM robustness through adversarial patient persuasion in emergency medicine. Across 20 LLMs and 1,875 encounters spanning three Choosing Wisely scenarios, acquiescence rates ranged from 0-100\%. Models showed higher vulnerability to imaging requests (38.8\%) than opioid prescriptions (25.0\%), with model capability poorly predicting robustness. All persuasion tactics proved equally effective (30.0-36.0\%), indicating general susceptibility rather than tactic-specific weakness. Our findings demonstrate that static benchmarks inadequately predict safety under social pressure, necessitating multi-turn adversarial testing for clinical AI certification.

</details>


### [3] [LLM is Not All You Need: A Systematic Evaluation of ML vs. Foundation Models for text and image based Medical Classification](https://arxiv.org/abs/2601.16549)
*Meet Raval,Tejul Pandit,Dhvani Upadhyay*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The combination of multimodal Vision-Language Models (VLMs) and Large Language Models (LLMs) opens up new possibilities for medical classification. This work offers a rigorous, unified benchmark by using four publicly available datasets covering text and image modalities (binary and multiclass complexity) that contrasts traditional Machine Learning (ML) with contemporary transformer-based techniques. We evaluated three model classes for each task: Classical ML (LR, LightGBM, ResNet-50), Prompt-Based LLMs/VLMs (Gemini 2.5), and Fine-Tuned PEFT Models (LoRA-adapted Gemma3 variants). All experiments used consistent data splits and aligned metrics. According to our results, traditional machine learning (ML) models set a high standard by consistently achieving the best overall performance across most medical categorization tasks. This was especially true for structured text-based datasets, where the classical models performed exceptionally well. In stark contrast, the LoRA-tuned Gemma variants consistently showed the worst performance across all text and image experiments, failing to generalize from the minimal fine-tuning provided. However, the zero-shot LLM/VLM pipelines (Gemini 2.5) had mixed results; they performed poorly on text-based tasks, but demonstrated competitive performance on the multiclass image task, matching the classical ResNet-50 baseline. These results demonstrate that in many medical categorization scenarios, established machine learning models continue to be the most reliable option. The experiment suggests that foundation models are not universally superior and that the effectiveness of Parameter-Efficient Fine-Tuning (PEFT) is highly dependent on the adaptation strategy, as minimal fine-tuning proved detrimental in this study.

</details>


### [4] [LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents](https://arxiv.org/abs/2601.16649)
*Amin Rakhsha,Thomas Hehn,Pietro Mazzaglia,Fabio Valerio Massoli,Arash Behboodi,Tribhuvanesh Orekondy*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models can perform well on many isolated tasks, yet they continue to struggle on multi-turn, long-horizon agentic problems that require skills such as planning, state tracking, and long context processing. In this work, we aim to better understand the relative importance of advancing these underlying capabilities for success on such tasks. We develop an oracle counterfactual framework for multi-turn problems that asks: how would an agent perform if it could leverage an oracle to perfectly perform a specific task? The change in the agent's performance due to this oracle assistance allows us to measure the criticality of such oracle skill in the future advancement of AI agents. We introduce a suite of procedurally generated, game-like tasks with tunable complexity. These controlled environments allow us to provide precise oracle interventions, such as perfect planning or flawless state tracking, and make it possible to isolate the contribution of each oracle without confounding effects present in real-world benchmarks. Our results show that while some interventions (e.g., planning) consistently improve performance across settings, the usefulness of other skills is dependent on the properties of the environment and language model. Our work sheds light on the challenges of multi-turn agentic environments to guide the future efforts in the development of AI agents and language models.

</details>


### [5] [AgentsEval: Clinically Faithful Evaluation of Medical Imaging Reports via Multi-Agent Reasoning](https://arxiv.org/abs/2601.16685)
*Suzhong Fu,Jingqi Dong,Xuan Ding,Rui Sun,Yiming Yang,Shuguang Cui,Zhen Li*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Evaluating the clinical correctness and reasoning fidelity of automatically generated medical imaging reports remains a critical yet unresolved challenge. Existing evaluation methods often fail to capture the structured diagnostic logic that underlies radiological interpretation, resulting in unreliable judgments and limited clinical relevance. We introduce AgentsEval, a multi-agent stream reasoning framework that emulates the collaborative diagnostic workflow of radiologists. By dividing the evaluation process into interpretable steps including criteria definition, evidence extraction, alignment, and consistency scoring, AgentsEval provides explicit reasoning traces and structured clinical feedback. We also construct a multi-domain perturbation-based benchmark covering five medical report datasets with diverse imaging modalities and controlled semantic variations. Experimental results demonstrate that AgentsEval delivers clinically aligned, semantically faithful, and interpretable evaluations that remain robust under paraphrastic, semantic, and stylistic perturbations. This framework represents a step toward transparent and clinically grounded assessment of medical report generation systems, fostering trustworthy integration of large language models into clinical practice.

</details>


### [6] [LongCat-Flash-Thinking-2601 Technical Report](https://arxiv.org/abs/2601.16725)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chen Gao,Chen Zhang,Chengcheng Han,Chenhui Yang,Chuyu Zhang,Cong Chen,Cunguang Wang,Daoru Pan,Defei Bu,Dengchang Zhao,Di Xiu,Dishan Liu,Dongyu Ru,Dunwei Tu,Fan Wu,Fengcheng Yuan,Fengcun Li,Gang Xu,Guanyu Wu,Guoyuan Lin,Haibin Wang,Hansi Yang,Hao Yang,Haonan Yan,Haoxiang Ma,Haoxing Wen,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiacheng Zhang,Jiahong Zhou,Jiahuan Li,Jiaming Wang,Jian Yang,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiapeng Zhu,Jiaqi Sun,Jiarong Shi,Jiarui Zhao,Jingang Wang,Jinluan Yang,Jinrui Ding,Jinwei Xiao,Jiyuan He,Juncan Xu,Kefeng Zhang,Keheng Wang,Li Wei,Lianhui Ma,Lin Qiu,Lingbing Kong,Lingchuan Liu,Linsen Guo,Mengshen Zhu,Mengxia Shen,Mingyang Zhu,Peiguang Li,Peng Pei,Pengcheng Jia,Pengtao Zhang,Peng Zhao,Qi Gu,Qiong Huang,Qiyuan Duan,Quanchi Weng,Rongxiang Weng,Rongzhi Zhang,Rumei Li,Shanglin Lei,Shengnan An,Shijun Dai,Shuaikang Liu,Shuang Zhou,Shuo Wang,Songyuan Zhao,Tao Liang,Tianhao Hu,Tianze Chen,Wei Liu,Wei Shi,Wei Wang,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Wentao Chen,Wentao Shi,Xi Su,Xiangcheng Liu,Xiandi Ma,Xiangyu Xi,Xiangyuan Liu,Xiangzhou Huang,Xiao Liu,Xiaodong Cai,Xiaolong Chen,Xiaowei Shi,Xiaoyu Li,Xin Chen,Xingchen Liu,Xuan Huang,Xuezhi Cao,Xunliang Cai,Yan Chen,Yang Bai,Yang Liu,Yang Yang,Yang Zheng,Yaoming Wang,Yaoming Zhu,Yaqi Huo,Yanyu Chen,Yaorui Shi,Yerui Sun,Yi Zhang,Yihao Chen,Yi-Kai Zhang,Yifan Lu,Yifan Zhao,Yitao Zhai,Yongjing Yin,Yongwei Zhou,Youshao Xiao,Yuchuan Dai,Yuchen Xie,Yuchen Yu,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunfan Liang,Yunke Zhao,Yuwei Jiang,Yuxin Bian,Yuxin Chen,Yuxin Liu,Yue Xu,Yueqing Sun,Zeyang Yu,Zhao Yang,Zhengsheng Huang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhimin Lin,Zhiyuan Yao,Zhuofan Chen,Zhuowen Han,Zijian Zhang,Ziran Li,Ziwen Wang,Ziyuan Zhuang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.

</details>


### [7] [An Efficient Insect-inspired Approach for Visual Point-goal Navigation](https://arxiv.org/abs/2601.16806)
*Lu Yihe,Barbara Webb*

Main category: cs.AI

TL;DR: 该工作开发了一种模仿昆虫的新型智能体，通过结合处理关联学习和路径整合的昆虫大脑结构模型，实现了与标准基准任务类似的表现，但计算成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 研究模仿昆虫路径导航能力的方法，并将其应用于机器人视觉导向导航任务，以减少计算开销并提高任务执行的鲁棒性。

Method: 开发了新颖的昆虫启发式智能体，该智能体基于两种昆虫大脑结构的模型进行构建。

Result: 昆虫启发式智能体在视觉导向导航任务中的表现与最新的最优模型相当，但在计算成本上要低很多。并在一个更现实的模拟环境中进行了测试，显示出更强的鲁棒性。

Conclusion: 该方法验证了昆虫启发式模型在视觉导向导航任务中的有效性，为未来减少计算资源需求和提高导航能力提供了新思路。

Abstract: In this work we develop a novel insect-inspired agent for visual point-goal navigation. This combines abstracted models of two insect brain structures that have been implicated, respectively, in associative learning and path integration. We draw an analogy between the formal benchmark of the Habitat point-goal navigation task and the ability of insects to learn and refine visually guided paths around obstacles between a discovered food location and their nest. We demonstrate that the simple insect-inspired agent exhibits performance comparable to recent SOTA models at many orders of magnitude less computational cost. Testing in a more realistic simulated environment shows the approach is robust to perturbations.

</details>


### [8] [Reasoning Promotes Robustness in Theory of Mind Tasks](https://arxiv.org/abs/2601.16853)
*Ian B. de Haan,Peter van der Putten,Max van Duijn*

Main category: cs.AI

TL;DR: 本研究通过使用增强学习强化奖励训练的推理模型在理论拟态任务中的表现，发现这些模型的稳健性有所提升，但这一提升更可能是由于找到正确答案的稳健性增加，而非全新的理论拟态推理能力。


<details>
  <summary>Details</summary>
Motivation: 论文旨在探讨强化学习强化奖励训练的推理模型在理论拟态任务中的表现，以评估大型语言模型在社会认知行为方面的表现。

Method: 通过机器心理实验的新型改编以及现有基准测试的结果来分析模型在理论拟态任务中的表现。

Result: 研究发现，推理模型在各种提示变化和任务扰动中的表现更稳健，并且这种增益更有可能归因于找到正确答案的稳健性，而不是真正具备了新型的理论拟态推理能力。

Conclusion: 论文认为，在评估大型语言模型的社会认知行为时，应该更多关注模型的稳健性表现，而非其是否进行了全新的理论拟态推理。

Abstract: Large language models (LLMs) have recently shown strong performance on Theory of Mind (ToM) tests, prompting debate about the nature and true performance of the underlying capabilities. At the same time, reasoning-oriented LLMs trained via reinforcement learning with verifiable rewards (RLVR) have achieved notable improvements across a range of benchmarks. This paper examines the behavior of such reasoning models in ToM tasks, using novel adaptations of machine psychological experiments and results from established benchmarks. We observe that reasoning models consistently exhibit increased robustness to prompt variations and task perturbations. Our analysis indicates that the observed gains are more plausibly attributed to increased robustness in finding the correct solution, rather than to fundamentally new forms of ToM reasoning. We discuss the implications of this interpretation for evaluating social-cognitive behavior in LLMs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [9] [A New Paradigm for Trusted Respiratory Monitoring Via Consumer Electronics-grade Radar Signals](https://arxiv.org/abs/2601.16241)
*Xinyu Li,Jinyang Huang,Feng-Qi Cui,Meng Wang,Peng Zhao,Meng Li,Dan Guo,Meng Wang*

Main category: cs.CR

TL;DR: Tru-RM是基于VMD和对抗损失的加密方法，提出了一种新颖的可信赖呼吸监测框架，能够在准确监测呼吸的同时匿名化用户敏感身份信息。


<details>
  <summary>Details</summary>
Motivation: 随着雷达技术在呼吸监测领域的应用，如何在保证监测准确性的同时保护用户隐私成为了一个挑战。

Method: Tru-RM通过VMD分解信号并使用FPE和PTN分别实现身份信息的解耦和扰动后的呼吸监测。具体而言，AFD将原始雷达信号分解为通用呼吸成分、个人差异成分和其他无关成分，FPE通过大量的噪声和相位噪声算法分别抑制个人差异成分中的身份信息及其余无关成分，PTN则用于从扰动的信号中正确检测呼吸。

Result: 基于不同检测距离、呼吸模式和时间长度的大量实验显示，Tru-RM在保护用户敏感身份信息的匿名性和扰动后的呼吸波形检测准确性方面表现出优越性能。

Conclusion: Tru-RM提供了一种有效的方式，既保证了雷达呼吸监测的准确性，又能够在不损害呼吸监测质量的前提下保护了用户的隐私。

Abstract: Respiratory monitoring is an extremely important task in modern medical services. Due to its significant advantages, e.g., non-contact, radar-based respiratory monitoring has attracted widespread attention from both academia and industry. Unfortunately, though it can achieve high monitoring accuracy, consumer electronics-grade radar data inevitably contains User-sensitive Identity Information (USI), which may be maliciously used and further lead to privacy leakage. To track these challenges, by variational mode decomposition (VMD) and adversarial loss-based encryption, we propose a novel Trusted Respiratory Monitoring paradigm, Tru-RM, to perform automated respiratory monitoring through radio signals while effectively anonymizing USI. The key enablers of Tru-RM are Attribute Feature Decoupling (AFD), Flexible Perturbation Encryptor (FPE), and robust Perturbation Tolerable Network (PTN) used for attribute decomposition, identity encryption, and perturbed respiratory monitoring, respectively. Specifically, AFD is designed to decompose the raw radar signals into the universal respiratory component, the personal difference component, and other unrelated components. Then, by using large noise to drown out the other unrelated components, and the phase noise algorithm with a learning intensity parameter to eliminate USI in the personal difference component, FPE is designed to achieve complete user identity information encryption without affecting respiratory features. Finally, by designing the transferred generalized domain-independent network, PTN is employed to accurately detect respiration when waveforms change significantly. Extensive experiments based on various detection distances, respiratory patterns, and durations demonstrate the superior performance of Tru-RM on strong anonymity of USI, and high detection accuracy of perturbed respiratory waveforms.

</details>


### [10] [FC-GUARD: Enabling Anonymous yet Compliant Fiat-to-Cryptocurrency Exchanges](https://arxiv.org/abs/2601.16298)
*Shaoyu Li,Hexuan Yu,Md Mohaimin Al Barat,Yang Xiao,Y. Thomas Hou,Wenjing Lou*

Main category: cs.CR

TL;DR: FC-GUARD 是一种隐私保护的货币交易平台，能够在不泄露用户个人信息的情况下进行法定货币与加密货币的交易。该系统采用零知识证明和可验证凭证技术，同时确保符合与加密货币使用相关的监管要求。


<details>
  <summary>Details</summary>
Motivation: 当前的加密货币交易平台在提供便利的同时，也存在严重的隐私泄露问题，如真实身份信息和加密地址被暴露。FC-GUARD 的提出意在解决这一问题，为用户提供匿名交易的同时满足监管要求。

Method: FC-GUARD 采用了零知识证明和可验证凭证技术来实现加密货币交易过程中的隐私保护，并通过法定化反匿名机制来确保交易平台的监管合规性。

Result: 实验结果表明 FC-GUARD 在桌面和移动平台上具有实际部署的可能性，能够在保护用户隐私的前提下满足监管要求。

Conclusion: 我们验证了 FC-GUARD 系统在增强用户隐私保护的同时，依然能够符合与加密货币交易相关的法律法规要求。

Abstract: With the rise of decentralized finance, fiat-to-cryptocurrency exchange platforms have become popular entry points into the cryptocurrency ecosystem. However, these platforms frequently fail to ensure adequate privacy protection, as evidenced by real-world breaches that exposed personally identifiable information (PII) and crypto addresses. Such leaks enable adversaries to link real-world identities to cryptocurrency transactions, undermining the presumed anonymity of cryptocurrency use.
  We propose FC-GUARD, a privacy-preserving exchange system designed to preserve user anonymity without compromising regulatory compliance in the exchange of fiat currency for cryptocurrencies. Leveraging verifiable credentials and zero-knowledge proof techniques, FC-GUARD enables fiat-to-cryptocurrency exchanges without revealing users' PII or fiat account details. This breaks the linkage between users' real-world identities and their cryptocurrency addresses, thereby upholding anonymity, a fundamental expectation in the cryptocurrency ecosystem. In addition, FC-GUARD complies with key regulations over cryptocurrency usage, such as know-your-customer requirements and auditability for tax reporting obligations by integrating a lawful de-anonymization mechanism that allows the auditing authority to identify misbehaving users. This ensures regulatory compliance while defaulting to privacy protection. We implement our system on both desktop and mobile platforms, and our evaluation shows its feasibility for practical deployment.

</details>


### [11] [NOIR: Privacy-Preserving Generation of Code with Open-Source LLMs](https://arxiv.org/abs/2601.16354)
*Khoa Nguyen,Khiem Ton,NhatHai Phan,Issa Khalil,Khang Tran,Cristian Borcea,Ruoming Jin,Abdallah Khreishah,My T. Thai*

Main category: cs.CR

TL;DR: 本文提出了NOIR框架，该框架通过在客户端使用加密机制保护客户端提示和生成代码，防止云服务提供商的潜在威胁。


<details>
  <summary>Details</summary>
Motivation: 为了应对大型语言模型（LLM）驱动的代码生成带来的知识产权和数据安全风险，特别是云提供商能够观测到客户端的提示和生成代码，本文提出了NOIR框架，旨在保护这些敏感信息。

Method: NOIR框架包含客户端的编码器和解码器，用于将提示的嵌入传输给云以获得增强的嵌入，再通过本地解码生成代码。同时引入了一种基于词汇表的本地差分隐私保护机制和客户端的随机化分词器，在不影响代码生成质量的前提下，增强对重建攻击和频率分析攻击的防御。

Result: 使用开源LLM进行的广泛分析和结果显示，NOIR框架在开源评价基准上的表现显著优于现有基线，尤其是在Evalplus（MBPP和HumanEval，命中率为76.7和77.4）和BigCodeBench（命中率为38.7，仅比原始LLM低1.77%）的测试中。

Conclusion: NOIR框架在严格的隐私防护条件下显著提升了性能，尽管在某些细节上略有下降，但在整体上仍是最有效的方法之一，展示了保护知识产权和数据安全方面的实际效果。

Abstract: Although boosting software development performance, large language model (LLM)-powered code generation introduces intellectual property and data security risks rooted in the fact that a service provider (cloud) observes a client's prompts and generated code, which can be proprietary in commercial systems. To mitigate this problem, we propose NOIR, the first framework to protect the client's prompts and generated code from the cloud. NOIR uses an encoder and a decoder at the client to encode and send the prompts' embeddings to the cloud to get enriched embeddings from the LLM, which are then decoded to generate the code locally at the client. Since the cloud can use the embeddings to infer the prompt and the generated code, NOIR introduces a new mechanism to achieve indistinguishability, a local differential privacy protection at the token embedding level, in the vocabulary used in the prompts and code, and a data-independent and randomized tokenizer on the client side. These components effectively defend against reconstruction and frequency analysis attacks by an honest-but-curious cloud. Extensive analysis and results using open-source LLMs show that NOIR significantly outperforms existing baselines on benchmarks, including the Evalplus (MBPP and HumanEval, Pass@1 of 76.7 and 77.4), and BigCodeBench (Pass@1 of 38.7, only a 1.77% drop from the original LLM) under strong privacy against attacks.

</details>


### [12] [Ringmaster: How to juggle high-throughput host OS system calls from TrustZone TEEs](https://arxiv.org/abs/2601.16448)
*Richard Habeeb,Man-Ki Yoon,Hao Chen Zhong Shao*

Main category: cs.CR

TL;DR: Ringmaster框架通过结合Linux的io_uring技术与ARM TrustZone最小内核，让可信执行环境(TEEs)能异步访问可能不完全可信的丰富操作系统服务，同时保持高安全性和高效性。


<details>
  <summary>Details</summary>
Motivation: 许多安全关键系统依赖及时处理传感器输入以避免潜在的安全隐患，但与此同时，为了支持有用的特性，这些系统还需要包含庞大的操作系统，增加了安全漏洞的风险。因此，恶意用户一旦获得了监督权限，就能通过拒绝服务危及时间关键型程序。

Method: Ringmaster框架具体采用Linux的io_uring技术，使TEEs能够异步访问受限操作系统的丰富服务。当操作系统的服务被拒绝时，TEEs可以从受限操作系统的资源切换到ARM TrustZone平台上运行的精简内核，保证关键驱动程序的可用性。

Result: 实验结果显示，在搭载Raspberry Pi 4b的无人驾驶航空器上，Ringmaster实现了近乎1GiB/s的数据流速进入TEEs中，且对于非TEEs任务相比，吞吐量性能仅有0-3%的微小提升。

Conclusion: Ringmaster框架成功地实现了一个高效、安全的系统，显示出了其平衡安全与效率的优势，并为进一步研究提供了新思路。

Abstract: Many safety-critical systems require timely processing of sensor inputs to avoid potential safety hazards. Additionally, to support useful application features, such systems increasingly have a large rich operating system (OS) at the cost of potential security bugs. Thus, if a malicious party gains supervisor privileges, they could cause real-world damage by denying service to time-sensitive programs. Many past approaches to this problem completely isolate time-sensitive programs with a hypervisor; however, this prevents the programs from accessing useful OS services
  We introduce Ringmaster, a novel framework that enables enclaves or TEEs (Trusted Execution Environments) to asynchronously access rich, but potentially untrusted, OS services via Linux's io_uring. When service is denied by the untrusted OS, enclaves continue to operate on Ringmaster's minimal ARM TrustZone kernel with access to small, critical device drivers. This approach balances the need for secure, time-sensitive processing with the convenience of rich OS services. Additionally, Ringmaster supports large unmodified programs as enclaves, offering lower overhead compared to existing systems. We demonstrate how Ringmaster helps us build a working highly-secure system with minimal engineering. In our experiments with an unmanned aerial vehicle, Ringmaster achieved nearly 1GiB/sec of data into enclave on a Raspberry Pi4b, 0-3% throughput overhead compared to non-enclave tasks.

</details>


### [13] [Cutting the Gordian Knot: Detecting Malicious PyPI Packages via a Knowledge-Mining Framework](https://arxiv.org/abs/2601.16463)
*Wenbo Guo,Chengwei Liu,Ming Kang,Yiran Zhang,Jiahui Wu,Zhengzi Xu,Vinay Sachidananda,Yang Liu*

Main category: cs.CR

TL;DR: PyGuard是一款知识驱动的框架，通过转换现有检测工具的误报和漏报为有用的行为知识，结合层次模式挖掘、大规模语言模型和上下文推理，实现了对Python包中恶意行为的高准确检测。


<details>
  <summary>Details</summary>
Motivation: 现有检测工具由于依赖简单的语法规则而误报率高，无法区分合法与恶意的API调用。PyGuard旨在提供一种知识驱动的方法，以提高恶意包检测的准确性和泛化能力。

Method: PyGuard利用层次模式挖掘提取差异行为序列，使用大规模语言模型生成语义抽象，并结合精确模式匹配和上下文推理，构建了一个高效准确的检测系统。

Result: PyGuard在真实部署中准确率高达99.50%，误报为2，而在现有工具中误报则高达1,927-2,117。它还能在混淆代码中保持98.28%的准确率，并发现219个未知恶意包。此外，该方法显示出跨生态系统的适用性，NPM包上的准确率为98.07%。

Conclusion: PyGuard通过知识提取和模式识别提高了Python包检测的准确性和泛化能力，对防范通过包传播的恶意行为有重要意义。

Abstract: The Python Package Index (PyPI) has become a target for malicious actors, yet existing detection tools generate false positive rates of 15-30%, incorrectly flagging one-third of legitimate packages as malicious. This problem arises because current tools rely on simple syntactic rules rather than semantic understanding, failing to distinguish between identical API calls serving legitimate versus malicious purposes. To address this challenge, we propose PyGuard, a knowledge-driven framework that converts detection failures into useful behavioral knowledge by extracting patterns from existing tools' false positives and negatives. Our method utilizes hierarchical pattern mining to identify behavioral sequences that distinguish malicious from benign code, employs Large Language Models to create semantic abstractions beyond syntactic variations, and combines this knowledge into a detection system that integrates exact pattern matching with contextual reasoning. PyGuard achieves 99.50% accuracy with only 2 false positives versus 1,927-2,117 in existing tools, maintains 98.28% accuracy on obfuscated code, and identified 219 previously unknown malicious packages in real-world deployment. The behavioral patterns show cross-ecosystem applicability with 98.07% accuracy on NPM packages, demonstrating that semantic understanding enables knowledge transfer across programming languages.

</details>


### [14] [DeMark: A Query-Free Black-Box Attack on Deepfake Watermarking Defenses](https://arxiv.org/abs/2601.16473)
*Wei Song,Zhenchang Xing,Liming Zhu,Yulei Sui,Jingling Xue*

Main category: cs.CR

TL;DR: 研究人员提出了DeMark，一种无需查询的黑盒攻击框架，旨在挑战图像水印方案对深度伪造的抵抗力。DeMark利用编码器-解码器水印模型中的潜在空间漏洞，通过压缩感知过程稀疏化水印信号，从而降低水印检测准确性，同时保持自然的视觉质量。这项研究揭示了当前的水印方案在面临潜在空间操纵时的脆弱性，提出了对更稳健的水印方法的需求。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造的快速发展，防御框架依赖的水印方案的安全性成为亟待解决的问题。鉴于现有研究普遍假设水印方案是无损的，DeMark旨在验证这一假设，强调了开发更稳健水印方案的重要性。

Method: DeMark采用了一种针对图像水印方案的查询免费黑盒攻击方法。它利用编码器-解码器水印模型中的潜在空间漏洞，应用压缩感知算法对水印进行稀疏化处理，从而在保持真实性和自然视觉质量的同时，削弱了水印的检测效果。

Result: 针对8种最先进的水印方案，DeMark将水印检测成功率从100%降低到了32.9%，同时维护了图像的自然视觉质量。此外，评估表明图像超分辨率、稀疏水印和对抗训练等三项预防策略在提高水印稳健性方面基本无效。

Conclusion: 这项研究表明，当前的编码器-解码器水印方案对于潜在空间操作仍然存在脆弱性，需要开发更加稳健的水印方法来提升检测深度伪造的能力。

Abstract: The rapid proliferation of realistic deepfakes has raised urgent concerns over their misuse, motivating the use of defensive watermarks in synthetic images for reliable detection and provenance tracking. However, this defense paradigm assumes such watermarks are inherently resistant to removal. We challenge this assumption with DeMark, a query-free black-box attack framework that targets defensive image watermarking schemes for deepfakes. DeMark exploits latent-space vulnerabilities in encoder-decoder watermarking models through a compressive sensing based sparsification process, suppressing watermark signals while preserving perceptual and structural realism appropriate for deepfakes. Across eight state-of-the-art watermarking schemes, DeMark reduces watermark detection accuracy from 100% to 32.9% on average while maintaining natural visual quality, outperforming existing attacks. We further evaluate three defense strategies, including image super resolution, sparse watermarking, and adversarial training, and find them largely ineffective. These results demonstrate that current encoder decoder watermarking schemes remain vulnerable to latent-space manipulations, underscoring the need for more robust watermarking methods to safeguard against deepfakes.

</details>


### [15] [A High Performance and Efficient Post-Quantum Crypto-Processor for FrodoKEM](https://arxiv.org/abs/2601.16500)
*Kai Li,Jiahao Lu,Fu Yao,Guang Zeng,Dongsheng Liu,Shengfei Gu,Zhengpeng Zhao,Jiachen Wang*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: FrodoKEM is a lattice-based post-quantum key encapsulation mechanism (KEM). It has been considered for standardization by the International Organization for Standardization (ISO) due to its robust security profile. However, its hardware implementation exhibits a weakness of high latency and heavy resource burden, hindering its practical application. Moreover, diverse usage scenarios call for comprehensive functionality. To address these challenges, this paper presents a high-performance and efficient crypto-processor for FrodoKEM. A multiple-instruction overlapped execution scheme is introduced to enable efficient multi-module scheduling and minimize operational latency. Furthermore, a high-speed, reconfigurable parallel multiplier array is integrated to handle intensive matrix computations under diverse computation patterns, significantly enhancing hardware efficiency. In addition, a compact memory scheduling strategy shortens the lifespan of intermediate matrices, thereby reducing overall storage requirements. The proposed design provides full support for all FrodoKEM security levels and protocol phases. It consumes 13467 LUTs, 6042 FFs, and 14 BRAMs on an Artix-7 FPGA and achieves the fastest reported execution time. Compared with state-of-the-art hardware implementations, our design improves the area-time product (ATP) by 1.75-2.00 times.

</details>


### [16] [SafeThinker: Reasoning about Risk to Deepen Safety Beyond Shallow Alignment](https://arxiv.org/abs/2601.16506)
*Xianya Fang,Xianying Luo,Yadong Wang,Xiang Chen,Yu Tian,Zequn Sun,Rui Liu,Jun Fang,Naiqiang Tan,Yuanning Cui,Sheng-Jun Huang*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Despite the intrinsic risk-awareness of Large Language Models (LLMs), current defenses often result in shallow safety alignment, rendering models vulnerable to disguised attacks (e.g., prefilling) while degrading utility. To bridge this gap, we propose SafeThinker, an adaptive framework that dynamically allocates defensive resources via a lightweight gateway classifier. Based on the gateway's risk assessment, inputs are routed through three distinct mechanisms: (i) a Standardized Refusal Mechanism for explicit threats to maximize efficiency; (ii) a Safety-Aware Twin Expert (SATE) module to intercept deceptive attacks masquerading as benign queries; and (iii) a Distribution-Guided Think (DDGT) component that adaptively intervenes during uncertain generation. Experiments show that SafeThinker significantly lowers attack success rates across diverse jailbreak strategies without compromising utility, demonstrating that coordinating intrinsic judgment throughout the generation process effectively balances robustness and practicality.

</details>


### [17] [Eclipse Attacks on Ethereum's Peer-to-Peer Network](https://arxiv.org/abs/2601.16560)
*Ruisheng Shi,Yuxuan Liang,Zijun Guo,Qin Wang,Lina Lan,Chenfeng Wang,Zhuoyi Zheng*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Eclipse attacks isolate blockchain nodes by monopolizing their peer-to-peer connections. The attacks were extensively studied in Bitcoin (SP'15, SP'20, CCS'21, SP'23) and Monero (NDSS'25), but their practicality against Ethereum nodes remains underexplored, particularly in the post-Merge settings.
  We present the first end-to-end implementation of an eclipse attack targeting Ethereum (2.0 version) execution-layer nodes. Our attack exploits the bootstrapping and peer management logic of Ethereum to fully isolate a node upon restart. We introduce a multi-stage strategy that majorly includes (i) poisoning the node's discovery table via unsolicited messages, (ii) infiltrating Ethereum's DNS-based peerlist by identifying and manipulating the official DNS crawler, and (iii) hijacking idle incoming connection slots across the network to block benign connections. Our DNS list poisoning is the first in the cryptocurrency context and requires only 28 IP addresses over 100 days. Slots hijacking raises outgoing redirection success from 45\% to 95\%. We validate our approach through controlled experiments on Ethereum's Sepolia testnet and broad measurements on the mainnet. Our findings demonstrate that over 80\% of public nodes do not leave sufficient idle capacity for effective slots occupation, highlighting the feasibility and severity of the threat. We further propose concrete countermeasures and responsibly disclosed all findings to Ethereum's security team.

</details>


### [18] [Emerging Threats and Countermeasures in Neuromorphic Systems: A Survey](https://arxiv.org/abs/2601.16589)
*Pablo Sorrentino,Stjepan Picek,Ihsen Alouani,Nikolaos Athanasios Anagnostopoulos,Francesco Regazzoni,Lejla Batina,Tamalika Banerjee,Fatih Turkmen*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Neuromorphic computing mimics brain-inspired mechanisms through spiking neurons and energy-efficient processing, offering a pathway to efficient in-memory computing (IMC). However, these advancements raise critical security and privacy concerns. As the adoption of bio-inspired architectures and memristive devices increases, so does the urgency to assess the vulnerability of these emerging technologies to hardware and software attacks. Emerging architectures introduce new attack surfaces, particularly due to asynchronous, event-driven processing and stochastic device behavior. The integration of memristors into neuromorphic hardware and software implementations in spiking neural networks offers diverse possibilities for advanced computing architectures, including their role in security-aware applications. This survey systematically analyzes the security landscape of neuromorphic systems, covering attack methodologies, side-channel vulnerabilities, and countermeasures. We focus on both hardware and software concerns relevant to spiking neural networks (SNNs) and hardware primitives, such as Physical Unclonable Functions (PUFs) and True Random Number Generators (TRNGs) for cryptographic and secure computation applications. We approach this analysis from diverse perspectives, from attack methodologies to countermeasure strategies that integrate efficiency and protection in brain-inspired hardware. This review not only maps the current landscape of security threats but provides a foundation for developing secure and trustworthy neuromorphic architectures.

</details>


### [19] [From Transactions to Exploits: Automated PoC Synthesis for Real-World DeFi Attacks](https://arxiv.org/abs/2601.16681)
*Xing Su,Hao Wu,Hanzhong Liang,Yunlin Jiang,Yuxi Cheng,Yating Liu,Fengyuan Xu*

Main category: cs.CR

TL;DR: TracExp 是一个自动化的框架，通过交易跟踪逆向工程和代码生成功能，直接从链上攻击执行中合成可验证的概念证明 (PoC)，同时降低了合成成本，提高了攻击再现的效果。


<details>
  <summary>Details</summary>
Motivation: 目前在区块链系统中，攻击者利用合同漏洞进行快速、隐蔽的攻击，导致系统的分析和再现变得非常困难。这种攻击需要手动构建概念证明的过程耗时费力，且需求较高的专业知识，难以扩展。

Method: TracExp 通过从低级交易跟踪中恢复攻击者的逻辑，使用大语言模型的代码生成能力将其转换为可执行的漏洞利用，通过标志性执行上下文的本地化和引入新型双重反汇编器来生成语义增强的漏洞利用伪代码。

Result: TracExp 在过去 20 个月的 321 起实际攻击中，成功合成 PoC 达到了 93%，其中 58.78% 可直接验证，平均成本仅为每起案件 70 美分。TracExp 还使得大量未公开的概念证明得以释放，收到了 900 美元的悬赏金，显示出强大的实际影响。

Conclusion: TracExp 为理解和防范区块链攻击提供了一种新的方法，并且在概念验证合成的自动化和成本效益方面取得了显著进展。

Abstract: Blockchain systems are increasingly targeted by on-chain attacks that exploit contract vulnerabilities to extract value rapidly and stealthily, making systematic analysis and reproduction highly challenging. In practice, reproducing such attacks requires manually crafting proofs-of-concept (PoCs), a labor-intensive process that demands substantial expertise and scales poorly. In this work, we present the first automated framework for synthesizing verifiable PoCs directly from on-chain attack executions. Our key insight is that attacker logic can be recovered from low-level transaction traces via trace-driven reverse engineering, and then translated into executable exploits by leveraging the code-generation capabilities of large language models (LLMs). To this end, we propose TracExp, which localizes attack-relevant execution contexts from noisy, multi-contract traces and introduces a novel dual-decompiler to transform concrete executions into semantically enriched exploit pseudocode. Guided by this representation, TracExp synthesizes PoCs and refines them to preserve exploitability-relevant semantics. We evaluate TracExp on 321 real-world attacks over the past 20 months. TracExp successfully synthesizes PoCs for 93% of incidents, with 58.78% being directly verifiable, at an average cost of only \$0.07 per case. Moreover, TracExp enabled the release of a large number of previously unavailable PoCs to the community, earning a $900 bounty and demonstrating strong practical impact.

</details>


### [20] [Building a Robust Risk-Based Access Control System to Combat Ransomware's Capability to Encrypt: A Machine Learning Approach](https://arxiv.org/abs/2601.16795)
*Kenan Begovic,Abdulaziz Al-Ali,Qutaibah Malluhi*

Main category: cs.CR

TL;DR: 该研究通过实现出色的风险关联和解释能力，提供了一个基于函数的实时加密控制架构，能够有效识别和阻断恶意加密活动，同时不影响合法使用。


<details>
  <summary>Details</summary>
Motivation: 鉴于勒索软件及加密勒索的需求，作者识别到了现有控制手段的不足，旨在设计一种既能提高检测精度又能保持低资源消耗的加密控制方法。

Method: 作者提出了一种结合机器学习推理与强制访问控制的综合方法，在Linux内核中实现了细粒度的函数级加密行为跟踪，通过将函数_graph跟踪器与内核函数执行高分辨率轨迹相结合，建立了专用于分类模型的动态数据集。

Result: 研究构建了一个实验原型，证明了能够实时监控并决策对疑似加密行为的允许或拒绝，在不影响合法使用的同时，展现出与细粒度行为模型相应的检测准确性，同时强调了在企业部署中进一步减少硬件资源消耗的重要性。

Conclusion: 基于以上研究方法，该工作最终研发出了一种既能满足风险检测需求，又能保持系统性能的加密控制系统，为企业及生产环境中的Linux系统的安全性提供了新的可能。

Abstract: Ransomware core capability, unauthorized encryption, demands controls that identify and block malicious cryptographic activity without disrupting legitimate use. We present a probabilistic, risk-based access control architecture that couples machine learning inference with mandatory access control to regulate encryption on Linux in real time. The system builds a specialized dataset from the native ftrace framework using the function_graph tracer, yielding high-resolution kernel-function execution traces augmented with resource and I/O counters. These traces support both a supervised classifier and interpretable rules that drive an SELinux policy via lightweight booleans, enabling context-sensitive permit/deny decisions at the moment encryption begins. Compared to approaches centered on sandboxing, hypervisor introspection, or coarse system-call telemetry, the function-level tracing we adopt provides finer behavioral granularity than syscall-only telemetry while avoiding the virtualization/VMI overhead of sandbox-based approaches. Our current user-space prototype has a non-trivial footprint under burst I/O; we quantify it and recognize that a production kernel-space solution should aim to address this. We detail dataset construction, model training and rule extraction, and the run-time integration that gates file writes for suspect encryption while preserving benign cryptographic workflows. During evaluation, the two-layer composition retains model-level detection quality while delivering rule-like responsiveness; we also quantify operational footprint and outline engineering steps to reduce CPU and memory overhead for enterprise deployment. The result is a practical path from behavioral tracing and learning to enforceable, explainable, and risk-proportionate encryption control on production Linux systems.

</details>
