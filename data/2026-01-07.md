<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 11]
- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [SWaRL: Safeguard Code Watermarking via Reinforcement Learning](https://arxiv.org/abs/2601.02602)
*Neusha Javidnia,Ruisi Zhang,Ashish Kundu,Farinaz Koushanfar*

Main category: cs.CR

TL;DR: SWaRL 是一个通过利用低秩适应 (LoRA) 和强化学习的协同培训框架，在生成代码中嵌入独特且可验证的水印，以保护代码 LLM 拥有者的知识产权，同时保持水印的检测准确性和代码的功能性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在嵌入水印时依赖手动设计的转换规则或在推理时篡改令牌生成概率，可能导致编译错误。SWaRL 旨在保护代码知识产权并通过使用编译器反馈和共同训练的保密验证器建立的功能正确性，以及作为奖励信号以确保水印的检出性，来解决这些问题。

Method: SWaRL 使用了基于强化学习的协同培训框架，该框架通过低秩适应 (LoRA) 在微调期间加以利用。它结合了编译器反馈来确保功能正确性和保密验证器作为奖励信号以保持水印的检出性。

Result: 实验表明，SWaRL 的水印检测准确率高于现有方法，同时完全保持水印代码的功能性。采用 LoRA 的水印嵌入方式引导基础模型以水印特定的方式生成和解决代码，而没有显著的计算成本。此外，SWaRL 对重写和对抗性转换攻击表现出较强的鲁棒性。

Conclusion: SWaRL 提供了一个稳健且保留保真度的水印框架，用于保护代码 LLM 拥有者的知识产权，有效保护了代码的功能性和水印的检出性。

Abstract: We present SWaRL, a robust and fidelity-preserving watermarking framework designed to protect the intellectual property of code LLM owners by embedding unique and verifiable signatures in the generated output. Existing approaches rely on manually crafted transformation rules to preserve watermarked code functionality or manipulate token-generation probabilities at inference time, which are prone to compilation errors. To address these challenges, SWaRL employs a reinforcement learning-based co-training framework that uses compiler feedback for functional correctness and a jointly trained confidential verifier as a reward signal to maintain watermark detectability. Furthermore, SWaRL employs low-rank adaptation (LoRA) during fine-tuning, allowing the learned watermark information to be transferable across model updates. Extensive experiments show that SWaRL achieves higher watermark detection accuracy compared to prior methods while fully maintaining watermarked code functionality. The LoRA-based signature embedding steers the base model to generate and solve code in a watermark-specific manner without significant computational overhead. Moreover, SWaRL exhibits strong resilience against refactoring and adversarial transformation attacks.

</details>


### [2] [LAsset: An LLM-assisted Security Asset Identification Framework for System-on-Chip (SoC) Verification](https://arxiv.org/abs/2601.02624)
*Md Ajoad Hasan,Dipayan Saha,Khan Thamid Hasan,Nashmin Alam,Azim Uddin,Sujan Kumar Saha,Mark Tehranipoor,Farimah Farahmandi*

Main category: cs.CR

TL;DR: 该研究提出了一种名为LAsset的自动化框架，利用大型语言模型从硬件设计规范和寄存器传输级（RTL）描述中自动识别安全资产，显著提高了效率并支持了更安全的硬件开发。


<details>
  <summary>Details</summary>
Motivation: 现代SoC和IP设计的复杂性使得确保其安全性变得越来越困难。传统的资产识别方式需安全专家手动完成，耗时耗力。为解决这一问题，本文提出LAsset框架。

Method: LAsset框架通过结构和语义分析从硬件设计规范和RTL描述中识别主从安全资产，并推导出跨模块关系以系统地在设计层面描述安全依赖关系。

Result: 实验结果显示，该框架在SoC设计中能实现高达90%的召回率，在IP设计中能实现93%的召回率。自动化资产识别大幅减少了手动工作量，支持了更安全的硬件开发。

Conclusion: LAsset框架通过利用大型语言模型来自动化识别安全资产，显著提高了安全验证的效率与准确性，为硬件设计的安全性优化提供了新的途径。

Abstract: The growing complexity of modern system-on-chip (SoC) and IP designs is making security assurance difficult day by day. One of the fundamental steps in the pre-silicon security verification of a hardware design is the identification of security assets, as it substantially influences downstream security verification tasks, such as threat modeling, security property generation, and vulnerability detection. Traditionally, assets are determined manually by security experts, requiring significant time and expertise. To address this challenge, we present LAsset, a novel automated framework that leverages large language models (LLMs) to identify security assets from both hardware design specifications and register-transfer level (RTL) descriptions. The framework performs structural and semantic analysis to identify intra-module primary and secondary assets and derives inter-module relationships to systematically characterize security dependencies at the design level. Experimental results show that the proposed framework achieves high classification accuracy, reaching up to 90% recall rate in SoC design, and 93% recall rate in IP designs. This automation in asset identification significantly reduces manual overhead and supports a scalable path forward for secure hardware development.

</details>


### [3] [Adversarial Contrastive Learning for LLM Quantization Attacks](https://arxiv.org/abs/2601.02680)
*Dinghong Song,Zhiwei Xu,Hai Wan,Xibin Zhao,Pengfei Su,Dong Li*

Main category: cs.CR

TL;DR: 该研究提出了一种名为对抗对比学习（ACL）的新颖量化攻击方法，通过最大化良性与有害响应概率之间的差距，显著提高了攻击的成功率。


<details>
  <summary>Details</summary>
Motivation: 近年来研究发现，即使是全精度的好模型在量化后也可能表现出恶意行为，因此研究高效且安全的量化方法变得尤为重要。

Method: ACL方法将攻击目标公式化为三元对比损失，并结合投影梯度下降的两阶段分布式细调策略来确保优化的稳定性和效率。

Result: 通过广泛的实验，ACL方法在拒绝过度请求、逃狱和广告注入攻击方面分别达到了86.00%、97.69%和92.40%的成功率，分别比最先进的方法高出44.67%、18.84%和50.80%。

Conclusion: 该研究证明了ACL方法的有效性，并表明这种方法能够显著提升模型在量化后的攻击抵抗能力。

Abstract: Model quantization is critical for deploying large language models (LLMs) on resource-constrained hardware, yet recent work has revealed severe security risks that benign LLMs in full precision may exhibit malicious behaviors after quantization. In this paper, we propose Adversarial Contrastive Learning (ACL), a novel gradient-based quantization attack that achieves superior attack effectiveness by explicitly maximizing the gap between benign and harmful responses probabilities. ACL formulates the attack objective as a triplet-based contrastive loss, and integrates it with a projected gradient descent two-stage distributed fine-tuning strategy to ensure stable and efficient optimization. Extensive experiments demonstrate ACL's remarkable effectiveness, achieving attack success rates of 86.00% for over-refusal, 97.69% for jailbreak, and 92.40% for advertisement injection, substantially outperforming state-of-the-art methods by up to 44.67%, 18.84%, and 50.80%, respectively.

</details>


### [4] [Privacy-Preserving AI-Enabled Decentralized Learning and Employment Records System](https://arxiv.org/abs/2601.02720)
*Yuqiao Xu,Mina Namazi,Sahith Reddy Jalapally,Osama Zafar,Youngjin Yoo,Erman Ayday*

Main category: cs.CR

TL;DR: 该论文提出了一种私密保护的、基于AI的去中心化学习和就业记录系统，该系统能够接受数字签名的教育机构转录记录，并在一个受信任的执行环境中通过自然语言处理管道分析正式和非正式的学习记录以自动生成可验证的技能证书，以实现安全的教育和就业证书管理，确保隐私并减少职业匹配中的筛选偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的基于区块链的平台普遍缺少自动化技能证书生成和非结构化学习证明的整合能力，因此该论文旨在弥补这些不足。

Method: 该系统通过接受数字化签名的教育机构文档和在受信任执行环境中使用自然语言处理管道来内置技能证书。该系统在迁移过程中确保敏感信息的私密性和不可伪造性。

Result: NLP组件在样例数据上的评估结果显示，映射遵循了验证的Syllabus到O*NET方法，且重复运行的稳定性测试观察到技能排名前五项的<5%差异。而且提供了形式化安全声明和证明草图，确保了证书的不可伪造性和敏感信息的保密。

Conclusion: 该系统支持安全的教育和就业证书管理，并能够提供稳定的技能自动提取和可验证的正式文凭验证，这对于建立一个隐私保护的分散框架至关重要。

Abstract: Learning and Employment Record (LER) systems are emerging as critical infrastructure for securely compiling and sharing educational and work achievements. Existing blockchain-based platforms leverage verifiable credentials but typically lack automated skill-credential generation and the ability to incorporate unstructured evidence of learning. In this paper,a privacy-preserving, AI-enabled decentralized LER system is proposed to address these gaps. Digitally signed transcripts from educational institutions are accepted, and verifiable self-issued skill credentials are derived inside a trusted execution environment (TEE) by a natural language processing pipeline that analyzes formal records (e.g., transcripts, syllabi) and informal artifacts. All verification and job-skill matching are performed inside the enclave with selective disclosure, so raw credentials and private keys remain enclave-confined. Job matching relies solely on attested skill vectors and is invariant to non-skill resume fields, thereby reducing opportunities for screening bias.The NLP component was evaluated on sample learner data; the mapping follows the validated Syllabus-to-O*NET methodology,and a stability test across repeated runs observed <5% variance in top-ranked skills. Formal security statements and proof sketches are provided showing that derived credentials are unforgeable and that sensitive information remains confidential. The proposed system thus supports secure education and employment credentialing, robust transcript verification,and automated, privacy-preserving skill extraction within a decentralized framework.

</details>


### [5] [Quality Degradation Attack in Synthetic Data](https://arxiv.org/abs/2601.02947)
*Qinyi Liu,Dong Liu,Farhad Vadiee,Mohammad Khalil,Pedro P. Vergara Barrios*

Main category: cs.CR

TL;DR: 研究针对数据生成过程中可能发生的质量下降攻击，发现即使轻微的数据篡改也能严重影响合成数据的质量，强调了需结合隐私保护和完整性验证来提升合成数据框架的可靠性和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的合成数据生成研究大多关注数据接收方发起的隐私威胁，而忽视了数据所有者、合成数据提供商或潜在入侵者可能发起的完整性威胁。因此本文旨在填补这一空白，探讨此类攻击的有效性。

Method: 本文建立了一种相应的威胁模型，并通过实证分析在真实数据上进行标签翻转和基于特征重要性的干预，评估这些操作对生成合成数据质量的影响。

Result: 实验结果显示，即使是轻微的数据篡改也能显著降低合成数据下游预测性能并增加统计差异，揭示了合成数据生成过程中的脆弱性。

Conclusion: 研究指出，需要在合成数据分享框架中集成完整性验证和鲁棒性机制，以确保其可靠性和可信度，结合隐私保护的同时，提升数据保护综合能力。

Abstract: Synthetic Data Generation (SDG) can be used to facilitate privacy-preserving data sharing. However, most existing research focuses on privacy attacks where the adversary is the recipient of the released synthetic data and attempts to infer sensitive information from it. This study investigates quality degradation attacks initiated by adversaries who possess access to the real dataset or control over the generation process, such as the data owner, the synthetic data provider, or potential intruders. We formalize a corresponding threat model and empirically evaluate the effectiveness of targeted manipulations of real data (e.g., label flipping and feature-importance-based interventions) on the quality of generated synthetic data. The results show that even small perturbations can substantially reduce downstream predictive performance and increase statistical divergence, exposing vulnerabilities within SDG pipelines. This study highlights the need to integrate integrity verification and robustness mechanisms, alongside privacy protection, to ensure the reliability and trustworthiness of synthetic data sharing frameworks.

</details>


### [6] [Exploring Blockchain Interoperability: Frameworks, Use Cases, and Future Challenges](https://arxiv.org/abs/2601.02949)
*Stanly Wilson,Kwabena Adu-Duodu,Yinhao Li,Ellis Solaiman,Omer Rana,Rajiv Ranjan*

Main category: cs.CR

TL;DR: 该论文讨论了区块链平台的互操作性解决方案，强调了它们连接异构区块链的能力，并阐述了互操作性的重要性和益处。


<details>
  <summary>Details</summary>
Motivation: 随着更多实体将其应用程序迁移到区块链，产生大量数据并使数据共享成为必要，因此研究和开发了互操作性解决方案来连接不同区块链。

Method: 论文探讨了几种提供互操作性的区块链平台，并通过案例研究展示了互操作性的价值。

Result: 介绍了几种提供互操作性的区块链平台，并分析了互操作性的需求。

Conclusion: 提出了互操作性领域需要解决的几个关键问题。

Abstract: Trust between entities in any scenario without a trusted third party is very difficult, and trust is exactly what blockchain aims to bring into the digital world with its basic features. Many applications are moving to blockchain adoption, enabling users to work in a trustworthy manner. The early generations of blockchain have a problem; they cannot share information with other blockchains. As more and more entities move their applications to the blockchain, they generate large volumes of data, and as applications have become more complex, sharing information between different blockchains has become a necessity. This has led to the research and development of interoperable solutions allowing blockchains to connect together. This paper discusses a few blockchain platforms that provide interoperable solutions, emphasising their ability to connect heterogeneous blockchains. It also discusses a case study scenario to illustrate the importance and benefits of using interoperable solutions. We also present a few topics that need to be solved in the realm of interoperability.

</details>


### [7] [Selfish Mining in Multi-Attacker Scenarios: An Empirical Evaluation of Nakamoto, Fruitchain, and Strongchain](https://arxiv.org/abs/2601.02984)
*Martin Perešíni,Tomáš Hladký,Jakub Kubík,Ivan Homoliak*

Main category: cs.CR

TL;DR: 本文提出了一种随机模拟框架，用于分析包括Fruitchain和Strongchain在内的多种共识协议中的多攻击者自私挖掘攻击。与以往研究仅关注单一攻击者的情况不同，本文探讨了两个及以上攻击者的情形，并发现了新的阈值。


<details>
  <summary>Details</summary>
Motivation: 传统的自私挖掘研究主要集中在单一自私矿工的情景下，忽略了多攻击者的情况。本文旨在填补这一研究空白，通过建立随机模拟框架来增强区块链安全。

Method: 本文采用了随机模拟框架，构建了不同共识协议（如PoW Nakamoto共识、Fruitchain、Strongchain）的模型，以分析自私挖掘攻击。

Result: 本文验证了文献中已有的阈值，并通过模拟发现了针对两个及以上攻击者的新型阈值。

Conclusion: 本文的框架为研究人员提供了评估和对比任何新增共识协议在其模型中表现的机会，从而增强区块链安全。

Abstract: The aim of this work is to enhance blockchain security by deepening the understanding of selfish mining attacks in various consensus protocols, especially the ones that have the potential to mitigate selfish mining. Previous research was mainly focused on a particular protocol with a single selfish miner, while only limited studies have been conducted on two or more attackers. To address this gap, we proposed a stochastic simulation framework that enables analysis of selfish mining with multiple attackers across various consensus protocols. We created the model of Proof-of-Work (PoW) Nakamoto consensus (serving as the baseline) as well as models of two additional consensus protocols designed to mitigate selfish mining: Fruitchain and Strongchain. Using our framework, thresholds reported in the literature were verified, and several novel thresholds were discovered for 2 and more attackers. We made the source code of our framework available, enabling researchers to evaluate any newly added protocol with one or more selfish miners and cross-compare it with already modeled protocols.

</details>


### [8] [JPU: Bridging Jailbreak Defense and Unlearning via On-Policy Path Rectification](https://arxiv.org/abs/2601.03005)
*Xi Wang,Songlei Jian,Shasha Li,Xiaopeng Li,Zhaoye Li,Bin Ji,Baosheng Wang,Jie Yu*

Main category: cs.CR

TL;DR: 本文研究了尽管进行了广泛的安全对齐，大语言模型（LLMs）仍然容易受到拜金攻击的问题。通过实验证明，失败机制在于激活未被删除的参数。为此，提出了Jailbreak Path Unlearning（JPU）方法，能够识别并修正动态拜金路径，增强模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 当前的机器遗忘方法针对大多数拜金攻击仍存在漏洞，因为激活了未被删除的参数。JPU旨在弥补现有遗忘防御的缺陷，通过发现和纠正动态拜金路径来增强模型的安全性。

Method: JPU通过动态挖掘政策对等奖励样本来识别和修正动态拜金路径。这种方法首先识别潜在的拜金路径，然后通过对抗样本验证这些路径的存在，并最终与安全锚点对齐，从而使模型更具拜金攻击抵抗力。

Result: 实验证明，JPU显著提高了模型抵抗动态拜金攻击的能力，同时保持了模型的实用性。

Conclusion: JPU是首个有效应对动态拜金路径的方法，为解决大语言模型的安全性问题提供了新的视角。

Abstract: Despite extensive safety alignment, Large Language Models (LLMs) often fail against jailbreak attacks. While machine unlearning has emerged as a promising defense by erasing specific harmful parameters, current methods remain vulnerable to diverse jailbreaks. We first conduct an empirical study and discover that this failure mechanism is caused by jailbreaks primarily activating non-erased parameters in the intermediate layers. Further, by probing the underlying mechanism through which these circumvented parameters reassemble into the prohibited output, we verify the persistent existence of dynamic $\textbf{jailbreak paths}$ and show that the inability to rectify them constitutes the fundamental gap in existing unlearning defenses. To bridge this gap, we propose $\textbf{J}$ailbreak $\textbf{P}$ath $\textbf{U}$nlearning (JPU), which is the first to rectify dynamic jailbreak paths towards safety anchors by dynamically mining on-policy adversarial samples to expose vulnerabilities and identify jailbreak paths. Extensive experiments demonstrate that JPU significantly enhances jailbreak resistance against dynamic attacks while preserving the model's utility.

</details>


### [9] [LLMs, You Can Evaluate It! Design of Multi-perspective Report Evaluation for Security Operation Centers](https://arxiv.org/abs/2601.03013)
*Hiroyuki Okada,Tatsumi Oba,Naoto Yanai*

Main category: cs.CR

TL;DR: 本文提出了一种新的LLM基础概念框架MESSALA，并通过一种分析报告检查表和两种新技术进一步改进，实验结果显示，与现有的LLM方法相比，MESSALA的评估结果与资深SOC分析师的评价最为接近，揭示了改进分析报告的关键见解。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在SOC分析报告生成中的潜在应用前景，以及了解资深分析师如何评估报告的重要性，本文旨在通过构建检查表和提出新技术来提升基于LLM的分析报告生成框架。

Method: 本文首先通过文献回顾和SOC从业者用户研究构建了一个分析师专有的检查表，然后设计了一个名为MESSALA的新型LLM概念框架，引入了粒度指南和多视角评价两种新技术。

Result: 实验结果表明，使用MESSALA的评价结果最接近资深SOC分析师的评价，且该框架能够提供改进分析报告的可操作建议。

Conclusion: 本文提出了MESSALA这一改进的LLM分析报告生成框架，证明了其在提升分析报告质量和改进建议方面的能力。

Abstract: Security operation centers (SOCs) often produce analysis reports on security incidents, and large language models (LLMs) will likely be used for this task in the near future. We postulate that a better understanding of how veteran analysts evaluate reports, including their feedback, can help produce analysis reports in SOCs. In this paper, we aim to leverage LLMs for analysis reports. To this end, we first construct a Analyst-wise checklist to reflect SOC practitioners' opinions for analysis report evaluation through literature review and user study with SOC practitioners. Next, we design a novel LLM-based conceptual framework, named MESSALA, by further introducing two new techniques, granularization guideline and multi-perspective evaluation. MESSALA can maximize report evaluation and provide feedback on veteran SOC practitioners' perceptions. When we conduct extensive experiments with MESSALA, the evaluation results by MESSALA are the closest to those of veteran SOC practitioners compared with the existing LLM-based methods. We then show two key insights. We also conduct qualitative analysis with MESSALA, and then identify that MESSALA can provide actionable items that are necessary for improving analysis reports.

</details>


### [10] [FlexProofs: A Vector Commitment with Flexible Linear Time for Computing All Proofs](https://arxiv.org/abs/2601.03031)
*Jing Liu,Liang Feng Zhang*

Main category: cs.CR

TL;DR: FlexProofs 提出了一种新的向量提交（VC）方案，可以在最优时间 ${rak O}(N)$ 内为长度为 $N$ 的向量生成所有个体开证明，并且还可以通过调整批量大小参数 $b$ 进一步缩短生成证明的时间。它还直接兼容用于多指幂的一组 zkSNARKs。实验表明，与 HydraProofs 相比，FlexProofs 在某些参数下可以更快，使其适用于实际应用如可验证秘密共享和可验证鲁棒聚合。


<details>
  <summary>Details</summary>
Motivation: 当前的向量提交方案无法在处理大规模数据时同时兼顾最优时间和灵活性。HydraProofs 能在 ${rak O}(N)$ 内计算所有证明，但缺乏灵活性。FlexProofs 则提供了一种从固定批量大小到灵活批量大小的过渡方案，在优化证明生成时间和灵活性方面取得了新的突破。

Method: FlexProofs 采用了改进的向量提交机制，首先提出了一种新的功能提交（FC）方案，用于多指幂的批量打开。然后，通过引入可调整的批量大小参数 $b$，可以在保证性能的同时提高灵活性。最后，FlexProofs 通过与适合的 zkSNARKs 结合使用，实现数据验证。

Result: 实验结果显示，对于长度为 $2^{16}$ 的向量，FlexProofs 凭借调整批量大小参数，能在 ${rak O}(N)$ 时间内生成所有证明，并且相比 HydraProofs 能提高 6 倍的速度，提升了系统的实用性和效率。FlexProofs 的实现使得可验证秘密共享、可验证鲁棒聚合等应用成为可能。

Conclusion: FlexProofs 提出了一种优化向量提交方案，提升了证明生成的效率和灵活性。其方法与实际应用的集成展示了其在区块链、密码学等领域中的巨大潜力。

Abstract: In this paper, we introduce FlexProofs, a new vector commitment (VC) scheme that achieves two key properties: (1) the prover can generate all individual opening proofs for a vector of size $N$ in optimal time ${\cal O}(N)$, and there is a flexible batch size parameter $b$ that can be increased to further reduce the time to generate all proofs; and (2) the scheme is directly compatible with a family of zkSNARKs that encode their input as a multi-linear polynomial. As a critical building block, we propose the first functional commitment (FC) scheme for multi-exponentiations with batch opening. Compared with HydraProofs, the only existing VC scheme that computes all proofs in optimal time ${\cal O}(N)$ and is directly compatible with zkSNARKs, FlexProofs may speed up the process of generating all proofs, if the parameter $b$ is properly chosen. Our experiments show that for $N=2^{16}$ and $b=\log^2 N$, FlexProofs can be $6\times$ faster than HydraProofs. Moreover, when combined with suitable zkSNARKs, FlexProofs enable practical applications such as verifiable secret sharing and verifiable robust aggregation.

</details>


### [11] [SLIM: Stealthy Low-Coverage Black-Box Watermarking via Latent-Space Confusion Zones](https://arxiv.org/abs/2601.03242)
*Hengyu Wu,Yang Cao*

Main category: cs.CR

TL;DR: SLIM框架通过训练模型让语义相似的前缀生成出语义上不同的后续，实现数据溯源验证，具有超低覆盖度、强黑盒验证能力和良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 训练数据是大型语言模型开发中的关键且通常私有的资产，需要保护数据不被滥用，同时保证模型的实用性和隐私性。

Method: SLIM框架利用大型语言模型本身的内在特性，通过训练在潜在空间中创造混淆区域，使得语义相似的前缀生成不同的后续，从而在不修改大量数据的情况下进行有效的数据溯源验证。

Result: 实验结果表明，SLIM能够在超低的数据覆盖下实现强黑盒验证性能和良好的可扩展性，同时保持高度的隐秘性和模型实用性。

Conclusion: SLIM提供了一种在保护训练数据的同时不牺牲模型性能的解决方案，为现代大型语言模型开发中的数据保护问题提供了一个强大而可靠的方法。

Abstract: Training data is a critical and often proprietary asset in Large Language Model (LLM) development, motivating the use of data watermarking to embed model-transferable signals for usage verification. We identify low coverage as a vital yet largely overlooked requirement for practicality, as individual data owners typically contribute only a minute fraction of massive training corpora. Prior methods fail to maintain stealthiness, verification feasibility, or robustness when only one or a few sequences can be modified. To address these limitations, we introduce SLIM, a framework enabling per-user data provenance verification under strict black-box access. SLIM leverages intrinsic LLM properties to induce a Latent-Space Confusion Zone by training the model to map semantically similar prefixes to divergent continuations. This manifests as localized generation instability, which can be reliably detected via hypothesis testing. Experiments demonstrate that SLIM achieves ultra-low coverage capability, strong black-box verification performance, and great scalability while preserving both stealthiness and model utility, offering a robust solution for protecting training data in modern LLM pipelines.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [SimpleMem: Efficient Lifelong Memory for LLM Agents](https://arxiv.org/abs/2601.02553)
*Jiaqi Liu,Yaofeng Su,Peng Xia,Siwei Han,Zeyu Zheng,Cihang Xie,Mingyu Ding,Huaxiu Yao*

Main category: cs.AI

TL;DR: SimpleMem 是一种基于语义无损压缩的高效记忆框架，通过三阶段流程优化信息密度和Token利用，显著提升鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有框架要么通过被动上下文扩展保留完整的交互历史，带来冗余；要么依赖迭代推理过滤噪声，导致高Token成本。为解决此问题，提出SimpleMem框架。

Method: 从结构化的角度应用熵意识过滤进行语义压缩，实现信息密集化；通过递归内存整合减少冗余；并在查询过程中动态调整检索范围，提高检索效率。

Result: 实验表明，SimpleMem在准确度、检索效率和推理成本上均优于基准方法，F1值提高26.4%，推理时间的Token消耗减少30倍。

Conclusion: SimpleMem框架展示了出色的性能与效率之间的平衡。

Abstract: To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.

</details>


### [13] [An Empirical Study of On-Device Translation for Real-Time Live-Stream Chat on Mobile Devices](https://arxiv.org/abs/2601.02641)
*Jeiyoon Park,Daehwan Lee,Changmin Yeo,Yongshin Han,Minseop Kim*

Main category: cs.AI

TL;DR: 本文通过实验研究了在实际部署边缘设备AI模型时所需的关键问题，即模型选择及资源消耗，以及边缘模型的领域适应能力。


<details>
  <summary>Details</summary>
Motivation: 鉴于边缘设备AI模型在实际应用中面临的CPU利用率和热管理问题，本文旨在解决关键部署问题，确保满足大规模多样用户的需求。

Method: 作者构建了LiveChatBench基准测试集，并在五种移动设备上进行了详尽的实验研究。

Result: 实验结果显示，尽管需要在高度受限的部署环境中仔细考虑模型选择以应对大量多样用户，所提出的方法在特定任务上仍能与商业模型（如GPT-5.1）媲美。

Conclusion: 本文的研究成果将为边缘AI社区提供宝贵的见解。

Abstract: Despite its efficiency, there has been little research on the practical aspects required for real-world deployment of on-device AI models, such as the device's CPU utilization and thermal conditions. In this paper, through extensive experiments, we investigate two key issues that must be addressed to deploy on-device models in real-world services: (i) the selection of on-device models and the resource consumption of each model, and (ii) the capability and potential of on-device models for domain adaptation. To this end, we focus on a task of translating live-stream chat messages and manually construct LiveChatBench, a benchmark consisting of 1,000 Korean-English parallel sentence pairs. Experiments on five mobile devices demonstrate that, although serving a large and heterogeneous user base requires careful consideration of highly constrained deployment settings and model selection, the proposed approach nevertheless achieves performance comparable to commercial models such as GPT-5.1 on the well-targeted task. We expect that our findings will provide meaningful insights to the on-device AI community.

</details>


### [14] [Learning from Prompt itself: the Hierarchical Attribution Prompt Optimization](https://arxiv.org/abs/2601.02683)
*Dongyu Chen,Jian Ma,Xianpeng Zhang,Lei Zhang,Haonan Lu,Chen Chen,Chuangchuang Wang,Kai Tang*

Main category: cs.AI

TL;DR: 本文提出了一种名为HAPO的层级归因提示优化框架，旨在通过动态归因机制、语义单元优化和多模态友好的进展来解决当前提示优化方法中的局限性，从而提高优化效率。


<details>
  <summary>Details</summary>
Motivation: 当前的提示优化方法存在提示漂移和降低解释性的问题。为了克服这些不足，本文提出了HAPO框架来提升优化的效率和解释性。

Method: HAPO框架包括三个创新：动态归因机制、语义单元优化和多模态友好的进展。动态归因机制针对训练数据和提示历史中的错误模式；语义单元优化主要用于编辑功能提示片段；多模态友好的进展则支持端到端的LLM及LLM-MLLM工作流程。

Result: 实验结果表明，HAPO框架在单/多图像问答（如OCRV2）和复杂任务分析（如BBH）等场景中表现出更高的优化效率，并优于现有的自动化提示优化方法。

Conclusion: HAPO为可扩展的提示工程方法提供了扩展范式，未来可能适用于更多场景并进一步集成到提示生成流程中。

Abstract: Optimization is fundamental across numerous disciplines, typically following an iterative process of refining an initial solution to enhance performance. This principle is equally critical in prompt engineering, where designing effective prompts for large language models constitutes a complex optimization challenge. A structured optimization approach requires automated or semi-automated procedures to develop improved prompts, thereby reducing manual effort, improving performance, and yielding an interpretable process. However, current prompt optimization methods often induce prompt drift, where new prompts fix prior failures but impair performance on previously successful tasks. Additionally, generating prompts from scratch can compromise interpretability. To address these limitations, this study proposes the Hierarchical Attribution Prompt Optimization (HAPO) framework, which introduces three innovations: (1) a dynamic attribution mechanism targeting error patterns in training data and prompting history, (2) semantic-unit optimization for editing functional prompt segments, and (3) multimodal-friendly progression supporting both end-to-end LLM and LLM-MLLM workflows. Applied in contexts like single/multi-image QA (e.g., OCRV2) and complex task analysis (e.g., BBH), HAPO demonstrates enhanced optimization efficiency, outperforming comparable automated prompt optimization methods and establishing an extensible paradigm for scalable prompt engineering.

</details>


### [15] [LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery](https://arxiv.org/abs/2601.02757)
*Zixuan Xiao,Jun Ma*

Main category: cs.AI

TL;DR: ChangeGPT整合LLM与视觉基础模型，采用分层结构减少幻觉。在140个分类问题上展示了强大的泛化能力，尤其擅长多步骤推理的变更查询。


<details>
  <summary>Details</summary>
Motivation: 现有变更检测方法缺乏多样性处理能力和智能分析能力，为解决这一问题，本研究提出ChangeGPT框架，结合LLM与视觉基础模型，提高变更检测的准确性和适应性。

Method: 该研究设计了一个分层结构的变更检测框架，利用LLM进行智能推理和决策，并通过视觉基础模型处理图像数据，以减少幻觉。框架还通过评估工具选择能力和涵盖复杂查询的准确性进行评估。

Result: ChangeGPT在140个分类问题上的评估中表现出色，尤其是对于需要多步推理和工具选择的变更查询。GPT-4-turbo后端版本取得了90.71%的匹配率。

Conclusion: ChangeGPT所提供的智能、适应性和多类型变更分析能力，为遥感应用中的决策提供了一个强大的解决方案。

Abstract: Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.

</details>


### [16] [HAL: Inducing Human-likeness in LLMs with Alignment](https://arxiv.org/abs/2601.02813)
*Masum Hasan,Junjie Zhao,Ehsan Hoque*

Main category: cs.AI

TL;DR: 该研究提出了HAL框架，一种用于语言模型与对话人似性的对齐方法。通过对比对话数据，HAL提取明确的对话特征并将其转化为透明的奖励信号，从而实现对不同规模模型的人似性对齐。


<details>
  <summary>Details</summary>
Motivation: 由于难以定义、测量和优化对话人似性，目前人类-AI互动中的人似性行为改进主要依赖于规模或广泛的监督训练，而不是有针对性的对齐。HAL框架的提出旨在填补这一空白，通过数据驱动的方法使模糊的人似性变成可测量和对齐的属性。

Method: HAL框架从对比对话数据中提取明确定义的对话特征，并将它们组合成一个紧凑的标度分数。使用这种方法，HAL 使用这个分数作为透明的奖励信号来进行对齐。该框架利用标准偏好优化方法实现了不同规模模型的对话人似性对齐。

Result: 在大规模的人类评估中，使用HAL对齐的模型在对话中的感知人似性更高。HAL 还允许对齐行为的检查和对意外影响的诊断，这对于理解语言质量、如礼貌和准确性的软、定性属性如何通过可解释和可解释的方式进行对齐非常有益。

Conclusion: HAL框架展示了一种新的方法，通过将软、定性的人似性属性转变为可量化和可解释的形式，为语言模型的对话人似性对齐提供了新的途径。

Abstract: Conversational human-likeness plays a central role in human-AI interaction, yet it has remained difficult to define, measure, and optimize. As a result, improvements in human-like behavior are largely driven by scale or broad supervised training, rather than targeted alignment. We introduce Human Aligning LLMs (HAL), a framework for aligning language models to conversational human-likeness using an interpretable, data-driven reward. HAL derives explicit conversational traits from contrastive dialogue data, combines them into a compact scalar score, and uses this score as a transparent reward signal for alignment with standard preference optimization methods. Using this approach, we align models of varying sizes without affecting their overall performance. In large-scale human evaluations, models aligned with HAL are more frequently perceived as human-like in conversation. Because HAL operates over explicit, interpretable traits, it enables inspection of alignment behavior and diagnosis of unintended effects. More broadly, HAL demonstrates how soft, qualitative properties of language--previously outside the scope for alignment--can be made measurable and aligned in an interpretable and explainable way.

</details>


### [17] [Causal-Enhanced AI Agents for Medical Research Screening](https://arxiv.org/abs/2601.02814)
*Duc Ngo,Arya Rahgoza*

Main category: cs.AI

TL;DR: 该研究提出了一种增强因果图检索增强生成系统，结合显式因果推理和双重知识图，实现无幻觉的系统评价，并在痴呆症运动研究中验证其准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的人工系统评价无法应对每年超过1.5万篇文献，而现有AI系统在系统评价任务中存在幻觉问题，影响临床决策。

Method: 研究设计了一个结合因果图和双重知识图的生成系统，通过检索和生成工具支持显式因果推理，确保每个因果断言都基于文献检索结果并自动生成有向无环图。

Result: 在痴呆症运动研究的234个摘要评估中，该系统达到了95%的准确率和100%的检索成功率，且无任何幻觉，而基线AI系统准确率仅为34%且有10%的幻觉。

Conclusion: 该研究展示了增强因果图系统的有效性，并证明了因果推理在高风险医疗领域中的潜在价值。

Abstract: Systematic reviews are essential for evidence-based medicine, but reviewing 1.5 million+ annual publications manually is infeasible. Current AI approaches suffer from hallucinations in systematic review tasks, with studies reporting rates ranging from 28--40% for earlier models to 2--15% for modern implementations which is unacceptable when errors impact patient care.
  We present a causal graph-enhanced retrieval-augmented generation system integrating explicit causal reasoning with dual-level knowledge graphs. Our approach enforces evidence-first protocols where every causal claim traces to retrieved literature and automatically generates directed acyclic graphs visualizing intervention-outcome pathways.
  Evaluation on 234 dementia exercise abstracts shows CausalAgent achieves 95% accuracy, 100% retrieval success, and zero hallucinations versus 34% accuracy and 10% hallucinations for baseline AI. Automatic causal graphs enable explicit mechanism modeling, visual synthesis, and enhanced interpretability. While this proof-of-concept evaluation used ten questions focused on dementia exercise research, the architectural approach demonstrates transferable principles for trustworthy medical AI and causal reasoning's potential for high-stakes healthcare.

</details>


### [18] [Sample-Efficient Neurosymbolic Deep Reinforcement Learning](https://arxiv.org/abs/2601.02850)
*Celeste Veronese,Daniele Meli,Alessandro Farinelli*

Main category: cs.AI

TL;DR: 论文提出了一种结合符号知识的深度强化学习方法，旨在提高样本效率并增强在更复杂任务中的泛化能力。通过将简单环境中的部分策略迁移作为先验知识，并利用逻辑规则表示和在线推理，论文在稀疏奖励环境和具有长期规划任务中展示了更快的收敛速度和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 目前的深度强化学习算法面临样本效率低和泛化能力不足的问题，尤其是在处理复杂任务时。为了提高算法性能并解决这些问题，论文提出了一种结合符号知识的神经符号深度强化学习方法。

Method: 该方法通过将简单环境中的部分策略作为有用的先验知识并进行迁移学习，利用逻辑规则表示这些部分策略，并通过在线推理机制在探索阶段偏向动作分布和在利用阶段重新缩放Q值来指导训练过程。

Result: 实验结果显示，该方法在网格世界环境的多种挑战性变体中表现优于现有的基于奖励机器的基准，特别是在完全可观测和部分可观测环境下。

Conclusion: 该论文提出的方法通过结合符号知识提高了神经符号深度强化学习的样本效率和泛化能力，尤其在稀疏奖励环境和具有长期规划任务中取得了良好效果。

Abstract: Reinforcement Learning (RL) is a well-established framework for sequential decision-making in complex environments. However, state-of-the-art Deep RL (DRL) algorithms typically require large training datasets and often struggle to generalize beyond small-scale training scenarios, even within standard benchmarks. We propose a neuro-symbolic DRL approach that integrates background symbolic knowledge to improve sample efficiency and generalization to more challenging, unseen tasks. Partial policies defined for simple domain instances, where high performance is easily attained, are transferred as useful priors to accelerate learning in more complex settings and avoid tuning DRL parameters from scratch. To do so, partial policies are represented as logical rules, and online reasoning is performed to guide the training process through two mechanisms: (i) biasing the action distribution during exploration, and (ii) rescaling Q-values during exploitation. This neuro-symbolic integration enhances interpretability and trustworthiness while accelerating convergence, particularly in sparse-reward environments and tasks with long planning horizons. We empirically validate our methodology on challenging variants of gridworld environments, both in the fully observable and partially observable setting. We show improved performance over a state-of-the-art reward machine baseline.

</details>


### [19] [ReTreVal: Reasoning Tree with Validation - A Hybrid Framework for Enhanced LLM Multi-Step Reasoning](https://arxiv.org/abs/2601.02880)
*Abhishek HS,Pavan C Shekar,Arpit Jain,Ashwanth Krishnan*

Main category: cs.AI

TL;DR: ReTreVal 提出了一种结合了 Tree-of-Thoughts 探索、自我校正、基于 LLM 的批评得分和反省记忆的混合框架，以实现有边界的验证多步推理，并且在数学问题和创意写作任务上的性能优于现有的方法。


<details>
  <summary>Details</summary>
Motivation: 当前的 LLM 在复杂领域如数学和创意写作中的多步推理仍然存在挑战，虽然一些方法如 ReAct、Reflexion 和 Self-Refine 提高了推理的迭代和反思，但对于替代解决方案路径的有组织探索和跨问题学习仍然不足。

Method: ReTreVal 通过构建基于问题复杂性的自适应深度结构化推理树，结合 Tree-of-Thoughts 探索、自我校正、基于 LLM 的批评得分和反省记忆机制。节点通过迭代自我批评和 LLM 生成反馈的引导下进行自我校正和改进。采用双重验证机制评估每一步推理的效果，并通过反省记忆存储成功和失败的路径，实现跨问题学习。此外，通过批评驱动得剪枝保留高得分数增加的节点来控制计算成本。

Result: 使用 Qwen 2.5 7B 作为底座模型，ReTreVal 在 500 个数学问题和创意写作任务上测试，结果表明其在多步推理的结构化探索、批评驱动的优化和跨问题记忆的支持下，取得了优于 ReAct、Reflexion 和 Self-Refine 的表现。

Conclusion: ReTreVal 针对 LLM 在复杂领域中的多步推理挑战提供了一个有效的解决方案，通过有效的结构探索、基于批评的改进和跨问题记忆，大大提高了推理的质量和效率。

Abstract: Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer.

</details>


### [20] [Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning](https://arxiv.org/abs/2601.02902)
*Xinglang Zhang,Yunyao Zhang,ZeLiang Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.AI

TL;DR: 研究发现在增加逻辑复杂度时，大型语言模型的逻辑推理能力会出现‘逻辑相变’现象，其性能在特定阶段内保持稳定但在超越某一逻辑深度后急剧下降；提出了神经符号课程调节框架，通过自适应地调整自然语言与逻辑符号之间的表示，以及重新塑造训练动态，增强逻辑深度上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在复杂逻辑场景下的表现，并提出改进方法，增强其在高风险领域的决策能力。

Method: 通过控制逻辑复杂度进行系统分析；识别逻辑相变现象；设计神经符号课程调节框架进行优化。

Result: 展示了该方法在五个基准上的效果，包括提高逻辑推理能力、增强对未见过的逻辑组合的泛化能力。

Conclusion: 提出了神经符号课程调节框架，验证了其在提高大型语言模型逻辑推理能力方面的有效性。

Abstract: Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions.

</details>


### [21] [Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning](https://arxiv.org/abs/2601.02950)
*Xuan Yang,Furong Jia,Roy Xie,Xiong Xi,Hengwei Bian,Jian Li,Monica Agrawal*

Main category: cs.AI

TL;DR: Batch-of-Thought (BoT) 方法通过联合处理相关查询实现跨实例学习，提高模型准确性、信心校准并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在处理查询时独立进行，错过了利用共享推理模式和一致性检查的机会，而 BoT 方法旨在通过联合处理相关查询来弥补这一不足。

Method: BoT 方法通过比较不同批次之间的分析结果来识别高质量的推理模板，并通过一致性检查检测错误。此外，它还通过共同评估减少了计算成本。

Result: 在三个模型家族和六个基准测试中，BoT-R 体系结构展示了在保持或提高准确性的同时，信心校准得到改进，推理成本降低了61%。

Conclusion: 理论和实验分析表明，批处理感知的推理能为大语言模型系统带来显著的好处，并揭示了其成效显著的原因与时点。

Abstract: Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems.

</details>


### [22] [Rationale-Grounded In-Context Learning for Time Series Reasoning with Multimodal Large Language Models](https://arxiv.org/abs/2601.02968)
*Qingxiang Liu,Zhiqing Cui,Xiaoliang Luo,Yuqian Wu,Zhuoyang Jiang,Huaiyu Wan,Sheng Sun,Lvchun Wang,Wei Yu,Yuxuan Liang*

Main category: cs.AI

TL;DR: 该研究提出了一种名为RationaleTS的方法，通过引入条件化理性引导推理单元，填补了现有模型在时间序列推理中的空白，从而提高模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大规模多模态语言模型在时间序列推理中的表现不佳在于缺乏连接时间观察和下游结果的先验理性，导致模型依赖于表面的模式匹配而非原则性的推理。

Method: 首先生成条件化理性，包括从可观察证据到潜在结果的推理路径。然后通过混合检索平衡时间模式和语义上下文来检索相关先验理性，用于对新样本的语境推理。

Result: 在三个领域的时间序列推理任务中进行了广泛的实验，证明了RationaleTS方法的有效性和效率。

Conclusion: 研究团队将发布代码以供复现。

Abstract: The underperformance of existing multimodal large language models for time series reasoning lies in the absence of rationale priors that connect temporal observations to their downstream outcomes, which leads models to rely on superficial pattern matching rather than principled reasoning. We therefore propose the rationale-grounded in-context learning for time series reasoning, where rationales work as guiding reasoning units rather than post-hoc explanations, and develop the RationaleTS method. Specifically, we firstly induce label-conditioned rationales, composed of reasoning paths from observable evidence to the potential outcomes. Then, we design the hybrid retrieval by balancing temporal patterns and semantic contexts to retrieve correlated rationale priors for the final in-context inference on new samples. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed RationaleTS on three-domain time series reasoning tasks. We will release our code for reproduction.

</details>


### [23] [A framework for assuring the accuracy and fidelity of an AI-enabled Digital Twin of en route UK airspace](https://arxiv.org/abs/2601.03120)
*Adam Keane,Nick Pepper,Chris Burr,Amy Hodgkin,Dewi Gould,John Korna,Marc Thomas*

Main category: cs.AI

TL;DR: 本研究开发了一种针对英国民用空中航行系统的概率数字孪生，并构建了一套评估框架以提升数字孪生的准确性与适用性，该框架运用了可信和伦理保障（TEA）方法来构建保障案例，提供了切实可行的目标及需证明的证据。


<details>
  <summary>Details</summary>
Motivation: 随着智能空中交通管理系统（ATM）的发展，为确保数字孪生用于ATM时具有高度的测度合理性与依赖性，避免应用风险，研究人员需要一套系统化的评估框架。

Method: 该研究采用了一系列工具和框架，包括使用TEA方法构建保障案例，通过详细论述每个保障目标所需证据、假设及论证，增强数字孪生模型的信任度。

Result: 研究建立了一套旨在评估数字孪生可靠性的保障框架，涵盖了各项具体应用情况，还提供了实质性的改善一致性征求意见，为未来该领域的标准化提供了参考。

Conclusion: 该论文为数字孪生在航空领域的应用提供了详细的保障框架指导，有助于提升其可靠性和验证过程的规范性，并更好地满足未来的监管需求。

Abstract: Digital Twins combine simulation, operational data and Artificial Intelligence (AI), and have the potential to bring significant benefits across the aviation industry. Project Bluebird, an industry-academic collaboration, has developed a probabilistic Digital Twin of en route UK airspace as an environment for training and testing AI Air Traffic Control (ATC) agents. There is a developing regulatory landscape for this kind of novel technology. Regulatory requirements are expected to be application specific, and may need to be tailored to each specific use case.
  We draw on emerging guidance for both Digital Twin development and the use of Artificial Intelligence/Machine Learning (AI/ML) in Air Traffic Management (ATM) to present an assurance framework. This framework defines actionable goals and the evidence required to demonstrate that a Digital Twin accurately represents its physical counterpart and also provides sufficient functionality across target use cases. It provides a structured approach for researchers to assess, understand and document the strengths and limitations of the Digital Twin, whilst also identifying areas where fidelity could be improved. Furthermore, it serves as a foundation for engagement with stakeholders and regulators, supporting discussions around the regulatory needs for future applications, and contributing to the emerging guidance through a concrete, working example of a Digital Twin.
  The framework leverages a methodology known as Trustworthy and Ethical Assurance (TEA) to develop an assurance case. An assurance case is a nested set of structured arguments that provides justified evidence for how a top-level goal has been realised. In this paper we provide an overview of each structured argument and a number of deep dives which elaborate in more detail upon particular arguments, including the required evidence, assumptions and justifications.

</details>


### [24] [Automatic Prompt Engineering with No Task Cues and No Tuning](https://arxiv.org/abs/2601.03130)
*Faisal Chowdhury,Nandana Mihindukulasooriya,Niharika S D'Souza,Horst Samulowitz,Neeru Gupta,Tomasz Hanusiak,Michal Kapitonow*

Main category: cs.AI

TL;DR: 本文介绍了一种简单易用的自动提示工程系统，在设计和应用上更简化，且与现有方法同样有效。该系统无需调优，也不需要任务的显式线索。该研究首次将自动提示工程应用于数据库表中的隐秘列名扩展任务，并且是首次尝试在非英语语言上应用自动提示工程。


<details>
  <summary>Details</summary>
Motivation: 鉴于隐秘列名扩展是个关键任务但研究较少，以及现有自动提示工程方法可能需要额外调优和显式任务线索的情况，提出一种简化且有效的自动提示工程系统。

Method: 该系统通过简化设计与应用，在不解码任务信息的情况下即能进行有效提示工程。

Result: 在英语和德语的数据库表隐秘列名扩展任务中进行了评估，结果表明该系统与现有方法同样有效。

Conclusion: 首次提出了将自动提示工程应用于隐秘列名扩展任务，并且是首例研究非英语语言中的自动提示工程应用。

Abstract: This paper presents a system for automatic prompt engineering that is much simpler in both design and application and yet as effective as the existing approaches. It requires no tuning and no explicit clues about the task. We evaluated our approach on cryptic column name expansion (CNE) in database tables, a task which is critical for tabular data search, access, and understanding and yet there has been very little existing work. We evaluated on datasets in two languages, English and German. This is the first work to report on the application of automatic prompt engineering for the CNE task. To the best of our knowledge, this is also the first work on the application of automatic prompt engineering for a language other than English.

</details>
