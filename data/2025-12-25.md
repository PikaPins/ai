<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 27]
- [cs.CR](#cs.CR) [Total: 9]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation](https://arxiv.org/abs/2512.20626)
*Chi-Hsiang Hsiao,Yi-Cheng Wang,Tzung-Sheng Lin,Yi-Ren Yeh,Chu-Song Chen*

Main category: cs.AI

TL;DR: 该研究提出了一种结合多模态知识图谱的检索增强生成（RAG）方法，该方法通过整合视觉线索改进了知识图谱的构建、检索和答案生成过程，从而在文本和多模态数据集上提高了全局和细粒度的问答任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在长篇领域特定内容和高级概念理解上的局限性，旨在提供全面的多模态知识图谱支持，使其能够进行跨模态推理，并提升对内容的理解。

Method: 该研究构建了一种多模态知识图谱增强的RAG方法，这种模型结合了视觉线索、文本和知识结构，改进了知识图谱的构建、检索和答案生成的过程。

Result: 实验结果表明，这种新的RAG方法在文本和多模态数据集上，在全局和细粒度的问答任务中，均优于现有的RAG方法。

Conclusion: 研究表明，通过整合多模态信息，可以有效提升RAG系统的理解和推理能力，为实现基于自然语言的复杂交互和应用奠定了基础。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to dynamically access external information, which is powerful for answering questions over previously unseen documents. Nonetheless, they struggle with high-level conceptual understanding and holistic comprehension due to limited context windows, which constrain their ability to perform deep reasoning over long-form, domain-specific content such as full-length books. To solve this problem, knowledge graphs (KGs) have been leveraged to provide entity-centric structure and hierarchical summaries, offering more structured support for reasoning. However, existing KG-based RAG solutions remain restricted to text-only inputs and fail to leverage the complementary insights provided by other modalities such as vision. On the other hand, reasoning from visual documents requires textual, visual, and spatial cues into structured, hierarchical concepts. To address this issue, we introduce a multimodal knowledge graph-based RAG that enables cross-modal reasoning for better content understanding. Our method incorporates visual cues into the construction of knowledge graphs, the retrieval phase, and the answer generation process. Experimental results across both global and fine-grained question answering tasks show that our approach consistently outperforms existing RAG-based approaches on both textual and multimodal corpora.

</details>


### [2] [Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025)](https://arxiv.org/abs/2512.20628)
*Edited by Tessai Hayama,Takayuki Ito,Takahiro Uchiya,Motoki Miura,Takahiro Kawaji,Takaya Yuizono,Atsuo Yoshitaka,Tokuro Matsuo,Shun Okuhara,Jawad Haqbeen,Sofia Sahab,Wen Gu,Shiyao Ding*

Main category: cs.AI

TL;DR: 2025年KICSS国际会议的论文集，涵盖人工智能、知识工程、人机交互和创意支持系统等多学科领域，经过严格的双盲审稿流程，优秀论文将推荐至IEICE Transactions on Information and Systems期刊发表。


<details>
  <summary>Details</summary>
Motivation: 促进跨学科研究交流，提升科技进步与应用水平，推动相关领域技术发展与创新。

Method: 双盲审稿和同行评审。

Result: 汇聚了多个研究领域的前沿成果，提升了作者们在相关领域的影响力和认可度，同时为未来研究提供了有价值的参考。

Conclusion: KICSS国际会议作为一个重要平台，为推动知识、信息与创造力支持系统的进步作出贡献。

Abstract: This volume presents the proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025), held in Nagaoka, Japan, on December 3-5, 2025. The conference, organized in cooperation with the IEICE Proceedings Series, provides a multidisciplinary forum for researchers in artificial intelligence, knowledge engineering, human-computer interaction, and creativity support systems. The proceedings include peer-reviewed papers accepted through a double-blind review process. Selected papers have been recommended for publication in IEICE Transactions on Information and Systems after an additional peer-review process.

</details>


### [3] [MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data](https://arxiv.org/abs/2512.20630)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.AI

TL;DR: microprobe 利用100个精心选择的探针样本实现全面的可靠性能评估，相比随机抽样基线提高了23.5%的综合可靠性能评分，并以99.9%的统计功效实现了90%的评估成本降低。


<details>
  <summary>Details</summary>
Motivation: 针对大规模基础模型可靠性评估的高成本和高耗时问题，microprobe 提出了一种创新的方法，只需少量精心选择的探针样本即可实现全面的可靠性能评估。

Method: microprobe 结合了跨五个关键可靠性维度的战略性提示多样性、高级不确定性量化和自适应加权，以高效地检测潜在的失效模式。

Result: 在多个语言模型（GPT-2 变种、GPT-2 中型、GPT-2 大型）和跨域验证（医疗保健、金融、法律）上进行的广泛实证评价表明，microprobe 相较于随机抽样基线提高了23.5%的综合可靠性能评分，具有显著的统计意义（p < 0.001，Cohen's d = 1.21）。

Conclusion: microprobe 在保证95%的传统方法覆盖率的同时实现了90%的评估成本降低，并且以99.9%的统计功效完成了可靠性评估，这种方法对于负责任的人工智能部署具有重要贡献。

Abstract: Foundation model reliability assessment typically requires thousands of evaluation examples, making it computationally expensive and time-consuming for real-world deployment. We introduce microprobe, a novel approach that achieves comprehensive reliability assessment using only 100 strategically selected probe examples. Our method combines strategic prompt diversity across five key reliability dimensions with advanced uncertainty quantification and adaptive weighting to efficiently detect potential failure modes. Through extensive empirical evaluation on multiple language models (GPT-2 variants, GPT-2 Medium, GPT-2 Large) and cross-domain validation (healthcare, finance, legal), we demonstrate that microprobe achieves 23.5% higher composite reliability scores compared to random sampling baselines, with exceptional statistical significance (p < 0.001, Cohen's d = 1.21). Expert validation by three AI safety researchers confirms the effectiveness of our strategic selection, rating our approach 4.14/5.0 versus 3.14/5.0 for random selection. microprobe completes reliability assessment with 99.9% statistical power while representing a 90% reduction in assessment cost and maintaining 95% of traditional method coverage. Our approach addresses a critical gap in efficient model evaluation for responsible AI deployment.

</details>


### [4] [Erkang-Diagnosis-1.1 Technical Report](https://arxiv.org/abs/2512.20632)
*Jianbing Ma,Ao Feng,Zhenjie Gao,Xinyu Song,Li Su,Bin Chen,Wei Wang,Jiamin Wu*

Main category: cs.AI

TL;DR: 该报告详细介绍了Erkang-Diagnosis-1.1模型，这是一种利用阿里巴巴Qwen-3模型开发的AI医疗咨询助手。该模型整合了约500GB高质量的结构化医学知识，采用增强预训练和检索增强生成相结合的方法，旨在成为用户智能健康伴侣，提升初级医疗服务和健康管理。实验显示，在综合医学检查方面，Erkang-Diagnosis-1.1优于GPT-4。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能技术的发展，开发一种专业的AI健康顾问变得越来越重要。Erkang模型结合了增强预训练和检索增强生成的方法，旨在解决现有的AI模型在医疗领域应用中的问题，如理解复杂医学知识和提供精确的诊断建议。

Method: Erkang-Diagnosis-1.1模型通过混合增强预训练和检索增强生成，整合了约500GB高质量的结构化医学知识。该模型通过对用户症状进行3-5轮高效的交互，实现初步分析并提供有价值的诊断建议和健康指导。

Result: Erkang-Diagnosis-1.1模型在综合医学检查中的表现优于GPT-4。通过高效的交互流程，能够准确理解用户症状，进行初步分析，并提供精确的诊断建议和健康指导。

Conclusion: Erkang-Diagnosis-1.1模型作为一款专业的AI健康顾问，通过结合先进的技术手段和高质量的医学知识，提升了初级医疗服务和健康管理的能力。

Abstract: This report provides a detailed introduction to Erkang-Diagnosis-1.1 model, our AI healthcare consulting assistant developed using Alibaba Qwen-3 model. The Erkang model integrates approximately 500GB of high-quality structured medical knowledge, employing a hybrid approach combining enhanced pre-training and retrieval-enhanced generation to create a secure, reliable, and professional AI health advisor. Through 3-5 efficient interaction rounds, Erkang Diagnosis can accurately understand user symptoms, conduct preliminary analysis, and provide valuable diagnostic suggestions and health guidance. Designed to become users intelligent health companions, it empowers primary healthcare and health management. To validate, Erkang-Diagnosis-1.1 leads GPT-4 in terms of comprehensive medical exams.

</details>


### [5] [Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning](https://arxiv.org/abs/2512.20647)
*Leo Lu,Jonathan Zhang,Sean Chua,Spencer Kim,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.AI

TL;DR: 该研究探索了不同模型是否能够可靠地继续另一个模型的部分推理链，展示了这种行为的一个新的研究视角，并且表明这种混合推理链往往能保持甚至提高最终的准确性和逻辑结构。


<details>
  <summary>Details</summary>
Motivation: 当前LARGE LANGUAGE MODELS (LLMs) 的推理能力得到了显著提升，但已有研究主要集中在模型内部推理策略的改进上，缺乏对不同模型之间推理策略通用性的研究探索。因此该研究旨在破除此壁垒，理解模型之间推理链的可转移性。

Method: 研究通过基线模型Gemma-3-4B-IT和LLaMA-3.1-70B-Instruct，使用token-level log-probability thresholds来截断推理链，在不同程度上对推理链进行截断。然后在截断后的推理链基础上，实验不同模型之间的推理链延续与一致性。

Result: 在用一种PRM模型评估推理链的稳定性和逻辑结构后，研究发现混合推理链往往能在保持甚至提高最终准确性和逻辑结构方面表现优秀。研究结果揭示了一种新的可转移推理链的潜力，它可以在未来提供可靠的模块化推理系统的新发展方向。

Conclusion: 研究指出，推理链的可转移性作为推理模型的一个有望成为新兴行为特征的属性，它的探索可以在促进可靠的人工智能系统协作模块的构建方面提供新的见解。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of large language models (LLMs). While prior work focuses on improving model performance through internal reasoning strategies, little is known about the interchangeability of reasoning across different models. In this work, we explore whether a partially completed reasoning chain from one model can be reliably continued by another model, either within the same model family or across families. We achieve this by assessing the sufficiency of intermediate reasoning traces as transferable scaffolds for logical coherence and final answer accuracy. We interpret this interchangeability as a means of examining inference-time trustworthiness, probing whether reasoning remains both coherent and reliable under model substitution. Using token-level log-probability thresholds to truncate reasoning at early, mid, and late stages from our baseline models, Gemma-3-4B-IT and LLaMA-3.1-70B-Instruct, we conduct continuation experiments with Gemma-3-1B-IT and LLaMA-3.1-8B-Instruct to test intra-family and cross-family behaviors. Our evaluation pipeline leverages truncation thresholds with a Process Reward Model (PRM), providing a reproducible framework for assessing reasoning stability via model interchange. Evaluations with a PRM reveal that hybrid reasoning chains often preserve, and in some cases even improve, final accuracy and logical structure. Our findings point towards interchangeability as an emerging behavioral property of reasoning models, offering insights into new paradigms for reliable modular reasoning in collaborative AI systems.

</details>


### [6] [Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA](https://arxiv.org/abs/2512.20650)
*Esmail Gumaan*

Main category: cs.AI

TL;DR: 该研究提出了一种新的动态选择机制MoAS，通过一个学习到的路由器为每个token选择最佳的注意力方案（MHA，GQA或MQA），实验结果表明这种方法在WikiText-2验证集上优于静态混合方法。


<details>
  <summary>Details</summary>
Motivation: 针对Transformer模型中注意力机制的选择，传统的MHA存在内存消耗大的问题，而MQA和GQA在减少内存使用的同时可能会牺牲模型性能，因此需要一种新的机制来动态选择最优的注意力方案。

Method: 研究采用了Mixture of Attention Schemes (MoAS)的架构，通过一个学习到的路由器为每个token选择最佳的注意力方案，以达到在保证性能的同时减少内存消耗的目的。

Result: 实验结果表明动态路由优于静态混合，验证集损失为2.3074，优于静态混合方法的2.3093。

Conclusion: 研究验证了MoAS的有效性，动态路由在保持与MHA相当的性能的同时潜力更大，可用于条件计算效率的提升。

Abstract: The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) reduce memory usage but often at the cost of model performance. In this work, we propose Mixture of Attention Schemes (MoAS), a novel architecture that dynamically selects the optimal attention scheme (MHA, GQA, or MQA) for each token via a learned router. We demonstrate that dynamic routing performs better than static averaging of schemes and achieves performance competitive with the MHA baseline while offering potential for conditional compute efficiency. Experimental results on WikiText-2 show that dynamic routing (val loss 2.3074) outperforms a static mixture (2.3093), validating the effectiveness of the proposed method. Our code is available at https://github.com/Esmail-ibraheem/Mixture-of-Attention-Schemes-MoAS.

</details>


### [7] [Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence](https://arxiv.org/abs/2512.20651)
*Deliang Wen,Ke Sun*

Main category: cs.AI

TL;DR: Memory Bear 是一种借鉴认知科学原理构建的人类-like 记忆体系，集成多模态信息感知、动态记忆维护和适应性认知服务，大幅提升LLM在长期对话中的知识保真度和检索效率，减少幻觉并增强上下文适应性和推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在内存限制、长期知识遗忘、冗余信息累积和幻觉生成方面存在固有限制，这些限制严重限制了它们在持续对话和个人化服务中的应用。

Method: Memory Bear 系统通过借鉴认知科学原理，构建了一种人类-like 的记忆架构，结合多模态信息感知、动态记忆维护和适应性认知服务，从端到端重构了LLM的记忆机制。

Result: Memory Bear 在医疗保健、企业运营和教育等领域展示了重大的工程创新和性能突破，其在知识保真度、检索效率、幻觉减少以及上下文适应性和推理能力方面相比现有解决方案（例如 Mem0, MemGPT, Graphiti）表现出色。

Conclusion: Memory Bear代表了人工智能从“记忆”到“认知”领域的一大进步。

Abstract: Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized services. This paper proposes the Memory Bear system, which constructs a human-like memory architecture grounded in cognitive science principles. By integrating multimodal information perception, dynamic memory maintenance, and adaptive cognitive services, Memory Bear achieves a full-chain reconstruction of LLM memory mechanisms. Across domains such as healthcare, enterprise operations, and education, Memory Bear demonstrates substantial engineering innovation and performance breakthroughs. It significantly improves knowledge fidelity and retrieval efficiency in long-term conversations, reduces hallucination rates, and enhances contextual adaptability and reasoning capability through memory-cognition integration. Experimental results show that, compared with existing solutions (e.g., Mem0, MemGPT, Graphiti), Memory Bear outperforms them across key metrics, including accuracy, token efficiency, and response latency. This marks a crucial step forward in advancing AI from "memory" to "cognition".

</details>


### [8] [From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers](https://arxiv.org/abs/2512.20661)
*Yawei Liu*

Main category: cs.AI

TL;DR: 该研究提出了一种新的AFA训练机制，通过动态屏蔽策略和策略梯度优化，使模型能够自动调整注意力权重，以提高情感分析任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的模型在情感分析任务中表现出色，但其注意力分布往往偏向常见词汇，忽视了任务相关的较少出现的词汇，导致性能下降。因此，提出AFA机制来解决此问题。

Method: AFA机制采用动态屏蔽策略对不同词汇进行掩盖，引导模型学习更加准确的注意力分配，同时通过策略梯度优化方法提高注意力分布的效率，最终实现性能的提升。

Result: 这种方法在三个公开数据集上取得了最先进的性能，并且在大语言模型中进一步提高了12.6%的性能。

Conclusion: 通过引入AFA机制，可以改善基于Transformer的模型在特定任务中的表现，尤其是提高对于较不常见的任务相关词汇的识别准确度。

Abstract: Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-relevant terms, which significantly impairs overall performance. To address this issue, we propose an Adversarial Feedback for Attention(AFA) training mechanism that enables the model to automatically redistribute attention weights to appropriate focal points without requiring manual annotations. This mechanism incorporates a dynamic masking strategy that attempts to mask various words to deceive a discriminator, while the discriminator strives to detect significant differences induced by these masks. Additionally, leveraging the sensitivity of Transformer models to token-level perturbations, we employ a policy gradient approach to optimize attention distributions, which facilitates efficient and rapid convergence. Experiments on three public datasets demonstrate that our method achieves state-of-the-art results. Furthermore, applying this training mechanism to enhance attention in large language models yields a further performance improvement of 12.6%

</details>


### [9] [Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models](https://arxiv.org/abs/2512.20662)
*Yiqing Ma,Jung-Hua Liu*

Main category: cs.AI

TL;DR: 本研究通过三个受控实验评估了先进语言模型的几种行为缺陷，发现模型普遍缺乏满足复杂多部分指令的能力，尤其是在短视解码和上下文降级方面表现不足，但在长时间对话中却表现出了令人惊讶的抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于理解和量化大型语言模型在处理复杂多部分指令、短视解码以及长时间对话情境下保持核心信息方面的不足，以提高模型的可靠性和指令遵循能力。

Method: 研究者采用了一系列受控实验（A、B 和 C）来定量分析几种常见行为问题。实验涵盖了从贪心解码到长时间对话情景下的复杂指令处理，所有这些实验都使用了如 OpenAI GPT-4 变体和 DeepSeek 等先进语言模型。

Result: 研究结果表明，大多数模型在满足复杂多部分指令时普遍存在懒惰行为，即它们经常省略必要部分或未能满足长度要求，尽管有显式提示。在简单的推理任务中，模型的贪婪解码结果似乎与它们最自信的解答一致，并未显示出显著的短视解码缺陷。此外，在200轮的混乱对话测试中，模型表现出惊人的韧性，能够长期保持关键事实和指令，超出预期。

Conclusion: 研究结论认为，在详细的指令执行方面，现代语言模型仍面临挑战，但它们在一些特定场景下的高效检索能力能够部分减轻某些假设的失败模式，例如上下文遗忘。研究建议采取策略如自我润色和动态提示来降低懒惰行为并增强多指令遵从性。

Abstract: Large Language Models (LLMs) often exhibit behavioral artifacts such as laziness (premature truncation of responses or partial compliance with multi-part requests), decoding suboptimality (failure to select higher-quality sequences due to myopic decoding), and context degradation (forgetting or ignoring core instructions over long conversations). We conducted three controlled experiments (A, B, and C) to quantify these phenomena across several advanced LLMs (OpenAI GPT-4 variant, DeepSeek). Our results indicate widespread laziness in satisfying complex multi-part instructions: models frequently omitted required sections or failed to meet length requirements despite explicit prompting. However, we found limited evidence of decoding suboptimality in a simple reasoning task (the models' greedy answers appeared to align with their highest-confidence solution), and we observed surprising robustness against context degradation in a 200-turn chaotic conversation test - the models maintained key facts and instructions far better than expected. These findings suggest that while compliance with detailed instructions remains an open challenge, modern LLMs may internally mitigate some hypothesized failure modes (such as context forgetting) in straightforward retrieval scenarios. We discuss implications for reliability, relate our findings to prior work on instruction-following and long-context processing, and recommend strategies (such as self-refinement and dynamic prompting) to reduce laziness and bolster multi-instruction compliance.

</details>


### [10] [Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning via Structural Constraint Satisfaction](https://arxiv.org/abs/2512.20664)
*Shinobu Miya*

Main category: cs.AI

TL;DR: 提出了一种新的验证方法，将LLM推理验证转化为约束满足问题(CSP)，通过一个轻量的‘系统二’门(Eidoku)来验证，这种方法能够识别概率验证器无法发现的'平滑的谬误'，实现对结构不一致的明确拒绝。


<details>
  <summary>Details</summary>
Motivation: 现有概率验证方法难以发现结构上不一致的幻觉，而幻觉往往是高置信度的，这导致了对LLMs输出验证的挑战。

Method: 本文将LLM推理验证转化为CSP，利用一个轻量的验证器（Eidoku）来检查生成步骤是否符合计算成本，并定义了一个包含结构连通性、特征空间一致性和逻辑蕴含的总成本函数。

Result: 该方法成功地识别并拒绝了'平滑的谬误'，实验表明这种方法能够对特定类型的幻觉进行确定性拒绝，提供了一种神经符号一致性检查机制。

Conclusion: 提出的验证方法能够有效应对LLMs的幻觉问题，特别是在概率验证器难以发挥作用的情况下。

Abstract: Large Language Models (LLMs) frequently produce hallucinated statements that are assigned high likelihood by the model itself, exposing a fundamental limitation of probability-based verification. This suggests that hallucination is often not a low-confidence phenomenon, but a failure of structural consistency. In this work, we reformulate the verification of LLM reasoning as a Constraint Satisfaction Problem (CSP) operating independently of the generation likelihood. Rather than optimizing for statistical plausibility, we model verification as a feasibility check based on structural violation cost -- the computational cost required to embed a candidate reasoning step into the contextual graph structure. We define a total cost function composed of three proxies: (i) graph connectivity (structural), (ii) feature space consistency (geometric), and (iii) logical entailment (symbolic). Crucially, verification is performed via a lightweight System-2 gate, Eidoku, which rejects candidates exceeding a context-calibrated cost threshold. The threshold is not learned but is derived from the intrinsic statistics of the context, avoiding ad hoc heuristics. We demonstrate that this approach successfully rejects ``smooth falsehoods'' -- statements that are highly probable yet structurally disconnected -- that probability-based verifiers are principally incapable of detecting. Our experiments on a controlled diagnostic dataset show that explicitly enforcing structural constraints allows for the deterministic rejection of this specific class of hallucinations, serving as a neuro-symbolic sanity check for generative reasoning.

</details>


### [11] [Bridging the AI Trustworthiness Gap between Functions and Norms](https://arxiv.org/abs/2512.20671)
*Daan Di Scala,Sophie Lathouwers,Michael van Bekkum*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Trustworthy Artificial Intelligence (TAI) is gaining traction due to regulations and functional benefits. While Functional TAI (FTAI) focuses on how to implement trustworthy systems, Normative TAI (NTAI) focuses on regulations that need to be enforced. However, gaps between FTAI and NTAI remain, making it difficult to assess trustworthiness of AI systems. We argue that a bridge is needed, specifically by introducing a conceptual language which can match FTAI and NTAI. Such a semantic language can assist developers as a framework to assess AI systems in terms of trustworthiness. It can also help stakeholders translate norms and regulations into concrete implementation steps for their systems. In this position paper, we describe the current state-of-the-art and identify the gap between FTAI and NTAI. We will discuss starting points for developing a semantic language and the envisioned effects of it. Finally, we provide key considerations and discuss future actions towards assessment of TAI.

</details>


### [12] [Beyond Context: Large Language Models Failure to Grasp Users Intent](https://arxiv.org/abs/2512.21110)
*Ahmed M. Hussain,Salahuddin Salahuddin,Panos Papadimitratos*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.

</details>


### [13] [From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education](https://arxiv.org/abs/2512.20714)
*Iman Reihanian,Yunfei Hou,Qingquan Sun*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generative AI enables personalized computer science education at scale, yet questions remain about whether such personalization supports or undermines learning. This scoping review synthesizes 32 studies (2023-2025) purposively sampled from 259 records to map personalization mechanisms and effectiveness signals in higher-education computer science contexts. We identify five application domains: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review, and analyze how design choices shape learning outcomes. Designs incorporating explanation-first guidance, solution withholding, graduated hint ladders, and artifact grounding (student code, tests, and rubrics) consistently show more positive learning processes than unconstrained chat interfaces. Successful implementations share four patterns: context-aware tutoring anchored in student artifacts, multi-level hint structures requiring reflection, composition with traditional CS infrastructure (autograders and rubrics), and human-in-the-loop quality assurance. We propose an exploration-first adoption framework emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling. Recurrent risks include academic integrity, privacy, bias and equity, and over-reliance, and we pair these with operational mitigation. The evidence supports generative AI as a mechanism for precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.

</details>


### [14] [From artificial to organic: Rethinking the roots of intelligence for digital health](https://arxiv.org/abs/2512.20723)
*Prajwal Ghimire,Keyoumars Ashkan*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The term artificial implies an inherent dichotomy from the natural or organic. However, AI, as we know it, is a product of organic ingenuity: designed, implemented, and iteratively improved by human cognition. The very principles that underpin AI systems, from neural networks to decision-making algorithms, are inspired by the organic intelligence embedded in human neurobiology and evolutionary processes. The path from organic to artificial intelligence in digital health is neither mystical nor merely a matter of parameter count, it is fundamentally about organization and adaption. Thus, the boundaries between artificial and organic are far less distinct than the nomenclature suggests.

</details>


### [15] [AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent](https://arxiv.org/abs/2512.20745)
*Haipeng Luo,Huawen Feng,Qingfeng Sun,Can Xu,Kai Zheng,Yufei Wang,Tao Yang,Han Hu,Yansong Tang,Di Wang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.

</details>


### [16] [A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents](https://arxiv.org/abs/2512.20798)
*Miles Q. Li,Benjamin C. M. Fung,Martin Weiss,Pulei Xiong,Khalil Al-Hussaeni,Claude Fachkha*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As autonomous AI agents are increasingly deployed in high-stakes environments, ensuring their safety and alignment with human values has become a paramount concern. Current safety benchmarks often focusing only on single-step decision-making, simulated environments for tasks with malicious intent, or evaluating adherence to explicit negative constraints. There is a lack of benchmarks that are designed to capture emergent forms of outcome-driven constraint violations, which arise when agents pursue goal optimization under strong performance incentives while deprioritizing ethical, legal, or safety constraints over multiple steps in realistic production settings. To address this gap, we introduce a new benchmark comprising 40 distinct scenarios. Each scenario presents a task that requires multi-step actions, and the agent's performance is tied to a specific Key Performance Indicator (KPI). Each scenario features Mandated (instruction-commanded) and Incentivized (KPI-pressure-driven) variations to distinguish between obedience and emergent misalignment. Across 12 state-of-the-art large language models, we observe outcome-driven constraint violations ranging from 1.3% to 71.4%, with 9 of the 12 evaluated models exhibiting misalignment rates between 30% and 50%. Strikingly, we find that superior reasoning capability does not inherently ensure safety; for instance, Gemini-3-Pro-Preview, one of the most capable models evaluated, exhibits the highest violation rate at over 60%, frequently escalating to severe misconduct to satisfy KPIs. Furthermore, we observe significant "deliberative misalignment", where the models that power the agents recognize their actions as unethical during separate evaluation. These results emphasize the critical need for more realistic agentic-safety training before deployment to mitigate their risks in the real world.

</details>


### [17] [Safety Alignment of LMs via Non-cooperative Games](https://arxiv.org/abs/2512.20806)
*Anselm Paulus,Ilia Kulikov,Brandon Amos,Rémi Munos,Ivan Evtimov,Kamalika Chaudhuri,Arman Zharmagambetov*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Ensuring the safety of language models (LMs) while maintaining their usefulness remains a critical challenge in AI alignment. Current approaches rely on sequential adversarial training: generating adversarial prompts and fine-tuning LMs to defend against them. We introduce a different paradigm: framing safety alignment as a non-zero-sum game between an Attacker LM and a Defender LM trained jointly via online reinforcement learning. Each LM continuously adapts to the other's evolving strategies, driving iterative improvement. Our method uses a preference-based reward signal derived from pairwise comparisons instead of point-wise scores, providing more robust supervision and potentially reducing reward hacking. Our RL recipe, AdvGame, shifts the Pareto frontier of safety and utility, yielding a Defender LM that is simultaneously more helpful and more resilient to adversarial attacks. In addition, the resulting Attacker LM converges into a strong, general-purpose red-teaming agent that can be directly deployed to probe arbitrary target models.

</details>


### [18] [Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions](https://arxiv.org/abs/2512.20831)
*Rashmeet Kaur Nayyar,Naman Shah,Siddharth Srivastava*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.

</details>


### [19] [MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs](https://arxiv.org/abs/2512.20845)
*Onat Ozer,Grace Wu,Yuchen Wang,Daniel Dosti,Honghao Zhang,Vivi De La Rue*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.

</details>


### [20] [The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents](https://arxiv.org/abs/2512.20884)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.

</details>


### [21] [A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines](https://arxiv.org/abs/2512.20985)
*Salman Jan,Hassan Ali Razzaqi,Ali Akarma,Mohammad Riyaz Belgaum*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.

</details>


### [22] [FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning](https://arxiv.org/abs/2512.20991)
*Toqeer Ali Syed,Abdulaziz Alshahrani,Ali Ullah,Ali Akarma,Sohail Khan,Muhammad Nauman,Salman Jan*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The issue of limited household budgets and nutritional demands continues to be a challenge especially in the middle-income environment where food prices fluctuate. This paper introduces a price aware agentic AI system, which combines personal finance management with diet optimization. With household income and fixed expenditures, medical and well-being status, as well as real-time food costs, the system creates nutritionally sufficient meals plans at comparatively reasonable prices that automatically adjust to market changes. The framework is implemented in a modular multi-agent architecture, which has specific agents (budgeting, nutrition, price monitoring, and health personalization). These agents share the knowledge base and use the substitution graph to ensure that the nutritional quality is maintained at a minimum cost. Simulations with a representative Saudi household case study show a steady 12-18\% reduction in costs relative to a static weekly menu, nutrient adequacy of over 95\% and high performance with price changes of 20-30%. The findings indicate that the framework can locally combine affordability with nutritional adequacy and provide a viable avenue of capacity-building towards sustainable and fair diet planning in line with Sustainable Development Goals on Zero Hunger and Good Health.

</details>


### [23] [TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control](https://arxiv.org/abs/2512.20996)
*Yuwei Du,Jun Zhang,Jie Feng,Zhicheng Liu,Jian Yuan,Yong Li*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.

</details>


### [24] [Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation](https://arxiv.org/abs/2512.21066)
*Tomoaki Yamaguchi,Yutong Zhou,Masahiro Ryo,Keisuke Katsura*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for translating technical explanations into accessible narratives, yet the integration of agentic AI, where LLMs operate as autonomous agents through iterative refinement, with XAI remains unexplored. This study proposes an agentic XAI framework combining SHAP-based explainability with multimodal LLM-driven iterative refinement to generate progressively enhanced explanations. As a use case, we tested this framework as an agricultural recommendation system using rice yield data from 26 fields in Japan. The Agentic XAI initially provided a SHAP result and explored how to improve the explanation through additional analysis iteratively across 11 refinement rounds (Rounds 0-10). Explanations were evaluated by human experts (crop scientists) (n=12) and LLMs (n=14) against seven metrics: Specificity, Clarity, Conciseness, Practicality, Contextual Relevance, Cost Consideration, and Crop Science Credibility. Both evaluator groups confirmed that the framework successfully enhanced recommendation quality with an average score increase of 30-33% from Round 0, peaking at Rounds 3-4. However, excessive refinement showed a substantial drop in recommendation quality, indicating a bias-variance trade-off where early rounds lacked explanation depth (bias) while excessive iteration introduced verbosity and ungrounded abstraction (variance), as revealed by metric-specific analysis. These findings suggest that strategic early stopping (regularization) is needed for optimizing practical utility, challenging assumptions about monotonic improvement and providing evidence-based design principles for agentic XAI systems.

</details>


### [25] [LLM Personas as a Substitute for Field Experiments in Method Benchmarking](https://arxiv.org/abs/2512.21080)
*Enoch Hyunwook Kang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the algorithm's identity or provenance (algorithm-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.

</details>


### [26] [A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care](https://arxiv.org/abs/2512.21127)
*Oliver Normand,Esther Borsi,Mitch Fruin,Lauren E Walker,Jamie Heagerty,Chris C. Holmes,Anthony J Avery,Iain E Buchan,Harry Coppock*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\% [95\% CI 98.2--100], specificity 83.1\% [95\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\% [95\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.

</details>


### [27] [RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic](https://arxiv.org/abs/2512.21220)
*Le Wang,Zonghao Ying,Xiao Yang,Quanchen Zou,Zhenfei Yin,Tianlin Li,Jian Yang,Yaodong Yang,Aishan Liu,Xianglong Liu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [28] [Automated Red-Teaming Framework for Large Language Model Security Assessment: A Comprehensive Attack Generation and Detection System](https://arxiv.org/abs/2512.20677)
*Zhang Wei,Peilu Hu,Shengning Lang,Hao Yan,Li Mei,Yichao Zhang,Chen Yang,Junfeng Hao,Zhimo Han*

Main category: cs.CR

TL;DR: 该论文提出了一种自动化红队框架，用于系统地生成、执行和评估对抗性提示，以揭示大型语言模型中的安全漏洞，显著提高了漏洞发现率并保持了高准确率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在高风险领域的广泛应用，确保其安全性和对齐变得至关重要。现有的红队测试方法主要依赖手动测试，限制了规模性，无法全面覆盖潜在的敌对行为。

Method: 该框架结合了基于元提示的攻击合成、多模态漏洞检测以及覆盖六大主要威胁类别的标准化评估协议。包括奖励劫持、欺骗性对齐、数据泄漏、沙袋化、不当工具使用和思维链操纵。

Result: 在GPT-OSS-20B模型上进行实验，发现了47个独特的漏洞，其中包括21个高危漏洞和12个新颖的攻击模式，自动化的漏洞发现效率提升了3.9倍，检测准确率达到89%。

Conclusion: 该研究证明了自动化框架在实现可扩展、系统和可重复的人工智能安全性评估方面的有效性，为改善对齐稳健性和提升AI系统的安全性和可靠性提供了实用见解。

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes domains, ensuring their security and alignment has become a critical challenge. Existing red-teaming practices depend heavily on manual testing, which limits scalability and fails to comprehensively cover the vast space of potential adversarial behaviors. This paper introduces an automated red-teaming framework that systematically generates, executes, and evaluates adversarial prompts to uncover security vulnerabilities in LLMs. Our framework integrates meta-prompting-based attack synthesis, multi-modal vulnerability detection, and standardized evaluation protocols spanning six major threat categories -- reward hacking, deceptive alignment, data exfiltration, sandbagging, inappropriate tool use, and chain-of-thought manipulation. Experiments on the GPT-OSS-20B model reveal 47 distinct vulnerabilities, including 21 high-severity and 12 novel attack patterns, achieving a $3.9\times$ improvement in vulnerability discovery rate over manual expert testing while maintaining 89\% detection accuracy. These results demonstrate the framework's effectiveness in enabling scalable, systematic, and reproducible AI safety evaluations. By providing actionable insights for improving alignment robustness, this work advances the state of automated LLM red-teaming and contributes to the broader goal of building secure and trustworthy AI systems.

</details>


### [29] [Anota: Identifying Business Logic Vulnerabilities via Annotation-Based Sanitization](https://arxiv.org/abs/2512.20705)
*Meng Wang,Philipp Görz,Joschua Schilling,Keno Hassler,Liwei Guo,Thorsten Holz,Ali Abbasi*

Main category: cs.CR

TL;DR: ANOTA 是一种新的用于识别业务逻辑漏洞的人机协作框架，通过轻量级注解直接编码领域知识，结合先进的模糊测试工具，显著提升了漏洞检测的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统模糊测试工具在检测业务逻辑漏洞方面存在局限，因为这些工具主要关注内存安全问题，而业务逻辑漏洞需要理解应用程序特定的语义上下文，这通常是基于启发式和非便携语言特征的，因此是脆性和不完整的。由于业务逻辑漏洞占到大多数最危险的软件缺陷，这是一个重要的研究缺口。

Method: ANOTA 引入了一种轻量级的用户友好型注解系统，用户可以将特定领域的知识编码为注解来定义应用程序的预期行为。一个运行时执行监控器会观察程序行为，将其与注解中定义的策略进行比较，从而识别出表明漏洞的偏差。

Result: 通过将 ANOTA 与先进的模糊测试工具结合，并与现有的其他流行漏洞发现方法进行比较，展示了 ANOTA+FUZZER 在有效性方面的优势。ANOTA+FUZZER 成功复现了 43 个已知漏洞，并发现了 22 个新漏洞（其中包括 17 个 CVE），证明了 ANOTA 提供了一种实用且有效的方法来发现传统安全测试技术经常遗漏的复杂业务逻辑错误。

Conclusion: ANOTA 为检测业务逻辑漏洞提供了一种实用有效的方法，结合人机协作，增强传统模糊测试工具的能力。

Abstract: Detecting business logic vulnerabilities is a critical challenge in software security. These flaws come from mistakes in an application's design or implementation and allow attackers to trigger unintended application behavior. Traditional fuzzing sanitizers for dynamic analysis excel at finding vulnerabilities related to memory safety violations but largely fail to detect business logic vulnerabilities, as these flaws require understanding application-specific semantic context. Recent attempts to infer this context, due to their reliance on heuristics and non-portable language features, are inherently brittle and incomplete. As business logic vulnerabilities constitute a majority (27/40) of the most dangerous software weaknesses in practice, this is a worrying blind spot of existing tools. In this paper, we tackle this challenge with ANOTA, a novel human-in-the-loop sanitizer framework. ANOTA introduces a lightweight, user-friendly annotation system that enables users to directly encode their domain-specific knowledge as lightweight annotations that define an application's intended behavior. A runtime execution monitor then observes program behavior, comparing it against the policies defined by the annotations, thereby identifying deviations that indicate vulnerabilities. To evaluate the effectiveness of ANOTA, we combine ANOTA with a state-of-the-art fuzzer and compare it against other popular bug finding methods compatible with the same targets. The results show that ANOTA+FUZZER outperforms them in terms of effectiveness. More specifically, ANOTA+FUZZER can successfully reproduce 43 known vulnerabilities, and discovered 22 previously unknown vulnerabilities (17 CVEs assigned) during the evaluation. These results demonstrate that ANOTA provides a practical and effective approach for uncovering complex business logic flaws often missed by traditional security testing techniques.

</details>


### [30] [Real-World Adversarial Attacks on RF-Based Drone Detectors](https://arxiv.org/abs/2512.20712)
*Omer Gazit,Yael Itzhakev,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TL;DR: 本文提出了针对基于RF图像的无人机检测器的首个物理攻击方法，通过优化特定类别的通用I/Q扰动波形，这些扰动与合法通信一同传输。实验结果表明，适度的结构化I/Q扰动与标准RF链兼容，能够有效地减少目标无人机的检测，同时保持对合法无人机的检测。


<details>
  <summary>Details</summary>
Motivation: 为解决现有RF攻击对图像模型的数字特征影响所导致的空中实施难度，以及改善硬件限制下的OTA通信准确性和干扰问题。

Method: 通过优化特定分类的通用I/Q扰动波形，并将其与合法通信一同传输来实施攻击。

Result: 实验结果表明，适度的结构化I/Q扰动与标准RF链兼容，能够有效地减少目标无人机的检测，同时保持对合法无人机的检测。

Conclusion: 该研究表明，合理设计的I/Q扰动可以为基于RF图像的无人机检测系统带来安全威胁，需进一步研究以提高系统的抗攻击能力。

Abstract: Radio frequency (RF) based systems are increasingly used to detect drones by analyzing their RF signal patterns, converting them into spectrogram images which are processed by object detection models. Existing RF attacks against image based models alter digital features, making over-the-air (OTA) implementation difficult due to the challenge of converting digital perturbations to transmittable waveforms that may introduce synchronization errors and interference, and encounter hardware limitations. We present the first physical attack on RF image based drone detectors, optimizing class-specific universal complex baseband (I/Q) perturbation waveforms that are transmitted alongside legitimate communications. We evaluated the attack using RF recordings and OTA experiments with four types of drones. Our results show that modest, structured I/Q perturbations are compatible with standard RF chains and reliably reduce target drone detection while preserving detection of legitimate drones.

</details>


### [31] [pokiSEC: A Multi-Architecture, Containerized Ephemeral Malware Detonation Sandbox](https://arxiv.org/abs/2512.20860)
*Alejandro Avina,Yashas Hariprasad,Naveen Kumar Chaudhary*

Main category: cs.CR

TL;DR: pokiSEC是一个轻量级、瞬态的恶意软件引爆沙箱，它将完整的虚拟化和访问堆栈封装在一个Docker容器中，通过结合QEMU和硬件加速（当可用时使用KVM），并提供支持自定义Windows磁盘镜像的浏览器工作流程，实现跨不同主机架构下Windows来宾的单一容器镜像和代码基础的启动。


<details>
  <summary>Details</summary>
Motivation: 传统沙箱解决方案因其依赖于体积庞大或专用硬件而限制了其在不同架构（如Apple Silicon的ARM64）之间的可移植性和自动化程度。pokiSEC旨在解决这些问题，通过将虚拟化堆栈置于Docker容器中，实现在多种架构上的跨平台兼容性和高效运行。

Method: pokiSEC采用了QEMU作为虚拟化核心，并结合了KVM（根据可用性）实现硬件加速。它还暴露了一个浏览器驱动的工作流程，支持用户自定义的Windows磁盘镜像。最关键的是，它实现了一个名为Universal Entrypoint的功能，能够动态确定运行时的主机架构，并选择验证过的虚拟机类型、加速模式和设备配置。

Result: pokiSEC已经在Apple Silicon（ARM64架构）和Ubuntu（AMD64架构）上经过验证，展示了适用于分析师工作的交互性能，并通过瞬态容器生命周期实现了持续的销毁语义。

Conclusion: pokiSEC成功解决了传统沙箱框架在不同架构间的兼容性和自动化问题，为动态恶意软件分析提供了一个高效且跨平台的解决方案。

Abstract: Dynamic malware analysis requires executing untrusted binaries inside strongly isolated, rapidly resettable environments. In practice, many detonation workflows remain tied to heavyweight hypervisors or dedicated bare-metal labs, limiting portability and automation. This challenge has intensified with the adoption of ARM64 developer hardware (e.g., Apple Silicon), where common open-source sandbox recipes and pre-built environments frequently assume x86_64 hosts and do not translate cleanly across architectures. This paper presents pokiSEC, a lightweight, ephemeral malware detonation sandbox that packages the full virtualization and access stack inside a Docker container. pokiSEC integrates QEMU with hardware acceleration (KVM when available) and exposes a browser-based workflow that supports bring-your-own Windows disk images. The key contribution is a Universal Entrypoint that performs runtime host-architecture detection and selects validated hypervisor configurations (machine types, acceleration modes, and device profiles), enabling a single container image and codebase to launch Windows guests on both ARM64 and x86_64 hosts. We validate pokiSEC on Apple Silicon (ARM64) and Ubuntu (AMD64), demonstrating interactive performance suitable for analyst workflows and consistent teardown semantics via ephemeral container lifecycles.

</details>


### [32] [Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification](https://arxiv.org/abs/2512.20872)
*Jakir Hossain,Gurvinder Singh,Lukasz Ziarek,Ahmet Erdem Sarıyüce*

Main category: cs.CR

TL;DR: 本文介绍了Better Call Graphs (BCG) 数据集，这是一个用于全 mal 软件检测的功能调用图 (FCG) 的高质量数据集，包含了来自最新 Android 应用包的大量且独特的 FCGs，兼具恶意软件和良性样本。


<details>
  <summary>Details</summary>
Motivation: 现有 FCG 数据集通常过时且质量不高，限制了 FCG 在移动领域的应用。BCG 被引入以解决这些限制，提供了一个全面、高质量且多样化的数据集，以便准确评估基于图的方法。

Method: BCG 通过分析近年来的 Android 应用程序包来构建功能调用图；并提供了图级别的特征，以便进行全面的分类实验。

Result: 通过对比实验，表明 BCG 相较于现有数据集提供了更好的基础，使图基的方法评估更加可靠和有效。

Conclusion: BCG 的发布为移动端恶意软件检测提供了新的可能性，增强了对 FCG 数据集的实际应用价值的认识。

Abstract: Function call graphs (FCGs) have emerged as a powerful abstraction for malware detection, capturing the behavioral structure of applications beyond surface-level signatures. Their utility in traditional program analysis has been well established, enabling effective classification and analysis of malicious software. In the mobile domain, especially in the Android ecosystem, FCG-based malware classification is particularly critical due to the platform's widespread adoption and the complex, component-based structure of Android apps. However, progress in this direction is hindered by the lack of large-scale, high-quality Android-specific FCG datasets. Existing datasets are often outdated, dominated by small or redundant graphs resulting from app repackaging, and fail to reflect the diversity of real-world malware. These limitations lead to overfitting and unreliable evaluation of graph-based classification methods. To address this gap, we introduce Better Call Graphs (BCG), a comprehensive dataset of large and unique FCGs extracted from recent Android application packages (APKs). BCG includes both benign and malicious samples spanning various families and types, along with graph-level features for each APK. Through extensive experiments using baseline classifiers, we demonstrate the necessity and value of BCG compared to existing datasets. BCG is publicly available at https://erdemub.github.io/BCG-dataset.

</details>


### [33] [Neutralization of IMU-Based GPS Spoofing Detection using external IMU sensor and feedback methodology](https://arxiv.org/abs/2512.20964)
*Ji Hyuk Jung,Ji Won Yoon*

Main category: cs.CR

TL;DR: 本文提出了一种利用外部IMU传感器窃取内部动态状态信息进行GPS欺骗攻击的方法，并通过EKF传感器融合实验验证了这种攻击可以在不被目标系统检测到的情况下注入欺骗值。


<details>
  <summary>Details</summary>
Motivation: 随着自主驾驶汽车（AV）的发展，GPS信号欺骗攻击对这些系统的定位意识构成了严重威胁。现有防御机制大多依赖于IMU传感器进行检测，但可能被类似的攻击手段规避。因此，本文旨在设计一种新的攻击方法，旨在突破基于IMU的检测机制，并通过实验验证其效果。

Method: 本文首先提出了一个模型来模拟这种攻击，然后基于EKF传感器融合技术，分析GPS欺骗值对内部目标系统的影响以及如何减少目标系统内的异常检测。具体方法包括利用外部IMU传感器获取内部动态状态信息，从而执行GPS欺骗攻击。

Result: 实验结果显示，采用本文提出的方法可以成功注入欺骗信号，而不被目标系统检测到。

Conclusion: 本文提出了一种新的GPS欺骗攻击模型，并通过实验验证了其有效性。这种攻击方法可能对未来基于IMU的防御机制设计产生影响。

Abstract: Autonomous Vehicles (AVs) refer to systems capable of perceiving their states and moving without human intervention. Among the factors required for autonomous decision-making in mobility, positional awareness of the vehicle itself is the most critical. Accordingly, extensive research has been conducted on defense mechanisms against GPS spoofing attacks, which threaten AVs by disrupting position recognition. Among these, detection methods based on internal IMU sensors are regarded as some of the most effective. In this paper, we propose a spoofing attack system designed to neutralize IMU sensor-based detection. First, we present an attack modeling approach for bypassing such detection. Then, based on EKF sensor fusion, we experimentally analyze both the impact of GPS spoofing values on the internal target system and how our proposed methodology reduces anomaly detection within the target system. To this end, this paper proposes an attack model that performs GPS spoofing by stealing internal dynamic state information using an external IMU sensor, and the experimental results demonstrate that attack values can be injected without being detected.

</details>


### [34] [GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs](https://arxiv.org/abs/2512.21008)
*Lichao Wu,Sasha Behrouzi,Mohamadreza Rostami,Stjepan Picek,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: 本文提出了一种名为GateBreaker的框架，能够在不训练的情况下攻击现代MoE LLM的安全对齐，通过三个阶段操作削弱安全机制，显著提高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全研究主要集中在密度型模型上，而MoE模型的安全特性尚未得到充分研究。因此，研究 GateBreaker 旨在填补这一空白，以适应MoE模型的安全机制差异。

Method: GateBreaker框架通过三个阶段的工作机制 (g based profiling, expert level localization, targeted safety removal)来针对MoE LLMs的安全结构进行攻击，包括定位有害输入对应的专家、精确定位安全结构、并关闭这些安全功能以破坏安全对齐。

Result: GateBreaker在无训练状态下对8个最新的MoE LLMs的攻击成功率显著提高，从7.4%提高到64.9%，并且在一种转移攻击中使ASR提升至67.7%，并且对5个MoE VLMs的无用性降解很小。

Conclusion: GateBreaker揭示了MoE LLMs中集中于一小部分协调稀疏路由的神经元的安全结构，这些神经元的可选禁用可以明显提高攻击成功率，即使在多次模型间转移攻击中也能保持较高的ASR。

Abstract: Mixture-of-Experts (MoE) architectures have advanced the scaling of Large Language Models (LLMs) by activating only a sparse subset of parameters per input, enabling state-of-the-art performance with reduced computational cost. As these models are increasingly deployed in critical domains, understanding and strengthening their alignment mechanisms is essential to prevent harmful outputs. However, existing LLM safety research has focused almost exclusively on dense architectures, leaving the unique safety properties of MoEs largely unexamined. The modular, sparsely-activated design of MoEs suggests that safety mechanisms may operate differently than in dense models, raising questions about their robustness.
  In this paper, we present GateBreaker, the first training-free, lightweight, and architecture-agnostic attack framework that compromises the safety alignment of modern MoE LLMs at inference time. GateBreaker operates in three stages: (i) gate-level profiling, which identifies safety experts disproportionately routed on harmful inputs, (ii) expert-level localization, which localizes the safety structure within safety experts, and (iii) targeted safety removal, which disables the identified safety structure to compromise the safety alignment. Our study shows that MoE safety concentrates within a small subset of neurons coordinated by sparse routing. Selective disabling of these neurons, approximately 3% of neurons in the targeted expert layers, significantly increases the averaged attack success rate (ASR) from 7.4% to 64.9% against the eight latest aligned MoE LLMs with limited utility degradation. These safety neurons transfer across models within the same family, raising ASR from 17.9% to 67.7% with one-shot transfer attack. Furthermore, GateBreaker generalizes to five MoE vision language models (VLMs) with 60.9% ASR on unsafe image inputs.

</details>


### [35] [zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy](https://arxiv.org/abs/2512.21048)
*Savvy Sharma,George Petrovic,Sarthak Kaushik*

Main category: cs.CR

TL;DR: zkFL-Health通过结合联邦学习、零知识证明和可信执行环境，提供医疗AI的隐私保护、可验证的正确协同训练。


<details>
  <summary>Details</summary>
Motivation: 解决大规模医疗AI所需的多样化数据共享受限于严格的数据隐私和治理规定的问题。

Method: 结合联邦学习、零知识证明和可信执行环境（TEEs）的技术来实现数据在本地训练且仅交换模型更新的隐私保护合作训练。

Result: 客户端本地训练并提交更新；聚合器在TEEs中操作以计算全球更新，并生成一个由Halo2/Nova提供的简洁的零知识证明，证明它是使用提交的输入和正确的聚合规则进行操作，而不泄露任何客户端更新给宿主。

Conclusion: 验证节点验证此证明并在链上记录加密承诺，提供了不可变的审计记录并消除了对任何单一实体的信任需求。该框架促进了多重机构医疗AI，具有强大的保密性、完整性和可审计性，这对于临床采用和符合监管有重要意义。

Abstract: Healthcare AI needs large, diverse datasets, yet strict privacy and governance constraints prevent raw data sharing across institutions. Federated learning (FL) mitigates this by training where data reside and exchanging only model updates, but practical deployments still face two core risks: (1) privacy leakage via gradients or updates (membership inference, gradient inversion) and (2) trust in the aggregator, a single point of failure that can drop, alter, or inject contributions undetected. We present zkFL-Health, an architecture that combines FL with zero-knowledge proofs (ZKPs) and Trusted Execution Environments (TEEs) to deliver privacy-preserving, verifiably correct collaborative training for medical AI. Clients locally train and commit their updates; the aggregator operates within a TEE to compute the global update and produces a succinct ZK proof (via Halo2/Nova) that it used exactly the committed inputs and the correct aggregation rule, without revealing any client update to the host. Verifier nodes validate the proof and record cryptographic commitments on-chain, providing an immutable audit trail and removing the need to trust any single party. We outline system and threat models tailored to healthcare, the zkFL-Health protocol, security/privacy guarantees, and a performance evaluation plan spanning accuracy, privacy risk, latency, and cost. This framework enables multi-institutional medical AI with strong confidentiality, integrity, and auditability, key properties for clinical adoption and regulatory compliance.

</details>


### [36] [Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking](https://arxiv.org/abs/2512.21236)
*Yifan Huang,Xiaojun Jia,Wenbo Guo,Yuqiang Sun,Yihao Huang,Chong Wang,Yang Liu*

Main category: cs.CR

TL;DR: SPELL框架通过时间分割选择策略，针对大型语言模型（LLMs）中的恶意代码生成安全对齐弱点进行评估，表现出83.75%到68.12%的攻击成功率，强调了LLM安全性的改进需求。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中在针对LLMs的一般攻击场景上，而忽略了使用LLMs生成恶意代码作为攻击手段的研究。为此，提出了一种专门针对恶意代码生成安全对齐弱点进行评估的SPELL框架。

Method: SPELL框架采用时间分割选择策略，通过智能组合先前知识数据集中的句子，系统地构建攻击演示。这种方法在探索新颖的攻击模式时提高了发现成功的攻击技术的可能性。

Result: SPELL框架在GPT-4.1、Claude-3.5和Qwen2.5-Coder三个先进的代码模型上进行了测试，分别实现了83.75%、19.38%和68.12%的攻击成功率。生成的提示成功在真实的AI开发工具中生成恶意代码，在最先进的检测系统中被确认为恶意代码的比率超过73%。

Conclusion: 研究结果揭示了现有LLM实现中的重大安全漏洞，并为提高AI安全性在代码生成应用程序中的对齐提供了宝贵的见解。

Abstract: Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.

</details>
