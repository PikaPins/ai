<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 5]
- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [The TCF doesn't really A(A)ID -- Automatic Privacy Analysis and Legal Compliance of TCF-based Android Applications](https://arxiv.org/abs/2602.20222)
*Victor Morel,Cristiana Santos,Pontus Carlsson,Joel Ahlinder,Romaric Duvignau*

Main category: cs.CR

TL;DR: 研究发现，在Google Play Store上的4482个流行Android应用中，有12.85%的程序使用TCF框架，部分应用存在隐私违规行为，其中2.6%的应用仅在同意时存储用户选择，而不允许拒绝同意的用户关闭提示。进一步分析显示，超过三分之二使用TCF的应用在被动阶段传输了个人数据信息，在用户与隐私设置交互之前就泄露了Android广告标识符。


<details>
  <summary>Details</summary>
Motivation: 探讨TCF在移动应用中的实际应用情况，验证其在Android应用中的使用合规性，并深入研究可能存在的隐私泄露问题。

Method: 通过下载并分析Google Play Store上4482个最受欢迎的Android应用，研究其是否使用TCF，以及是否合理处理用户的同意选择。使用自动化脚本下载应用并在模拟设备上分析其流量，了解安装和使用阶段的数据传输情况。

Result: 研究揭示了部分使用TCF的应用存在隐私违规，具体表现为用户选择存储不完整以及在用户没有同意前就传输数据。

Conclusion: 需要进一步改善TCF在Android应用中的执行情况，确保用户隐私得到妥善保护。

Abstract: The Transparency and Consent Framework (TCF), developed by the Interactive Advertising Bureau (IAB) Europe, provides a de facto standard for requesting, recording, and managing user consent from European end-users. This framework has previously been found to infringe European data protection law and has subsequently been regularly updated. Previous research on the TCF focused exclusively on web contexts, with no attention given to its implementation in mobile applications. No work has systematically studied the privacy implications of the TCF on Android apps. To address this gap, we investigate the prevalence of the TCF in popular Android apps from the Google Play Store, and assess whether these apps respect users' consent banner choices. By scraping and downloading 4482 of the most popular Google Play Store apps on an emulated Android device, we automatically determine which apps use the TCF, automatically interact with consent banners, and analyze the apps' traffic in two different stages, passive (post choices) and active (during banner interaction and post choices).
  We found that 576 (12.85%) of the 4482 downloadable apps in our dataset implemented the TCF, and we identified potential privacy violations within this subset. In 15 (2.6%) of these apps, users' choices are stored only when consent is granted. Users who refuse consent are shown the consent banner again each time they launch the app. Network traffic analysis conducted during the passive stage reveals that 66.2% of the analyzed TCF-based apps share personal data, through the Android Advertising ID (AAID), in the absence of a lawful basis for processing. 55.3% of apps analyzed during the active stage share AAID before users interact with the apps' consent banners, violating the prior consent requirement.

</details>


### [2] [OptiLeak: Efficient Prompt Reconstruction via Reinforcement Learning in Multi-tenant LLM Services](https://arxiv.org/abs/2602.20595)
*Longxiang Wang,Xiang Zheng,Xuhao Zhang,Yao Zhang,Ye Wu,Cong Wang*

Main category: cs.CR

TL;DR: 通过引入OptiLeak框架，提出了一种基于奖励学习的方法来最大化提示重建效率，并利用领域特定的难以预测但充满敏感信息的‘硬令牌’自动识别，显著降低了缓存基于提示泄露的风险。


<details>
  <summary>Details</summary>
Motivation: 当前的共享缓存机制虽然提高了效率，但也可能导致提示泄露的安全漏洞，而现有研究更多关注攻击范围的扩大而非攻击性能的优化。

Method: OptiLeak框架采用两阶段微调，并通过概率排名自动识别领域特定的难以预测但包含敏感信息的‘硬令牌’，进而进行直接偏好优化。

Result: 该框架在三个包含医疗和金融领域的基准测试中，平均每个令牌请求减少了12.48倍，且在不同参数规模的模型上均实现了一致的改进。

Conclusion: 验证了基于缓存的提示泄露要比已报告的问题更为严重，强调生产部署中需要进行严格的缓存隔离。

Abstract: Multi-tenant LLM serving frameworks widely adopt shared Key-Value caches to enhance efficiency. However, this creates side-channel vulnerabilities enabling prompt leakage attacks. Prior studies identified these attack surfaces yet focused on expanding attack vectors rather than optimizing attack performance, reporting impractically high attack costs that underestimate the true privacy risk. We propose OptiLeak, a reinforcement learning-enhanced framework that maximizes prompt reconstruction efficiency through two-stage fine-tuning. Our key insight is that domain-specific ``hard tokens'' -- terms difficult to predict yet carrying sensitive information -- can be automatically identified via likelihood ranking and used to construct preference pairs for Direct Preference Optimization, eliminating manual annotation. This enables effective preference alignment while avoiding the overfitting issues of extended supervised fine-tuning. Evaluated on three benchmarks spanning medical and financial domains, OptiLeak achieves up to $12.48\times$ reduction in average requests per token compared to baseline approaches, with consistent improvements across model scales from 3B to 14B parameters. Our findings demonstrate that cache-based prompt leakage poses a more severe threat than previously reported, underscoring the need for robust cache isolation in production deployments.

</details>


### [3] [Post-Quantum Sanitizable Signatures from McEliece-Based Chameleon Hashing](https://arxiv.org/abs/2602.20657)
*Shahzad Ahmad,Stefan Rass,Zahra Seyedi*

Main category: cs.CR

TL;DR: 该研究提出了一种基于McEliece密码系统的新型后量子时期可清理签名方案。通过使用甘伯斯解码器和特定的随机化器权重限制，实现了可授权修改部分内容的同时保持其他内容不可篡改。


<details>
  <summary>Details</summary>
Motivation: 在面临量子计算威胁的应用场景下，保障信息的完整性和可修改性，同时确保签名的透明性和不可伪造性。

Method: 研究利用Chameleon哈希函数和McEliece密码系统的Goppa代码设计了一种独特的签名方案。通过特定的随机化器权重限制来控制签名的透明度。

Result: 提出了一个具备完美透明性和不可伪造性的后量子时期可清理签名方案，验证了其在随机预言模型中的安全性。

Conclusion: 该研究为长期安全应用提供了理论保障，并指明了其实用性的实施路径。

Abstract: We introduce a novel post-quantum sanitizable signature scheme constructed upon a chameleon hash function derived from the McEliece cryptosystem. In this design, the designated sanitizer possesses the inherent trapdoor of a Goppa code, which facilitates controlled collision-finding via Patterson decoding. This mechanism enables authorized modification of specific message blocks while ensuring all other content remains immutably bound. We provide formal security definitions and rigorous proofs of existential unforgeability and immutability, grounded in the hardness of syndrome decoding in the random-oracle model, where a robust random oracle thwarts trivial linear hash collisions. A key innovation lies in our precise characterization of the transparency property: by imposing a specific weight constraint on the randomizers generated by the signer, we achieve perfect transparency, rendering sanitized signatures indistinguishable from freshly signed ones. This work establishes the first transparent, code-based, post-quantum sanitizable signature scheme, offering strong theoretical guarantees and a pathway for practical deployment in long-term secure applications.

</details>


### [4] [Vanishing Watermarks: Diffusion-Based Image Editing Undermines Robust Invisible Watermarking](https://arxiv.org/abs/2602.20680)
*Fan Guo,Jiyu Kang,Qi Ming,Emily Davis,Finn Carter*

Main category: cs.CR

TL;DR: 本文研究了扩散模型对鲁棒水印系统的威胁，证明了一种指导化的扩散攻击能显著降低水印的可检测性，并且扩散变换会使水标志与隐藏信息的互信息接近于零，导致解码失败。实验结果表明，多种先进的水印方法在经过扩散编辑后几乎无法恢复水印，但图像的视觉保真度仍然很高。


<details>
  <summary>Details</summary>
Motivation: 讨论了当前鲁棒水印技术面临的挑战，特别是扩散基生成模型对其造成的威胁。鉴于扩散模型在图像生成和编辑中的广泛应用，这一研究旨在评估和揭示其对抗鲁棒水印系统的有效性，并通过理论证明和实验证据强调需要开发新的抗扩散编辑的水印策略。

Method: 通过理论分析扩散模型如何消除水印，实验评估了不同鲁棒水印方法在面对扩散编辑时的性能，并引入了一种指导化的扩散攻击方法来针对嵌入的水印信号。

Result: 研究证明了扩散编辑能够有效移除水印，即使这些水印旨在抵御常规干扰。实验结果显示，多款先进的鲁棒水印算法在经过扩散处理后几乎无法恢复水印，同时生成的图像的视觉保真度仍然很高。

Conclusion: 本文揭示了扩散基生成模型对当前鲁棒水印技术的潜在威胁，并呼吁开发新的抗扩散编辑的水印策略以应对这一挑战。

Abstract: Robust invisible watermarking schemes aim to embed hidden information into images such that the watermark survives common manipulations. However, powerful diffusion-based image generation and editing techniques now pose a new threat to these watermarks. In this paper, we present a comprehensive theoretical and empirical analysis demonstrating that diffusion models can effectively erase robust watermarks even when those watermarks were designed to withstand conventional distortions. We show that a diffusion-driven image regeneration process, which leverages generative models to recreate an image, can remove embedded watermarks while preserving the image's perceptual content. Furthermore, we introduce a guided diffusion-based attack that explicitly targets the embedded watermark signal during generation, significantly degrading watermark detectability. Theoretically, we prove that as an image undergoes sufficient diffusion transformations, the mutual information between the watermarked image and the hidden payload approaches zero, leading to inevitable decoding failure. Experimentally, we evaluate multiple state-of-the-art watermarking methods (including deep learning-based schemes like StegaStamp, TrustMark, and VINE) and demonstrate that diffusion edits yield near-zero watermark recovery rates after attack, while maintaining high visual fidelity of the regenerated images. Our findings reveal a fundamental vulnerability in current robust watermarking techniques against generative model-based edits, underscoring the need for new strategies to ensure watermark resilience in the era of powerful diffusion models.

</details>


### [5] [A Secure and Interoperable Architecture for Electronic Health Record Access Control and Sharing](https://arxiv.org/abs/2602.20830)
*Tayeb Kenaza,Islam Debicha,Youcef Fares,Mehdi Sehaki,Sami Messai*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Electronic Health Records (EHRs) store sensitive patient information, necessitating stringent access control and sharing mechanisms to uphold data security and comply with privacy regulations such as the General Data Protection Regulation (GDPR). In this paper, we propose a comprehensive architecture with a suite of efficient protocols that leverage the synergistic capabilities of the Blockchain and Interplanetary File System (IPFS) technologies to enable secure access control and sharing of EHRs. Our approach is based on a private blockchain, wherein smart contracts are deployed to enforce control exclusively by patients. By granting patients exclusive control over their EHRs, our solution ensures compliance with personal data protection laws and empowers individuals to manage their health information autonomously. Notably, our proposed architecture seamlessly integrates with existing health provider information systems, facilitating interoperability and effectively addressing security and data heterogeneity challenges. To demonstrate the effectiveness of our approach, we developed a prototype based on a private implementation of the Hyperledger platform, enabling the simulation of diverse scenarios involving access control and health data sharing among healthcare practitioners. Our experimental results demonstrate the scalability of our solution, thereby substantiating its efficacy and robustness in real-world healthcare settings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [An artificial intelligence framework for end-to-end rare disease phenotyping from clinical notes using large language models](https://arxiv.org/abs/2602.20324)
*Cathy Shyr,Yan Hu,Rory J. Tinker,Thomas A. Cassini,Kevin W. Byram,Rizwan Hamid,Daniel V. Fabbri,Adam Wright,Josh F. Peterson,Lisa Bastarache,Hua Xu*

Main category: cs.AI

TL;DR: RARE-PHENIX 是一个端到端的AI框架，用于罕见病表型表征，整合了基于大语言模型的表型提取、基于本体的标准规范化以及监督排序以突出诊断信息相关的表型。研究结果表明，它在整体评估中优于现有的PhenoBERT，且在临床应用中比单一表型提取任务更具价值。


<details>
  <summary>Details</summary>
Motivation: 现有的人工智计算机方法通常只优化了罕见病表型的单一组件，而RARE-PHENIX 则填补了这一空白，旨在提供一个完整的临床工作流来实现从临床文本中提取特征、将它们标准转换为HPO术语并优先考虑诊断相关的HPO术语。

Method: RARE-PHENIX采用了大语言模型进行表型提取，使用本体论进行标准化至HPO术语，并采用监督学习对诊断信息更丰富的表型进行排序的方法。它利用了来自11家临床中心2671名患者的数据进行训练，并通过Vanderbilt大学医学中心的实际临床记录进行了外部验证。

Result: RARE-PHENIX 在基于本体的相似度和精确召回F1指标上超过了PhenoBERT。当在临床工作流模型中使用每个模块时，展示了性能改进，证明了建模完整的临床表型流程的价值。它能更好地与临床人员的标记术语保持一致，有助于在实际环境中支持带有临床人员共同参与的罕见病诊断。

Conclusion: RARE-PHENIX 拓展了 AI 在罕见病表型分析中的应用，为临床实践中的人机交互式诊断提供了新的工具。

Abstract: Phenotyping is fundamental to rare disease diagnosis, but manual curation of structured phenotypes from clinical notes is labor-intensive and difficult to scale. Existing artificial intelligence approaches typically optimize individual components of phenotyping but do not operationalize the full clinical workflow of extracting features from clinical text, standardizing them to Human Phenotype Ontology (HPO) terms, and prioritizing diagnostically informative HPO terms. We developed RARE-PHENIX, an end-to-end AI framework for rare disease phenotyping that integrates large language model-based phenotype extraction, ontology-grounded standardization to HPO terms, and supervised ranking of diagnostically informative phenotypes. We trained RARE-PHENIX using data from 2,671 patients across 11 Undiagnosed Diseases Network clinical sites, and externally validated it on 16,357 real-world clinical notes from Vanderbilt University Medical Center. Using clinician-curated HPO terms as the gold standard, RARE-PHENIX consistently outperformed a state-of-the-art deep learning baseline (PhenoBERT) across ontology-based similarity and precision-recall-F1 metrics in end-to-end evaluation (i.e., ontology-based similarity of 0.70 vs. 0.58). Ablation analyses demonstrated performance improvements with the addition of each module in RARE-PHENIX (extraction, standardization, and prioritization), supporting the value of modeling the full clinical phenotyping workflow. By modeling phenotyping as a clinically aligned workflow rather than a single extraction task, RARE-PHENIX provides structured, ranked phenotypes that are more concordant with clinician curation and has the potential to support human-in-the-loop rare disease diagnosis in real-world settings.

</details>


### [7] [DMCD: Semantic-Statistical Framework for Causal Discovery](https://arxiv.org/abs/2602.20333)
*Samarth KaPatel,Sofia Nikiforova,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: DMCD是一种两阶段因果发现框架，结合了基于LLM的变量元数据语义草图和观测数据的统计验证，在多个实际数据集上表现出色，尤其是在召回率和F1分数方面。


<details>
  <summary>Details</summary>
Motivation: 在复杂的现实场景中，精确了解变量间的因果关系非常重要，但传统的统计方法可能受限于数据不足或结构复杂性。因此，提出了一种结合语义推理和统计验证的DMCD框架，以提高因果结构学习的效率和准确性。

Method: DMCD分为两阶段：第一阶段使用大型语言模型生成稀疏的草案DAG，并考虑变量元数据的语义信息；第二阶段通过条件独立性测试审核和优化这个草案，纠正模型错误。

Result: DMCD在工业工程、环境监测和IT系统分析的三个实际数据集上进行了评估，表现出与多种基准方法相当或优秀的表现，特别是在召回率和F1分数方面。

Conclusion: DMCD展示了将语义先验与原理验证相结合在因果结构学习方面的优势，提供了一种高效和实用的因果推理方法。

Abstract: We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed prior over the space of possible causal structures. In Phase II, this draft is audited and refined via conditional independence testing, with detected discrepancies guiding targeted edge revisions.
  We evaluate our approach on three metadata-rich real-world benchmarks spanning industrial engineering, environmental monitoring, and IT systems analysis. Across these datasets, DMCD achieves competitive or leading performance against diverse causal discovery baselines, with particularly large gains in recall and F1 score. Probing and ablation experiments suggest that these improvements arise from semantic reasoning over metadata rather than memorization of benchmark graphs. Overall, our results demonstrate that combining semantic priors with principled statistical verification yields a high-performing and practically effective approach to causal structure learning.

</details>


### [8] [Diffusion Modulation via Environment Mechanism Modeling for Planning](https://arxiv.org/abs/2602.20422)
*Hanping Zhang,Yuhong Guo*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的基于扩散模型的规划方法，称为环境机制建模下的扩散调制（DMEMM），它通过融入RL环境中的关键机制优化了扩散模型的训练，实验表明这种方法在离线强化学习中的规划性能达到了最先进的水平。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的方法在规划中未能充分考虑在RL环境中生成轨迹时所需的转换一致性，这会导致生成的轨迹与实际环境的机制不符。

Method: DMEMM 通过引入关键的 RL 环境机制，尤其是转换动态和奖励函数来优化扩散模型的训练。

Result: 实验结果表明，DMEMM 在离线强化学习的规划中达到了最先进的性能。

Conclusion: 文章提出了一种新颖的方法来提高基于扩散模型的规划方法在离线强化学习中的性能。

Abstract: Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.

</details>


### [9] [PreScience: A Benchmark for Forecasting Scientific Contributions](https://arxiv.org/abs/2602.20459)
*Anirudh Ajith,Amanpreet Singh,Jay DeYoung,Nadav Kunievsky,Austin C. Kozlowski,Oyvind Tafjord,James Evans,Daniel S. Weld,Tom Hope,Doug Downey*

Main category: cs.AI

TL;DR: 本研究提出了一种名为PreScience的科学预测基准，将研究过程分解为四部分独立生成任务，并构建了一个包含98K个AI相关研究论文的数据集。虽然各任务已有一定的进展，但在贡献生成方面仍有改进空间。将各任务组合起来进行12个月的科学产出模拟后，生成的合成数据集在多样性与创新性上均低于同期人类撰写的研究作品。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索AI系统是否能够预测未来的科学进展，该能力有助于研究人员识别合作伙伴和具有影响力的领域，提前预判哪些问题和方法将成为焦点。

Method: 构建了PreScience基准，定义了四个相互关联的生成任务：合作者预测、先前工作选择、贡献生成和影响预测。开发了包括LACERScore在内的一系列基线和评估方法。

Result: 尽管各任务有所进展，但仍存在改进空间。在贡献生成方面，先进的语言模型仅能达到中度相似度。将多个任务组合成12个月的科学生产模拟时，生成的数据集在多样性和创新性方面不如同期的人类撰写的研究。

Conclusion: 研究结果表明，虽然现有的AI系统在预测科学进展方面取得了一定进展，但仍有显著提升空间，特别是在贡献生成方面。

Abstract: Can AI systems trained on the scientific record up to a fixed point in time forecast the scientific advances that follow? Such a capability could help researchers identify collaborators and impactful research directions, and anticipate which problems and methods will become central next. We introduce PreScience -- a scientific forecasting benchmark that decomposes the research process into four interdependent generative tasks: collaborator prediction, prior work selection, contribution generation, and impact prediction. PreScience is a carefully curated dataset of 98K recent AI-related research papers, featuring disambiguated author identities, temporally aligned scholarly metadata, and a structured graph of companion author publication histories and citations spanning 502K total papers. We develop baselines and evaluations for each task, including LACERScore, a novel LLM-based measure of contribution similarity that outperforms previous metrics and approximates inter-annotator agreement. We find substantial headroom remains in each task -- e.g. in contribution generation, frontier LLMs achieve only moderate similarity to the ground-truth (GPT-5, averages 5.6 on a 1-10 scale). When composed into a 12-month end-to-end simulation of scientific production, the resulting synthetic corpus is systematically less diverse and less novel than human-authored research from the same period.

</details>


### [10] [KairosVL: Orchestrating Time Series and Semantics for Unified Reasoning](https://arxiv.org/abs/2602.20494)
*Haotian Si,Changhua Pei,Xiao He,Zeyan Li,Zhe Xie,Zexin Wang,Jiyao Hu,Zhaoyang Yu,Tieying Zhang,Dan Pei,Jianhui Li,Gaogang Xie*

Main category: cs.AI

TL;DR: 本文介绍了 Semantic-Conditional 时间序列推理任务，结合语义理解提升时间序列分析的复杂问题处理能力，并提出了一种两轮强化学习框架，最终模型 KairosVL 在多种任务中表现出色，验证了框架的有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 本文旨在应对时间序列分析中日益复杂的决策需求，通过结合语义理解来超越传统的数值建模，从而提升模型在复杂时间序列问题上的推理能力。

Method: 提出了一种两轮强化学习框架，首先增强模型对基本时间特性感知，随后进行基于语义的推理。最终构建了名为 KairosVL 的模型。

Result: KairosVL 在合成和真实世界任务中表现出竞争性性能，并且验证了框架不仅提升了性能，还保留了内在的推理能力和显著改善了对未见过场景的泛化。

Conclusion: 本研究展示了语义推理与时间建模结合的潜力，并提供了一个实际框架，用于解决真实世界中的时间序列智能问题。

Abstract: Driven by the increasingly complex and decision-oriented demands of time series analysis, we introduce the Semantic-Conditional Time Series Reasoning task, which extends conventional time series analysis beyond purely numerical modeling to incorporate contextual and semantic understanding. To further enhance the mode's reasoning capabilities on complex time series problems, we propose a two-round reinforcement learning framework: the first round strengthens the mode's perception of fundamental temporal primitives, while the second focuses on semantic-conditioned reasoning. The resulting model, KairosVL, achieves competitive performance across both synthetic and real-world tasks. Extensive experiments and ablation studies demonstrate that our framework not only boosts performance but also preserves intrinsic reasoning ability and significantly improves generalization to unseen scenarios. To summarize, our work highlights the potential of combining semantic reasoning with temporal modeling and provides a practical framework for real-world time series intelligence, which is in urgent demand.

</details>


### [11] [ActionEngine: From Reactive to Programmatic GUI Agents via State Machine Memory](https://arxiv.org/abs/2602.20502)
*Hongbin Zhong,Fazle Faisal,Luis França,Tanakorn Leesatapornwongsa,Adriana Szekeres,Kexin Rong,Suman Nath*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Existing Graphical User Interface (GUI) agents operate through step-by-step calls to vision language models--taking a screenshot, reasoning about the next action, executing it, then repeating on the new page--resulting in high costs and latency that scale with the number of reasoning steps, and limited accuracy due to no persistent memory of previously visited pages.
  We propose ActionEngine, a training-free framework that transitions from reactive execution to programmatic planning through a novel two-agent architecture: a Crawling Agent that constructs an updatable state-machine memory of the GUIs through offline exploration, and an Execution Agent that leverages this memory to synthesize complete, executable Python programs for online task execution.
  To ensure robustness against evolving interfaces, execution failures trigger a vision-based re-grounding fallback that repairs the failed action and updates the memory.
  This design drastically improves both efficiency and accuracy: on Reddit tasks from the WebArena benchmark, our agent achieves 95% task success with on average a single LLM call, compared to 66% for the strongest vision-only baseline, while reducing cost by 11.8x and end-to-end latency by 2x.
  Together, these components yield scalable and reliable GUI interaction by combining global programmatic planning, crawler-validated action templates, and node-level execution with localized validation and repair.

</details>


### [12] [Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination](https://arxiv.org/abs/2602.20517)
*Rakshit Trivedi,Kartik Sharma,David C Parkes*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.

</details>


### [13] [From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production](https://arxiv.org/abs/2602.20558)
*Yucheng Shi,Ying Li,Yu Wang,Yesu Feng,Arjun Rao,Rein Houthooft,Shradha Sehgal,Jin Wang,Hao Zhen,Ninghao Liu,Linas Baltrunas*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate fields, yielding suboptimal representations for recommendation. We propose a data-centric framework that learns verbalization for LLM-based recommendation. Using reinforcement learning, a verbalization agent transforms raw interaction histories into optimized textual contexts, with recommendation accuracy as the training signal. This agent learns to filter noise, incorporate relevant metadata, and reorganize information to improve downstream predictions. Experiments on a large-scale industrial streaming dataset show that learned verbalization delivers up to 93% relative improvement in discovery item recommendation accuracy over template-based baselines. Further analysis reveals emergent strategies such as user interest summarization, noise removal, and syntax normalization, offering insights into effective context construction for LLM-based recommender systems.

</details>


### [14] [CausalReasoningBenchmark: A Real-World Benchmark for Disentangled Evaluation of Causal Identification and Estimation](https://arxiv.org/abs/2602.20571)
*Ayush Sawarni,Jiyuan Tan,Vasilis Syrgkanis*

Main category: cs.AI

TL;DR: 该基准测试旨在评估系统在因果推理中的识别能力和估算能力，通过区分两者的分数来提供精确的诊断，发现一个先进的LLM在识别策略上表现较好，但在详细的研究设计上表现较差。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要关注单个数值输出，如平均治疗效应ATE，并未区分因果分析中的识别和估计两步。因此引入了CausalReasoningBenchmark，以促进更 robust的自动化因果推理系统的开发。

Method: CausalReasoningBenchmark 包含173个问题，涵盖138个实际数据集，来源于85篇同行评审的研究论文和4本广泛使用的因果推理教科书。每个问题要求系统提供结构化的识别规范和一个带有标准误的点估计。

Result: 基线结果表明，一个最先进的LLM在84%的情况下正确识别了高层次策略，但在完整的识别规范正确性方面仅为30%，表明瓶颈在于研究设计的细微差别，而不是计算。

Conclusion: CausalReasoningBenchmark 公开发布在Hugging Face上，旨在促进因果推理系统的发展，尤其关注识别环节的准确性和精细度。

Abstract: Many benchmarks for automated causal inference evaluate a system's performance based on a single numerical output, such as an Average Treatment Effect (ATE). This approach conflates two distinct steps in causal analysis: identification-formulating a valid research design under stated assumptions-and estimation-implementing that design numerically on finite data. We introduce CausalReasoningBenchmark, a benchmark of 173 queries across 138 real-world datasets, curated from 85 peer-reviewed research papers and four widely-used causal-inference textbooks. For each query a system must produce (i) a structured identification specification that names the strategy, the treatment, outcome, and control variables, and all design-specific elements, and (ii) a point estimate with a standard error. By scoring these two components separately, our benchmark enables granular diagnosis: it distinguishes failures in causal reasoning from errors in numerical execution. Baseline results with a state-of-the-art LLM show that, while the model correctly identifies the high-level strategy in 84 % of cases, full identification-specification correctness drops to only 30 %, revealing that the bottleneck lies in the nuanced details of research design rather than in computation. CausalReasoningBenchmark is publicly available on Hugging Face and is designed to foster the development of more robust automated causal-inference systems.

</details>


### [15] [When can we trust untrusted monitoring? A safety case sketch across collusion strategies](https://arxiv.org/abs/2602.20628)
*Nelson Gardner-Challis,Jonathan Bostock,Georgiy Kozhevnikov,Morgan Sinclaire,Joan Velja,Alessandro Abate,Charlie Griffin*

Main category: cs.AI

TL;DR: 本文研究了利用非可信监控来降低不安全人工智能系统风险的方法，开发了针对不安全模型可能使用策略的分类，并提出了一个安全论证概要。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能系统越来越自主和先进，部署一个不安全的人工智能系统带来的潜在灾难性风险也在增加。为了通过非可信监控减少这种风险，本文在现有方法的基础上进行改进，为不安全模型可能采取的合谋策略提供了更全面的分类，并提出了一个明确的安全论证概要。

Method: 本文开发了一种新的方法论，用于对非可信监控的安全性进行严格证明，以及提出了对不安全模型可能使用的合谋策略的分类标准。同时也提供了一个清晰的安全论证框架。

Result: 研究结果表明，被动自我识别可能比以前研究的合谋策略更有效。文章为未来非可信监控的研究提供了更坚实的基础。

Conclusion: 本文通过改进现有方法，并提供了一种全面的方法论，为非可信监控的安全性提供了更强的评估依据，并为未来的研究指明了方向。

Abstract: AIs are increasingly being deployed with greater autonomy and capabilities, which increases the risk that a misaligned AI may be able to cause catastrophic harm. Untrusted monitoring -- using one untrusted model to oversee another -- is one approach to reducing risk. Justifying the safety of an untrusted monitoring deployment is challenging because developers cannot safely deploy a misaligned model to test their protocol directly. In this paper, we develop upon existing methods for rigorously demonstrating safety based on pre-deployment testing. We relax assumptions that previous AI control research made about the collusion strategies a misaligned AI might use to subvert untrusted monitoring. We develop a taxonomy covering passive self-recognition, causal collusion (hiding pre-shared signals), acausal collusion (hiding signals via Schelling points), and combined strategies. We create a safety case sketch to clearly present our argument, explicitly state our assumptions, and highlight unsolved challenges. We identify conditions under which passive self-recognition could be a more effective collusion strategy than those studied previously. Our work builds towards more robust evaluations of untrusted monitoring.

</details>


### [16] [Identifying two piecewise linear additive value functions from anonymous preference information](https://arxiv.org/abs/2602.20638)
*Vincent Auriau,Khaled Belahcene,Emmanuel Malherbe,Vincent Mousseau,Marc Pirlot*

Main category: cs.AI

TL;DR: 本文提出了一种同时获取两位决策者偏好模型的方法，当边际价值函数是分段线性的，并且断点已知时，可以准确识别两位决策者的偏好模型。


<details>
  <summary>Details</summary>
Motivation: 为了改进偏好信息的收集效率，本文尝试同时获取两位决策者的偏好信息，旨在通过减少查询次数来提高决策过程的效率。

Method: 本文提出了一种新的偏好模型提取方法，该方法在每次查询时获取两位决策者的偏好信息，并利用边际价值函数的分段线性特性及已知断点来区分两位决策者的偏好。

Result: 在边际价值函数为分段线性的情况下，本文提出的方法可以准确地识别出两位决策者的偏好模型。

Conclusion: 本文提出的方法提高了同时获取多位决策者偏好信息的效率，为多决策者的偏好模型提取提供了新的解决方案。

Abstract: Eliciting a preference model involves asking a person, named decision-maker, a series of questions. We assume that these preferences can be represented by an additive value function. In this work, we query simultaneously two decision-makers in the aim to elicit their respective value functions. For each query we receive two answers, without noise, but without knowing which answer corresponds to which decision-maker.We propose an elicitation procedure that identifies the two preference models when the marginal value functions are piecewise linear with known breaking points.

</details>


### [17] [Recursive Belief Vision Language Model](https://arxiv.org/abs/2602.20659)
*Vaidehi Bagaria,Bijo Sebastian,Nirav Patel*

Main category: cs.AI

TL;DR: 提出了RB-VLA模型，该模型通过基于信念的架构以及自监督的世界模型目标进行训练，能够在长时 horizon 的操作中维持紧凑的潜状态表示，而无需存储原始观察或随时间扩展内存。这使得RB-VLA在多阶段pick-and-place和堆积任务中表现出色，与现有VLA模型相比，成功率分别提高了52.5%和37.5%，同时显著减少推理延迟。


<details>
  <summary>Details</summary>
Motivation: 当前的VLA模型在长时 horizon 操作下难以应对部分可观性问题，缺乏持久的、基于动作的状态表示，无法进行长期的因果推理，导致任务执行中断、动作重复和高推理延迟等问题。

Method: RB-VLA模型采用基于信念的体系结构，并通过自监督的世界模型目标进行训练，维持任务相关的历史、动力学和物体交互的紧凑潜状态表示。模型在高水平意图查询后，视觉语言模型提供任务说明，信念跟踪任务进度，并在部分可观性下进行阶段感知、因果上一致的控制，而不存储原始观察数据或随时间扩展内存。信念和意图共同调节扩散策略以实现 robust 的闭环执行。

Result: RB-VLA模型在长时 horizon 基准测试上表现出色。在多阶段pick-and-place任务中，成功率提高了52.5%；在堆积任务中，成功率提高了37.5%，相较于π0的表现。此外，与基线相比，推理延迟减少了5倍，并在所有时间步长中观察到内存增长的消除。

Conclusion: 基于信念的状态表示对于长时 horizon VLA策略的有效性表现出优势，能够同时解决部分可观性和长期规划问题，提升模型性能并降低推理延迟。

Abstract: Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to π0. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.

</details>


### [18] [How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective](https://arxiv.org/abs/2602.20687)
*Bo Peng,Pi Bu,Keyu Pan,Xinrun Xu,Yinxiu Zhao,Miao Chen,Yang Du,Lin Li,Jun Song,Tong Xu*

Main category: cs.AI

TL;DR: NativeEmbodied 是一个全新的基准测试，旨在评估基于视觉语言模型的机器人在低层级动作空间中的性能，通过在多样性模拟场景中评估复杂任务和基础技能，揭示了现有模型在多个基础技能上的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉-语言模型的机器人基准测试往往依赖于高层级命令或离散的动作空间，这些都与真实世界的控制大相径庭。NativeEmbodied 标准旨在弥补这些不足，通过统一的低层动作空间和复杂场景下的高层级任务更好地评估基于视觉语言模型的机器人的整体性能。

Method: NativeEmbodied 基准测试基于多种模拟场景，并包含三个代表性的高等级任务和四种针对基础技能的较低等级任务，以便进行跨任务和技能粒度的综合评估。

Result: 实验结果显示，当前最先进的视觉语言模型在多个基础技能上存在明显缺陷，并且这些限制显著影响了在高等级任务上的表现。

Conclusion: NativeEmbodied 突显了现有视觉语言模型驱动的机器人的关键挑战，并为未来的研究提供了指导性见解。

Abstract: Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.

</details>


### [19] [PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding](https://arxiv.org/abs/2602.20696)
*Baolong Bi,Yuyao Ge,Shenghua Liu,Yuchen He,Siqian Tong,Lizhe Chen,Lingrui Mei,Zehao Li,Yiwei Wang,Yujun Cai,Ming-Hsuan Yang,Xueqi Cheng*

Main category: cs.AI

TL;DR: 介绍了Polarity-Prompt Contrastive Decoding (PromptCD) 方法，这是一种在测试时控制模型行为的方法，它通过对齐正反向提示来增强模型的响应，适用于多模态场景，提升了LSTM和VLMs的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐方法大多在训练期间进行操作，需要额外的数据和较高的成本。而PromptCD则在测试阶段通过对比解码改进了特定能力，并将这种对比解码扩展到更广泛的应用场景。

Method: PromptCD通过创建目标行为的正向和反向指导提示，对比模型响应的具体概率分布和视觉注意力模式，以强化期望的结果。

Result: 对于LSTM，实验表明“3H”对齐目标（有用性、诚实和无害）上取得了显著改进，表明经过训练的模型可以在测试阶段实现有意义的自我增强。对于VLMs，研究表明PromptCD显著提高了基于视觉的理解性能。

Conclusion: PromptCD是一个简单且通用的战略，能够在保持高效率的同时在多模态中控制模型行为，从而提高系统的可靠性和行为一致性。

Abstract: Reliable AI systems require large language models (LLMs) to exhibit behaviors aligned with human preferences and values. However, most existing alignment approaches operate at training time and rely on additional high-quality data, incurring significant computational and annotation costs. While recent work has shown that contrastive decoding can leverage a model's internal distributions to improve specific capabilities, its applicability remains limited to narrow behavioral scopes and scenarios. In this work, we introduce Polarity-Prompt Contrastive Decoding (PromptCD), a test-time behavior control method that generalizes contrastive decoding to broader enhancement settings. PromptCD constructs paired positive and negative guiding prompts for a target behavior and contrasts model responses-specifically token-level probability distributions in LLMs and visual attention patterns in VLMs-to reinforce desirable outcomes. This formulation extends contrastive decoding to a wide range of enhancement objectives and is applicable to both LLMs and Vision-Language Models (VLMs) without additional training. For LLMs, experiments on the "3H" alignment objectives (helpfulness, honesty, and harmlessness) demonstrate consistent and substantial improvements, indicating that post-trained models can achieve meaningful self-enhancement purely at test time. For VLMs, we further analyze contrastive effects on visual attention, showing that PromptCD significantly improves VQA performance by reinforcing behavior-consistent visual grounding. Collectively, these results highlight PromptCD as a simple, general, and cost-efficient strategy for reliable behavior control across modalities.

</details>


### [20] [Online Algorithms with Unreliable Guidance](https://arxiv.org/abs/2602.20706)
*Julien Dallot,Yuval Emek,Yuval Gil,Maciej Pacut,Stefan Schmid*

Main category: cs.AI

TL;DR: 本文提出了一种新的ML增强在线决策模型OAG。OAG算法以请求-响应游戏为视角，每次接收一个可能被恶意篡改的概率为β的指导。DTB编译器可以将任何在线算法转换为在OAG模型中的学习增强算法。本文证明了通过DTB编译器生成的算法对于经典在线问题中的缓存和均匀度量任务系统是最佳的，并且对于具有对抗到达顺序的二分匹配问题，所提出算法优于现有最佳方案。


<details>
  <summary>Details</summary>
Motivation: 在当前的机器学习和在线算法研究中，如何同时保证系统在没有外部指导时的最优表现和在外部指导可能出错时的鲁棒性是一个关键研究问题。本文旨在提出一种模型OAG和相应的编译器DTB来解决这些问题。

Method: 通过将传统的无预测敏感的在线算法与一种新的编译方法相结合，提出了一种称为OAG的模型。该模型允许根据来自问题答案空间的指导进行决策，且该指导可能被恶意篡改。另一种称为DTB编译器的方法将任何无预测敏感的在线算法转换为一种学习增强算法。

Result: DTB编译器产生的算法在缓存和均匀度量任务系统中实现了最优表现，对于二分匹配问题，该算法也优于当前最佳方案。此外，证明了算法具备吸引力的稳健性保证。

Conclusion: 该论文提出了一个新的在线算法模型OAG及其编译方法DTB，并展示了它们在多种经典在线问题中的优越性，证明了即使在面临外部指导出错的情况下，OAG模型及其对应编译器也能提供有吸引力的性能保证。

Abstract: This paper introduces a new model for ML-augmented online decision making, called online algorithms with unreliable guidance (OAG). This model completely separates between the predictive and algorithmic components, thus offering a single well-defined analysis framework that relies solely on the considered problem. Formulated through the lens of request-answer games, an OAG algorithm receives, with each incoming request, a piece of guidance which is taken from the problem's answer space; ideally, this guidance is the optimal answer for the current request, however with probability $β$, the guidance is adversarially corrupted. The goal is to develop OAG algorithms that admit good competitiveness when $β= 0$ (a.k.a. consistency) as well as when $β= 1$ (a.k.a. robustness); the appealing notion of smoothness, that in most prior work required a dedicated loss function, now arises naturally as $β$ shifts from $0$ to $1$.
  We then describe a systematic method, called the drop or trust blindly (DTB) compiler, which transforms any online algorithm into a learning-augmented online algorithm in the OAG model. Given a prediction-oblivious online algorithm, its learning-augmented counterpart produced by applying the DTB compiler either follows the incoming guidance blindly or ignores it altogether and proceeds as the initial algorithm would have; the choice between these two alternatives is based on the outcome of a (biased) coin toss. As our main technical contribution, we prove (rigorously) that although remarkably simple, the class of algorithms produced via the DTB compiler includes algorithms with attractive consistency-robustness guarantees for three classic online problems: for caching and uniform metrical task systems our algorithms are optimal, whereas for bipartite matching (with adversarial arrival order), our algorithm outperforms the state-of-the-art.

</details>


### [21] [Counterfactual Simulation Training for Chain-of-Thought Faithfulness](https://arxiv.org/abs/2602.20710)
*Peter Hase,Christopher Potts*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Inspecting Chain-of-Thought reasoning is among the most common means of understanding why an LLM produced its output. But well-known problems with CoT faithfulness severely limit what insights can be gained from this practice. In this paper, we introduce a training method called Counterfactual Simulation Training (CST), which aims to improve CoT faithfulness by rewarding CoTs that enable a simulator to accurately predict a model's outputs over counterfactual inputs. We apply CST in two settings: (1) CoT monitoring with cue-based counterfactuals, to detect when models rely on spurious features, reward hack, or are sycophantic, and (2) counterfactual simulation over generic model-based counterfactuals, to encourage models to produce more faithful, generalizable reasoning in the CoT. Experiments with models up to 235B parameters show that CST can substantially improve monitor accuracy on cue-based counterfactuals (by 35 accuracy points) as well as simulatability over generic counterfactuals (by 2 points). We further show that: (1) CST outperforms prompting baselines, (2) rewriting unfaithful CoTs with an LLM is 5x more efficient than RL alone, (3) faithfulness improvements do not generalize to dissuading cues (as opposed to persuading cues), and (4) larger models do not show more faithful CoT out of the box, but they do benefit more from CST. These results suggest that CST can improve CoT faithfulness in general, with promising applications for CoT monitoring. Code for experiments in this paper is available at https://github.com/peterbhase/counterfactual-simulation-training

</details>


### [22] [Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning](https://arxiv.org/abs/2602.20722)
*Xu Wan,Yansheng Wang,Wenqi Huang,Mingyang Sun*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Traditional on-policy Reinforcement Learning with Verifiable Rewards (RLVR) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7% of problems that base models consistently fail to solve.

</details>


### [23] [Balancing Multiple Objectives in Urban Traffic Control with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2602.20728)
*Chenyang Zhao,Vinny Cahill,Ivana Dusparic*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reward design has been one of the central challenges for real world reinforcement learning (RL) deployment, especially in settings with multiple objectives. Preference-based RL offers an appealing alternative by learning from human preferences over pairs of behavioural outcomes. More recently, RL from AI feedback (RLAIF) has demonstrated that large language models (LLMs) can generate preference labels at scale, mitigating the reliance on human annotators. However, existing RLAIF work typically focuses only on single-objective tasks, leaving the open question of how RLAIF handles systems that involve multiple objectives. In such systems trade-offs among conflicting objectives are difficult to specify, and policies risk collapsing into optimizing for a dominant goal. In this paper, we explore the extension of the RLAIF paradigm to multi-objective self-adaptive systems. We show that multi-objective RLAIF can produce policies that yield balanced trade-offs reflecting different user priorities without laborious reward engineering. We argue that integrating RLAIF into multi-objective RL offers a scalable path toward user-aligned policy learning in domains with inherently conflicting objectives.

</details>


### [24] [CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference](https://arxiv.org/abs/2602.20732)
*Chao Fei,Guozhong Li,Chenxi Liu,Panos Kalnis*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local semantics, which undermines quality. Moreover, their irregular accesses and selection overheads yield only limited wall-clock speedups. To address this, we propose \textbf{CHESS}, an \textit{algorithm-system co-design} KV-cache management system. Algorithmically, CHESS introduces a context-aware, hierarchical selection policy that dynamically reconstructs a coherent context for the current decoding. System-wise, coarse granularity selection eliminates expensive data movement, fully realizing practical acceleration from theoretical sparsity. Extensive evaluations demonstrate that CHESS surpasses Full-KV quality using only \textbf{1\%} of the KV cache, delivers low-latency stable inference with up to \textbf{4.56$\times$} higher throughput, and consistently outperforms other strong baselines. Code is available at \href{https://anonymous.4open.science/r/CHESS-9958/}{https://anonymous.4open.science/r/CHESS/}.

</details>


### [25] [POMDPPlanners: Open-Source Package for POMDP Planning](https://arxiv.org/abs/2602.20810)
*Yaacov Pariente,Vadim Indelman*

Main category: cs.AI

TL;DR: POMDPPlanners 是一个开源 Python 包，旨在提供 POMDP 规划算法的评估工具，包含先进算法、基准环境、自动超参数优化等功能，旨在促进不确定性决策下的可扩展和可重复研究。


<details>
  <summary>Details</summary>
Motivation: 为了解决标准工具包在风险敏感设置中的不足，该包旨在促进对不确定性环境下的决策制定进行可扩展且可重复的研究。

Method: POMDPPlanners 包含了先进的 POMDP 规划算法，基于 Optuna 的自动超参数优化，持久缓存故障恢复功能以及可配置的并行仿真。

Result: 该包集成了一系列功能，为 POMDP 规划算法的研究提供了全面的支持，特别是在安全关键领域，显著降低了大规模仿真研究的开销。

Conclusion: POMDPPlanners 作为一个开源工具，为 POMDP 规划的评估和研究提供了方便的平台，推动了不确定性环境下决策制定的研究进展。

Abstract: We present POMDPPlanners, an open-source Python package for empirical evaluation of Partially Observable Markov Decision Process (POMDP) planning algorithms. The package integrates state-of-the-art planning algorithms, a suite of benchmark environments with safety-critical variants, automated hyperparameter optimization via Optuna, persistent caching with failure recovery, and configurable parallel simulation -- reducing the overhead of extensive simulation studies. POMDPPlanners is designed to enable scalable, reproducible research on decision-making under uncertainty, with particular emphasis on risk-sensitive settings where standard toolkits fall short.

</details>


### [26] [Qwen-BIM: developing large language model for BIM-based design with domain-specific benchmark and dataset](https://arxiv.org/abs/2602.20812)
*Jia-Rui Lin,Yun-Hong Cai,Xiang-Rui Ni,Shaojie Zhou,Peng Pan*

Main category: cs.AI

TL;DR: 该研究提出了一种针对BIM（建筑信息模型）设计的特定领域基准和高质量数据集，以评估大型语言模型在BIM设计中的性能，并对其进行微调，结果显示该特定领域模型在任务上比通用模型更高效。


<details>
  <summary>Details</summary>
Motivation: 随着建筑行业向数字化转型的推进，BIM设计成为智能建筑的关键驱动力。然而，由于缺乏专用数据集和LLM评估基准，大型语言模型在这方面的能力受到了显著限制。

Method: 本文提出了一种评估BIM设计的基准，包括相应的定量指标来评估LLM的性能，并提出了一种生成文本数据的方法从BIM构建对应的BIM衍生数据集用于LLM评估和微调，还提出了一种适配LLM进行BIM设计的策略。

Result: 研究结果表明，提出的专用领域基准可以有效地、全面地评估LLM的能力，指出一般LLM在领域特定任务中仍有不足。使用所提出的基准和数据集，Qwen-BIM在G-Eval得分上平均提高了21.0%，并且在只有14B参数下，其性能可与具有671B参数的一般LLM相媲美。

Conclusion: 这项研究通过引入综合基准和高质量数据集，开发了首个针对BIM设计的领域特定大语言模型，为各种领域发展BIM相关的大语言模型奠定了坚实基础。

Abstract: As the construction industry advances toward digital transformation, BIM (Building Information Modeling)-based design has become a key driver supporting intelligent construction. Despite Large Language Models (LLMs) have shown potential in promoting BIM-based design, the lack of specific datasets and LLM evaluation benchmarks has significantly hindered the performance of LLMs. Therefore, this paper addresses this gap by proposing: 1) an evaluation benchmark for BIM-based design together with corresponding quantitative indicators to evaluate the performance of LLMs, 2) a method for generating textual data from BIM and constructing corresponding BIM-derived datasets for LLM evaluation and fine-tuning, and 3) a fine-tuning strategy to adapt LLMs for BIM-based design. Results demonstrate that the proposed domain-specific benchmark effectively and comprehensively assesses LLM capabilities, highlighting that general LLMs are still incompetent for domain-specific tasks. Meanwhile, with the proposed benchmark and datasets, Qwen-BIM is developed and achieves a 21.0% average increase in G-Eval score compared to the base LLM model. Notably, with only 14B parameters, performance of Qwen-BIM is comparable to that of general LLMs with 671B parameters for BIM-based design tasks. Overall, this study develops the first domain-specific LLM for BIM-based design by introducing a comprehensive benchmark and high-quality dataset, which provide a solid foundation for developing BIM-related LLMs in various fields.

</details>


### [27] [Pressure Reveals Character: Behavioural Alignment Evaluation at Depth](https://arxiv.org/abs/2602.20813)
*Nora Petrova,John Burden*

Main category: cs.AI

TL;DR: 该研究引入了一种新的对齐基准，包括904个场景，涵盖六大类别：诚实性、安全性、非操控性、鲁棒性、可纠正性及诡计。该基准通过验证现实性和多轮升级的场景来评估模型的行为倾向，发现即使表现最好的模型在特定类别中仍然存在缺陷，而大多数模型在所有类别中表现出一致的弱点。


<details>
  <summary>Details</summary>
Motivation: 鉴于对齐失败所导致的现实伤害日益增加，现有的评估框架未能涵盖现实多轮情景，因此需要一种能够测试模型在实际压力下的行为基准。

Method: 该研究通过引入涵盖了多个类别的现实性情景，设计了一个多轮升级的评估框架，并使用了一种因子分析方法来检测对齐行为在不同类别之间的关系。

Result: 研究发现，虽然表现最好的模型在某些类别中表现良好，但大多数模型在所有类别中都表现出一致的弱点。对齐行为被视为一个综合性的因素，具有正相关性。

Conclusion: 该研究开发了一种新的多类别、现实性和多轮情景对齐评估基准，并计划在持续评估的同时扩展新的场景并引入新模型。

Abstract: Evaluating alignment in language models requires testing how they behave under realistic pressure, not just what they claim they would do. While alignment failures increasingly cause real-world harm, comprehensive evaluation frameworks with realistic multi-turn scenarios remain lacking. We introduce an alignment benchmark spanning 904 scenarios across six categories -- Honesty, Safety, Non-Manipulation, Robustness, Corrigibility, and Scheming -- validated as realistic by human raters. Our scenarios place models under conflicting instructions, simulated tool access, and multi-turn escalation to reveal behavioural tendencies that single-turn evaluations miss. Evaluating 24 frontier models using LLM judges validated against human annotations, we find that even top-performing models exhibit gaps in specific categories, while the majority of models show consistent weaknesses across the board. Factor analysis reveals that alignment behaves as a unified construct (analogous to the g-factor in cognitive research) with models scoring high on one category tending to score high on others. We publicly release the benchmark and an interactive leaderboard to support ongoing evaluation, with plans to expand scenarios in areas where we observe persistent weaknesses and to add new models as they are released.

</details>


### [28] [Diagnosing Causal Reasoning in Vision-Language Models via Structured Relevance Graphs](https://arxiv.org/abs/2602.20878)
*Dhita Putri Pratama,Soyeon Caren Han,Yihao Ding*

Main category: cs.AI

TL;DR: 本文提出了一个名为Vision-Language Causal Graphs (VLCGs) 的结构化表示方法，用于指导视觉语言模型进行因果推理。基于此，提出了一个新的诊断基准ViLCaR，用于因果归因、因果推理和问答任务。实验表明，注入结构化的相关性信息比零样本和标准的上下文学习方法能显著提高归因和推理一致性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在视觉问答任务上取得了很好的表现，但往往依赖于无效的相关性，而非基于因果的推理。现有评估主要关注答案的正确性，导致难以判断错误是由于推理能力不足还是未能正确辨识因果相关信息。因此，需要一个既能评估因果关联信息识别又能考察因果推理能力的诊断基准。

Method: 提出了一个名为Vision-Language Causal Graphs (VLCGs) 的结构化表示，能够明确编码因果相关对象、属性、关系和基于场景的假设。基于该表示方法，构建了一个新的诊断基准ViLCaR，包含因果归因、因果推理和问答任务。引入了与图对齐的评估指标，不仅考虑最终答案的准确性，还考虑因果相关信息的识别。

Result: 实验表明，通过注入结构化的相关性信息，可以在因果归因和推理一致性方面显著优于零样本和标准上下文学习方法。

Conclusion: 当前视觉语言模型在因果推理上的限制主要源自结构指导不足，而非推理能力的不足。进一步研究如何提高结构化的因果相关信息指导机制，有助于提升这些模型的因果推理能力。

Abstract: Large Vision-Language Models (LVLMs) achieve strong performance on visual question answering benchmarks, yet often rely on spurious correlations rather than genuine causal reasoning. Existing evaluations primarily assess the correctness of the answers, making it unclear whether failures arise from limited reasoning capability or from misidentifying causally relevant information. We introduce Vision-Language Causal Graphs (VLCGs), a structured, query-conditioned representation that explicitly encodes causally relevant objects, attributes, relations, and scene-grounded assumptions. Building on this representation, we present ViLCaR, a diagnostic benchmark comprising tasks for Causal Attribution, Causal Inference, and Question Answering, along with graph-aligned evaluation metrics that assess relevance identification beyond final answer accuracy. Experiments in state-of-the-art LVLMs show that injecting structured relevance information significantly improves attribution and inference consistency compared to zero-shot and standard in-context learning. These findings suggest that current limitations in LVLM causal reasoning stem primarily from insufficient structural guidance rather than a lack of reasoning capacity.

</details>


### [29] [Predicting Sentence Acceptability Judgments in Multimodal Contexts](https://arxiv.org/abs/2602.20918)
*Hyewon Jang,Nikolai Ilinykh,Sharid Loáiciga,Jey Han Lau,Shalom Lappin*

Main category: cs.AI

TL;DR: 研究探讨了视觉背景对人类和大规模语言模型（LLMs）处理句子可接受性判断的影响。发现人类判断受视觉背景影响较小，而LLMs则表现出与之前研究中发现的文档背景下的人类判断类似的压缩效应。不同类型的LLMs在去除视觉背景时能更准确地预测人类判断，但内部表示与生成预测之间的差距在存在视觉背景时会增大。


<details>
  <summary>Details</summary>
Motivation: 探讨人类和LLMs在处理带有视觉背景的句子可接受性判断时的差异和相似之处。

Method: 通过实验比较了人类和不同类型的LLMs在有无视觉背景下的句子可接受性判断，分析了LLMs内部表示与生成预测之间的关系。

Result: 发现视觉背景对人类可接受性判断影响较小，LLMs则表现出类似人类的判断压缩效应；不同LLMs在去除视觉背景时表现更佳，但内部表示差距在视觉背景下增大；LLMs对句子判断的预测与其生成概率高度相关，但在视觉背景下相关性下降。

Conclusion: 研究揭示了人类和LLMs在处理带视觉背景的句子时的一些重要区别和类似之处，强调了在多模态环境中理解LLMs的作用机制的重要性。

Abstract: Previous work has examined the capacity of deep neural networks (DNNs), particularly transformers, to predict human sentence acceptability judgments, both independently of context, and in document contexts. We consider the effect of prior exposure to visual images (i.e., visual context) on these judgments for humans and large language models (LLMs). Our results suggest that, in contrast to textual context, visual images appear to have little if any impact on human acceptability ratings. However, LLMs display the compression effect seen in previous work on human judgments in document contexts. Different sorts of LLMs are able to predict human acceptability judgments to a high degree of accuracy, but in general, their performance is slightly better when visual contexts are removed. Moreover, the distribution of LLM judgments varies among models, with Qwen resembling human patterns, and others diverging from them. LLM-generated predictions on sentence acceptability are highly correlated with their normalised log probabilities in general. However, the correlations decrease when visual contexts are present, suggesting that a higher gap exists between the internal representations of LLMs and their generated predictions in the presence of visual contexts. Our experimental work suggests interesting points of similarity and of difference between human and LLM processing of sentences in multimodal contexts.

</details>


### [30] [HELP: HyperNode Expansion and Logical Path-Guided Evidence Localization for Accurate and Efficient GraphRAG](https://arxiv.org/abs/2602.20926)
*Yuqi Huang,Ning Liao,Kai Yang,Anning Hu,Shengchao Hu,Xiaoxing Wang,Junchi Yan*

Main category: cs.AI

TL;DR: 本文提出了一种新的框架 HELP，它通过 HyperNode 扩展和逻辑路径导向的证据定位策略，有效地平衡了准确性和实用性。HELP 在多轮推理 QA 方面表现出色，比领先的基于图的 RAG 基准模型速度快 28.8 倍。


<details>
  <summary>Details</summary>
Motivation: 现有 RAG 方法虽然缓解了 LLM 的知识边界和幻觉问题，但在多跳推理方面仍存在结构性依赖问题。GraphRAG 在解决这一问题上做出努力，但通常面临准确性与效率之间的权衡。

Method: HELP 通过 HyperNode 扩展策略迭代链接知识三元组形成一致性推理路径，以捕捉复杂结构依赖并确保检索准确性；并通过逻辑路径导向的证据定位策略，利用预先计算的图-文本相关性直接映射路径到语料库，以实现高效检索。该方法避免了昂贵的随机游走和语义失真，维持知识完整性和大幅降低检索延迟。

Result: 实验结果证明 HELP 在多个简单的和多轮推理 QA 基准测试上表现出色，并且相比领先的基于图的 RAG 基准模型具有 28.8 倍的速度提升。

Conclusion: 本文通过 HELP 提出了有效的平衡准确性和实用性的解决方案，为多轮推理 QA 任务提出了有潜力的新方法。

Abstract: Large Language Models (LLMs) often struggle with inherent knowledge boundaries and hallucinations, limiting their reliability in knowledge-intensive tasks. While Retrieval-Augmented Generation (RAG) mitigates these issues, it frequently overlooks structural interdependencies essential for multi-hop reasoning. Graph-based RAG approaches attempt to bridge this gap, yet they typically face trade-offs between accuracy and efficiency due to challenges such as costly graph traversals and semantic noise in LLM-generated summaries. In this paper, we propose HyperNode Expansion and Logical Path-Guided Evidence Localization strategies for GraphRAG (HELP), a novel framework designed to balance accuracy with practical efficiency through two core strategies: 1) HyperNode Expansion, which iteratively chains knowledge triplets into coherent reasoning paths abstracted as HyperNodes to capture complex structural dependencies and ensure retrieval accuracy; and 2) Logical Path-Guided Evidence Localization, which leverages precomputed graph-text correlations to map these paths directly to the corpus for superior efficiency. HELP avoids expensive random walks and semantic distortion, preserving knowledge integrity while drastically reducing retrieval latency. Extensive experiments demonstrate that HELP achieves competitive performance across multiple simple and multi-hop QA benchmarks and up to a 28.8$\times$ speedup over leading Graph-based RAG baselines.

</details>


### [31] [LogicGraph : Benchmarking Multi-Path Logical Reasoning via Neuro-Symbolic Generation and Verification](https://arxiv.org/abs/2602.21044)
*Yanrui Wu,Lingling Zhang,Xinyu Zhang,Jiayu Chang,Pengyu Li,Xu Jiang,Jingtao Hu,Jun Liu*

Main category: cs.AI

TL;DR: 本文提出了LogicGraph，一个用于评估多路径逻辑推理能力的基准，通过神经符号框架生成包含高度深度多路径推理和逻辑干扰的问题，揭示了现有大语言模型倾向于过早锁定单一路径而未能充分探索其他可能路径的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型的评估主要集中在收敛逻辑推理上，这忽略了现实世界中许多问题可以有多种有效解的情况。真实世界的推理问题往往需要探索多种逻辑路径而不是仅仅局限于一种解决方案。为了填补这一空白，本文提出了一个全新的基准工具——LogicGraph，旨在系统地评估多路径逻辑推理。

Method: 本文使用神经符号框架，通过后向逻辑生成和语义实例化生成推理问题。生成的问题包含了高深度和多样化的逻辑路径，以及内在的逻辑干扰，每个问题实例都关联有一组详尽的最小证据集。此外，本文还提出了一种无需参考的标准评估框架，用于严格评估模型在收敛和发散推理方面的表现。

Result: 实验发现当前最先进的语言模型通常倾向于过早锁定单一推理路径，未能充分探索其他可能性。随着推理深度的增加，这种路径选择的局限性问题变得更为明显。LogicGraph能够明确暴露这种差异，并为未来改进提供洞察。

Conclusion: 本文提出的LogicGraph为我们提供了一个更全面的视角来评估大型语言模型的逻辑推理能力，并揭示了当前模型存在的局限性，希望未来研究以此为基础推动大模型能力的进一步提升。

Abstract: Evaluations of large language models (LLMs) primarily emphasize convergent logical reasoning, where success is defined by producing a single correct proof. However, many real-world reasoning problems admit multiple valid derivations, requiring models to explore diverse logical paths rather than committing to one route. To address this limitation, we introduce LogicGraph, the first benchmark aimed to systematically evaluate multi-path logical reasoning, constructed via a neuro-symbolic framework that leverages backward logic generation and semantic instantiation. This pipeline yields solver-verified reasoning problems formalized by high-depth multi-path reasoning and inherent logical distractions, where each instance is associated with an exhaustive set of minimal proofs. We further propose a reference-free evaluation framework to rigorously assess model performance in both convergent and divergent regimes. Experiments on state-of-the-art language models reveal a common limitation: models tend to commit early to a single route and fail to explore alternatives, and the coverage gap grows substantially with reasoning depth. LogicGraph exposes this divergence gap and provides actionable insights to motivate future improvements. Our code and data will be released at https://github.com/kkkkarry/LogicGraph.

</details>


### [32] [Tool Building as a Path to "Superintelligence"](https://arxiv.org/abs/2602.21061)
*David Koplow,Tomer Galanti,Tomaso Poggio*

Main category: cs.AI

TL;DR: 该框架通过设计一项基准测试来测量大模型在逻辑领域外推理中达到充分步骤成功概率$γ$的能力，发现小模型的$γ$值随深度增加呈超线性下降，而前沿模型在该任务上表现出部分稳健性。成功的大规模推理依赖于精确的工具调用，这强调了工具设计对大模型实现通用超智能的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究团队设计了这项基准测试是为了评估大语言模型（LLMs）在进行逻辑领域外推理时能否达到一个足够高的步骤成功概率$γ$，因为这直接关系到模型能否通过测试时的搜索机制达到超级智能。

Method: 研究引入了一类涉及GF(2)电路重构的递增困难任务，这些任务从信息论的角度看是不可能靠随机选择或部分信息准确解决的。研究使用这些任务来评估模型所需的成功步骤概率$γ$的变化。

Result: 结果显示，小模型的$γ$值随任务深度增加而呈现出超线性下降的趋势，而更加先进的大模型在这些任务上表现出了部分稳健性。进一步的分析揭示，大规模推理的成功关键在于模型能够进行精确的工具调用。

Conclusion: 此研究表明，大模型在扩展深度上具有一定的稳健性，但整个系统在实现超级智能时需要依赖精确的工具调用。工具设计因此成为了通过Diligent Learner框架实现大语言模型向垂直智能转变的关键环节。

Abstract: The Diligent Learner framework suggests LLMs can achieve superintelligence via test-time search, provided a sufficient step-success probability $γ$. In this work, we design a benchmark to measure $γ$ on logical out-of-distribution inference. We construct a class of tasks involving GF(2) circuit reconstruction that grow more difficult with each reasoning step, and that are, from an information-theoretic standpoint, impossible to reliably solve unless the LLM carefully integrates all of the information provided. Our analysis demonstrates that while the $γ$ value for small LLMs declines superlinearly as depth increases, frontier models exhibit partial robustness on this task. Furthermore, we find that successful reasoning at scale is contingent upon precise tool calls, identifying tool design as a critical capability for LLMs to achieve general superintelligence through the Diligent Learner framework.

</details>


### [33] [Motivation is Something You Need](https://arxiv.org/abs/2602.21064)
*Mehdi Acheli,Walid Gaaloul*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work introduces a novel training paradigm that draws from affective neuroscience. Inspired by the interplay of emotions and cognition in the human brain and more specifically the SEEKING motivational state, we design a dual-model framework where a smaller base model is trained continuously, while a larger motivated model is activated intermittently during predefined "motivation conditions". The framework mimics the emotional state of high curiosity and anticipation of reward in which broader brain regions are recruited to enhance cognitive performance. Exploiting scalable architectures where larger models extend smaller ones, our method enables shared weight updates and selective expansion of network capacity during noteworthy training steps. Empirical evaluation on the image classification task demonstrates that, not only does the alternating training scheme efficiently and effectively enhance the base model compared to a traditional scheme, in some cases, the motivational model also surpasses its standalone counterpart despite seeing less data per epoch. This opens the possibility of simultaneously training two models tailored to different deployment constraints with competitive or superior performance while keeping training cost lower than when training the larger model.

</details>


### [34] [A Benchmark for Deep Information Synthesis](https://arxiv.org/abs/2602.21143)
*Debjit Paul,Daniel Murphy,Milan Gritta,Ronald Cardenas,Victor Prokhorov,Lena Sophia Bolliger,Aysim Toker,Roy Miles,Andreea-Maria Oncescu,Jasivan Alex Sivakumar,Philipp Borchert,Ismail Elezi,Meiru Zhang,Ka Yiu Lee,Guchun Zhang,Jun Wang,Gerasimos Lampouras*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. To address this, we introduce DEEPSYNTH, a novel benchmark designed to evaluate agents on realistic, time-consuming problems that combine information gathering, synthesis, and structured reasoning to produce insights. DEEPSYNTH contains 120 tasks collected across 7 domains and data sources covering 67 countries. DEEPSYNTH is constructed using a multi-stage data collection pipeline that requires annotators to collect official data sources, create hypotheses, perform manual analysis, and design tasks with verifiable answers. When evaluated on DEEPSYNTH, 11 state-of-the-art LLMs and deep research agents achieve a maximum F1 score of 8.97 and 17.5 on the LLM-judge metric, underscoring the difficulty of the benchmark. Our analysis reveals that current agents struggle with hallucinations and reasoning over large information spaces, highlighting DEEPSYNTH as a crucial benchmark for guiding future research.

</details>


### [35] [CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning](https://arxiv.org/abs/2602.21154)
*Ziwei Niu,Hao Sun,Shujun Bian,Xihong Yang,Lanfen Lin,Yuxin Liu,Yueming Jin*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate interpretation of electrocardiogram (ECG) signals is crucial for diagnosing cardiovascular diseases. Recent multimodal approaches that integrate ECGs with accompanying clinical reports show strong potential, but they still face two main concerns from a modality perspective: (1) intra-modality: existing models process ECGs in a lead-agnostic manner, overlooking spatial-temporal dependencies across leads, which restricts their effectiveness in modeling fine-grained diagnostic patterns; (2) inter-modality: existing methods directly align ECG signals with clinical reports, introducing modality-specific biases due to the free-text nature of the reports. In light of these two issues, we propose CG-DMER, a contrastive-generative framework for disentangled multimodal ECG representation learning, powered by two key designs: (1) Spatial-temporal masked modeling is designed to better capture fine-grained temporal dynamics and inter-lead spatial dependencies by applying masking across both spatial and temporal dimensions and reconstructing the missing information. (2) A representation disentanglement and alignment strategy is designed to mitigate unnecessary noise and modality-specific biases by introducing modality-specific and modality-shared encoders, ensuring a clearer separation between modality-invariant and modality-specific representations. Experiments on three public datasets demonstrate that CG-DMER achieves state-of-the-art performance across diverse downstream tasks.

</details>


### [36] [Aletheia tackles FirstProof autonomously](https://arxiv.org/abs/2602.21201)
*Tony Feng,Junehyuk Jung,Sang-hyun Kim,Carlo Pagano,Sergei Gukov,Chiang-Chiang Tsai,David Woodruff,Adel Javanmard,Aryan Mokhtari,Dawsen Hwang,Yuri Chervonyi,Jonathan N. Lee,Garrett Bingham,Trieu H. Trinh,Vahab Mirrokni,Quoc V. Le,Thang Luong*

Main category: cs.AI

TL;DR: Aletheia 解决了 FirstProof 挑战中的6个数学问题中的6个，展示出了强大的数学研究能力。


<details>
  <summary>Details</summary>
Motivation: 为了展示 Aletheia 在数学研究领域的强大能力，特别是其基于 Gemini 3 Deep Think 的自动生成数学证明的功能。

Method: 通过提供原始提示和输出，公开了 Aletheia 在 FirstProof 挑战中的具体表现。

Result: Aletheia 在 FirstProof 挑战中解决了 6 个问题中的 6 个，专家评估一致，但对一个问题（问题 8）有分歧。

Conclusion: Aletheia 显示出了在数学问题解决方面的显著进展，但仍需进一步优化以保证所有问题的专家共识。

Abstract: We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.

</details>
