{"id": "2602.12398", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.12398", "abs": "https://arxiv.org/abs/2602.12398", "authors": ["Paul Keeler", "Ben Smyth"], "title": "Secrecy and Verifiability: An Introduction to Electronic Voting", "comment": "67 pages, 10 figures. Tutorial on cryptographic foundations of electronic voting", "summary": "Democracies are built upon secure and reliable voting systems. Electronic voting systems seek to replace ballot papers and boxes with computer hardware and software. Proposed electronic election schemes have been subjected to scrutiny, with researchers spotting inherent faults and weaknesses. Inspired by physical voting systems, we argue that any electronic voting system needs two essential properties: ballot secrecy and verifiability. These properties seemingly work against each other. An election scheme that is a complete black box offers ballot secrecy, but verification of the outcome is impossible. This challenge can be tackled using standard tools from modern cryptography, reaching a balance that delivers both properties.\n  This tutorial makes these ideas accessible to readers outside electronic voting. We introduce fundamental concepts such as asymmetric and homomorphic encryption, which we use to describe a general electronic election scheme while keeping mathematical formalism minimal. We outline game-based cryptography, a standard approach in modern cryptography, and introduce notation for formulating elections as games. We then give precise definitions of ballot secrecy and verifiability in the framework of game-based cryptography. A principal aim is introducing modern research approaches to electronic voting.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u73b0\u4ee3\u52a0\u5bc6\u6280\u672f\u548c\u535a\u5f08\u8bba\u5728\u7535\u5b50\u6295\u7968\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u65e8\u5728\u5b9e\u73b0\u9009\u7968\u673a\u5bc6\u6027\u548c\u9a8c\u8bc1\u6027\u7684\u5e73\u8861\u3002", "motivation": "\u4e3a\u4e86\u6539\u5584\u7535\u5b50\u6295\u7968\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u8fd0\u7528\u73b0\u4ee3\u52a0\u5bc6\u6280\u672f\u548c\u535a\u5f08\u8bba\uff0c\u786e\u4fdd\u9009\u7968\u79d8\u5bc6\u540c\u65f6\u80fd\u591f\u8fdb\u884c\u7ed3\u679c\u9a8c\u8bc1\u3002", "method": "\u6587\u7ae0\u901a\u8fc7\u5f15\u5165\u975e\u5bf9\u79f0\u52a0\u5bc6\u3001\u540c\u6001\u52a0\u5bc6\u7b49\u6982\u5ff5\uff0c\u5e76\u4f7f\u7528\u535a\u5f08\u7406\u8bba\u6846\u67b6\u6765\u63cf\u8ff0\u7535\u5b50\u9009\u4e3e\u65b9\u6848\u3002", "result": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u7535\u5b50\u9009\u4e3e\u65b9\u6848\u5b9a\u4e49\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u7b80\u6d01\u6570\u5b66\u5f62\u5f0f\u7684\u540c\u65f6\uff0c\u660e\u786e\u9009\u7968\u79d8\u5bc6\u6027\u548c\u9a8c\u8bc1\u6027\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u4e3a\u7535\u5b50\u6295\u7968\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u63d0\u4f9b\u65b0\u7684\u89c6\u89d2\u548c\u6280\u672f\u624b\u6bb5\uff0c\u4fc3\u8fdb\u4e86\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2602.12418", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12418", "abs": "https://arxiv.org/abs/2602.12418", "authors": ["Yannick Assogba", "Jacopo Cortellazzi", "Javier Abad", "Pau Rodriguez", "Xavier Suau", "Arno Blaas"], "title": "Sparse Autoencoders are Capable LLM Jailbreak Mitigators", "comment": "26 pages, 14 figures, 3 tables", "summary": "Jailbreak attacks remain a persistent threat to large language model safety. We propose Context-Conditioned Delta Steering (CC-Delta), an SAE-based defense that identifies jailbreak-relevant sparse features by comparing token-level representations of the same harmful request with and without jailbreak context. Using paired harmful/jailbreak prompts, CC-Delta selects features via statistical testing and applies inference-time mean-shift steering in SAE latent space. Across four aligned instruction-tuned models and twelve jailbreak attacks, CC-Delta achieves comparable or better safety-utility tradeoffs than baseline defenses operating in dense latent space. In particular, our method clearly outperforms dense mean-shift steering on all four models, and particularly against out-of-distribution attacks, showing that steering in sparse SAE feature space offers advantages over steering in dense activation space for jailbreak mitigation. Our results suggest off-the-shelf SAEs trained for interpretability can be repurposed as practical jailbreak defenses without task-specific training.", "AI": {"tldr": "CC-Delta \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSAE\u7684\u9632\u5fa1\u673a\u5236\uff0c\u901a\u8fc7\u5728\u7a00\u758f\u7279\u5f81\u7a7a\u95f4\u4e2d\u8fdb\u884c\u63a8\u7406\u65f6\u95f4\u7684\u5747\u503c\u504f\u79fb\u6307\u5bfc\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5bf9 Jailbreak \u653b\u51fb\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u6301\u7eed\u5a01\u80c1\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u6027\u7684 Jailbreak \u653b\u51fb\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9632\u5fa1\u65b9\u6cd5 CC-Delta\uff0c\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u5bf9\u6709\u5bb3\u8bf7\u6c42\u7684\u5904\u7406\u80fd\u529b\u3002", "method": "CC-Delta \u901a\u8fc7\u5c06\u6709\u5bb3\u8bf7\u6c42\u548c\u5b58\u5728 Jailbreak \u4e0a\u4e0b\u6587\u7684\u540c\u4e2a\u8bf7\u6c42\u7684\u8868\u793a\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc6\u522b\u51fa\u4e0e Jailbreak \u6709\u5173\u7684\u7a00\u758f\u7279\u5f81\u3002\u5229\u7528\u914d\u5bf9\u7684\u6709\u5bb3/ Jailbreak \u63d0\u793a\uff0c\u5b83\u8fdb\u884c\u4e86\u7edf\u8ba1\u6d4b\u8bd5\u6765\u9009\u62e9\u7279\u5f81\uff0c\u5e76\u5728 SAE \u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5e94\u7528\u63a8\u7406\u65f6\u95f4\u7684\u5747\u503c\u504f\u79fb\u6307\u5bfc\u3002", "result": "CC-Delta \u5728\u56db\u4e2a\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u548c\u5341\u4e8c\u79cd Jailbreak \u653b\u51fb\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u5728\u7a20\u5bc6\u6f5c\u5728\u7a7a\u95f4\u5de5\u4f5c\u7684\u57fa\u7ebf\u9632\u5fa1\u76f8\u6bd4\uff0c\u5177\u6709\u76f8\u4f3c\u6216\u66f4\u597d\u7684\u5b89\u5168-\u5b9e\u7528\u6027\u6743\u8861\u3002\u7279\u522b\u662f\u5728\u5904\u7406\u5f02\u5e38\u5206\u5e03\u653b\u51fb\u65b9\u9762\uff0cCC-Delta \u7684\u8868\u73b0\u4f18\u4e8e\u7a20\u5bc6\u5747\u503c\u504f\u79fb\u6307\u5bfc\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u9488\u5bf9 Jailbreak \u653b\u51fb\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u53ef\u89e3\u91ca\u6027 SAE \u53ef\u4ee5\u518d\u5229\u7528\u4f5c\u4e3a\u5b9e\u9645\u7684\u9632\u5fa1\u673a\u5236\uff0c\u65e0\u9700\u7279\u5b9a\u4efb\u52a1\u7684\u8bad\u7ec3\u3002"}}
{"id": "2602.12433", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.12433", "abs": "https://arxiv.org/abs/2602.12433", "authors": ["Niklas Klinger", "Jonas Sander", "Peterson Yuhala", "Pascal Felber", "Thomas Eisenbarth"], "title": "DRAMatic Speedup: Accelerating HE Operations on a Processing-in-Memory System", "comment": null, "summary": "Homomorphic encryption (HE) is a promising technology for confidential cloud computing, as it allows computations on encrypted data. However, HE is computationally expensive and often memory-bound on conventional computer architectures. Processing-in-Memory (PIM) is an alternative hardware architecture that integrates processing units and memory on the same chip or memory module. PIM enables higher memory bandwidth than conventional architectures and could thus be suitable for accelerating HE. In this work, we present DRAMatic, which implements operations foundational to HE on UPMEM's programmable, general-purpose PIM system, and evaluate its performance. DRAMatic incorporates many arithmetic optimizations, including residue number system and number-theoretic transform techniques, and can support the large parameters required for secure homomorphic evaluations. To compare performance, we evaluate DRAMatic against Microsoft SEAL, a popular open-source HE library, regarding both runtime and energy efficiency. The results show that DRAMatic significantly closes the gap between UPMEM PIM and Microsoft SEAL. However, we also show that DRAMatic is currently constrained by UPMEM PIM's multiplication performance and data transfer overhead. Finally, we discuss potential hardware extensions to UPMEM PIM.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86DRAMatic\uff0c\u4e00\u79cd\u5728UPMEM\u7684Processing-in-Memory\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u540c\u6001\u52a0\u5bc6\u57fa\u7840\u64cd\u4f5c\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u8fd0\u884c\u65f6\u548c\u80fd\u6548\u3002\u5c3d\u7ba1\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4ecd\u53d7\u9650\u4e8e\u7cfb\u7edf\u4e58\u6cd5\u6027\u80fd\u548c\u6570\u636e\u4f20\u8f93\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u540c\u6001\u52a0\u5bc6\u6280\u672f\u5728\u5e38\u89c4\u8ba1\u7b97\u673a\u67b6\u6784\u4e0b\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5185\u5b58\u7ed1\u5b9a\u4e25\u91cd\uff0c\u800cProcessing-in-Memory (PIM) \u67b6\u6784\u56e0\u5176\u9ad8\u5185\u5b58\u5e26\u5bbd\u6210\u4e3a\u53ef\u80fd\u7684\u52a0\u901f\u624b\u6bb5\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5b9e\u73b0\u4e86DRAMatic\uff0c\u9488\u5bf9UPMEM\u7684PIM\u7cfb\u7edf\u4f18\u5316\u5e76\u5229\u7528\u4e86\u591a\u79cd\u7b97\u672f\u4f18\u5316\u6280\u672f\uff0c\u5305\u62ec\u6a21\u6570\u7cfb\u7edf\u548c\u6570\u8bba\u53d8\u6362\u6280\u672f\uff0c\u4ee5\u652f\u6301\u5b89\u5168\u7684\u540c\u6001\u8ba1\u7b97\u3002\u5e76\u5c06DRAMatic\u4e0eMicrosoft SEAL\u8fdb\u884c\u4e86\u6027\u80fd\u6bd4\u8f83\u3002", "result": "DRAMatic\u5728\u8fd0\u884c\u65f6\u548c\u80fd\u6548\u65b9\u9762\u663e\u8457\u63d0\u9ad8\u4e86UPMEM PIM\u7cfb\u7edf\u7684\u8868\u73b0\uff0c\u76f8\u8f83\u4e8e\u5fae\u8f6f\u7684\u5f00\u6e90\u540c\u6001\u52a0\u5bc6\u5e93Microsoft SEAL\u3002", "conclusion": "\u5c3d\u7ba1\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u7814\u7a76\u6307\u51fa\u76ee\u524dDRAMatic\u4ecd\u53d7\u5236\u4e8eUPMEM PIM\u7cfb\u7edf\u4e58\u6cd5\u6027\u80fd\u548c\u6570\u636e\u4f20\u8f93\u5f00\u9500\u7684\u9650\u5236\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u53ef\u80fd\u7684\u786c\u4ef6\u6269\u5c55\u3002"}}
{"id": "2602.12600", "categories": ["cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.12600", "abs": "https://arxiv.org/abs/2602.12600", "authors": ["Mahfuzul I. Nissan", "James Wagner"], "title": "RADAR: Exposing Unlogged NoSQL Operations", "comment": null, "summary": "The widespread adoption of NoSQL databases has made digital forensics increasingly difficult as storage formats are diverse and often opaque, and audit logs cannot be assumed trustworthy when privileged insiders, such as DevOps or administrators, can disable, suppress, or manipulate logging to conceal activity. We present RADAR (Record & Artifact Detection, Alignment & Reporting), a log-adversary-aware framework that derives forensic ground truth by cross-referencing low-level storage artifacts against high-level application logs. RADAR analyzes artifacts reconstructed by the Automated NoSQL Carver (ANOC), which infers layouts and carves records directly from raw disk bytes, bypassing database APIs and the management system entirely, thereby treating physical storage as the independent evidence source. RADAR then reconciles carved artifacts with the audit log to identify delta artifacts such as unlogged insertions, silent deletions, and field-level updates that exist on disk but are absent from the logical history. We evaluate RADAR across ten NoSQL engines, including BerkeleyDB, LMDB, MDBX, etcd, ZODB, Durus, LiteDB, Realm, RavenDB, and NitriteDB, spanning key-value and document stores and multiple storage designs, e.g., copy-on-write/MVCC, B/B+ tree, and append-only. Under log-evasion scenarios, such as log suppression and post-maintenance attacks, including cases where historical bytes are pruned, RADAR consistently exposes unattributed operations while sustaining 31.7-397 MB/min processing throughput, demonstrating the feasibility of log-independent, trustworthy NoSQL forensics.", "AI": {"tldr": "RADAR \u662f\u4e00\u79cd\u65b0\u7684\u65e5\u5fd7\u654c\u4eba\u610f\u8bc6\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4f4e\u7ea7\u5b58\u50a8\u788e\u7247\u4e0e\u9ad8\u7ea7\u5e94\u7528\u7a0b\u5e8f\u65e5\u5fd7\u8fdb\u884c\u4ea4\u53c9\u5f15\u7528\uff0c\u6765\u63a8\u5bfc\u51fa\u6cd5\u533b\u771f\u76f8\uff0c\u4ece\u800c\u5728 NoSQL \u6570\u636e\u5e93\u4e2d\u8fdb\u884c\u53d6\u8bc1\uff0c\u5373\u4f7f\u5728\u65e5\u5fd7\u88ab\u7be1\u6539\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u68c0\u6d4b\u5230\u672a\u6ce8\u518c\u7684\u64cd\u4f5c\u3002", "motivation": "\u7531\u4e8e NoSQL \u6570\u636e\u5e93\u7684\u5b58\u50a8\u683c\u5f0f\u591a\u6837\u4e14\u4e0d\u900f\u660e\uff0c\u4e14\u4e0d\u88ab\u4fe1\u4efb\u7684\u5185\u90e8\u4eba\u58eb\u53ef\u4ee5\u7981\u7528\u3001\u6291\u5236\u6216\u64cd\u7eb5\u65e5\u5fd7\u8bb0\u5f55\u4ee5\u9690\u85cf\u6d3b\u52a8\uff0c\u56e0\u6b64\u6570\u5b57\u53d6\u8bc1\u53d8\u5f97\u66f4\u52a0\u56f0\u96be\u3002", "method": "RADAR \u901a\u8fc7\u5206\u6790\u81ea\u52a8\u5316 NoSQL \u91c7\u96c6\u5668\uff08ANOC\uff09\u91cd\u5efa\u7684\u788e\u7247\uff0c\u5e76\u5c06\u5b83\u4eec\u4e0e\u5ba1\u8ba1\u65e5\u5fd7\u8fdb\u884c\u5bf9\u6bd4\u6765\u8bc6\u522b\u5728\u78c1\u76d8\u4e0a\u5b58\u5728\u4f46\u672a\u5728\u903b\u8f91\u5386\u53f2\u8bb0\u5f55\u4e2d\u8bb0\u5f55\u7684\u64cd\u4f5c\u3002", "result": "RADAR \u80fd\u591f\u6709\u6548\u8bc6\u8bc6\u522b\u672a\u8bb0\u5f55\u7684\u64cd\u4f5c\uff0c\u5e76\u5728\u4e0d\u540c NoSQL \u6570\u636e\u5e93\u5f15\u64ce\u4e2d\u53d6\u5f97\u4e86 31.7-397 MB/min \u7684\u5904\u7406\u541e\u5410\u91cf\u3002", "conclusion": "RADAR \u63d0\u4f9b\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u65e5\u5fd7\u7684\u65e5 NoSQL \u6570\u636e\u5e93\u53d6\u8bc1\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u65e5\u5fd7\u88ab\u7be1\u6539\u7684\u60c5\u51b5\u4e0b\u4ecd\u7136\u68c0\u6d4b\u5230\u672a\u5f52\u8d23\u7684\u64cd\u4f5c\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86 NoSQL \u6570\u636e\u5e93\u7684\u6cd5\u533b\u5206\u6790\u80fd\u529b\u3002"}}
{"id": "2602.12630", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12630", "abs": "https://arxiv.org/abs/2602.12630", "authors": ["Oguzhan Baser", "Elahe Sadeghi", "Eric Wang", "David Ribeiro Alves", "Sam Kazemian", "Hong Kang", "Sandeep P. Chinchali", "Sriram Vishwanath"], "title": "TensorCommitments: A Lightweight Verifiable Inference for Language Models", "comment": "23 pages, 8 figures, under review", "summary": "Most large language models (LLMs) run on external clouds: users send a prompt, pay for inference, and must trust that the remote GPU executes the LLM without any adversarial tampering. We critically ask how to achieve verifiable LLM inference, where a prover (the service) must convince a verifier (the client) that an inference was run correctly without rerunning the LLM. Existing cryptographic works are too slow at the LLM scale, while non-cryptographic ones require a strong verifier GPU. We propose TensorCommitments (TCs), a tensor-native proof-of-inference scheme. TC binds the LLM inference to a commitment, an irreversible tag that breaks under tampering, organized in our multivariate Terkle Trees. For LLaMA2, TC adds only 0.97% prover and 0.12% verifier time over inference while improving robustness to tailored LLM attacks by up to 48% over the best prior work requiring a verifier GPU.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86TensorCommitments (TCs) \u65b9\u6848\uff0c\u901a\u8fc7\u7ed1\u5b9aLLM\u63a8\u7406\u4e0e\u627f\u8bfa\uff0c\u6765\u5b9e\u73b0\u65e0\u9700\u91cd\u65b0\u8fd0\u884cLLM\u5373\u53ef\u9a8c\u8bc1\u63a8\u7406\u6b63\u786e\u6027\u7684\u76ee\u6807\u3002\u5bf9\u4e8eLLaMA2\uff0c\u8be5\u65b9\u6848\u4ec5\u589e\u52a0\u5fae\u4e4e\u5176\u5fae\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u63d0\u9ad8\u5bf9\u5b9a\u5236\u653b\u51fb\u7684\u62b5\u6297\u529b\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u65b9\u6cd5\u5728\u6548\u7387\u6216\u786c\u4ef6\u8981\u6c42\u4e0a\u5b58\u5728\u7f3a\u9677\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u627e\u5230\u4e00\u79cd\u80fd\u591f\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u6027\u7684\u65b0\u65b9\u6848\uff0c\u4ee5\u786e\u4fdd\u63a8\u7406\u7ed3\u679c\u7684\u771f\u5b9e\u6027\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86TensorCommitments (TCs)\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u5f20\u91cf\u7684\u63a8\u7406\u8bc1\u660e\u65b9\u6848\u3002TC\u5c06LLM\u63a8\u7406\u7ed3\u679c\u4e0e\u4e00\u4e2a\u4e0d\u53ef\u9006\u6807\u8bb0\uff08\u627f\u8bfa\uff09\u7ed1\u5b9a\uff0c\u8be5\u6807\u8bb0\u4e0b\u5728\u906d\u53d7\u7be1\u6539\u65f6\u4f1a\u5931\u6548\u3002\u627f\u8bfa\u7ec4\u7ec7\u5728Terkle Tree\u4e2d\u5b58\u5728\u7684\u591a\u53d8\u91cf\u7ed3\u6784\u4e2d\u3002", "result": "\u5bf9\u4e8eLLaMA2\uff0c\u8be5\u65b9\u6848\u5728\u9a8c\u8bc1\u8005\u7aef\u548c\u8bc1\u660e\u8005\u7aef\u5206\u522b\u589e\u52a0\u4e86\u4ec50.97%\u548c0.12%\u7684\u65f6\u95f4\u5f00\u9500\uff0c\u540c\u65f6\u80fd\u591f\u5c06\u9488\u5bf9\u7279\u5b9aLLM\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\u63d0\u9ad848%\uff0c\u8d85\u8fc7\u4e86\u539f\u6709\u9700\u8981\u4f7f\u7528\u5f3a\u5927\u9a8c\u8bc1\u8005GPU\u7684\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u4fe1\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u4ec5\u964d\u4f4e\u4e86\u5bf9\u786c\u4ef6\u7684\u9700\u6c42\uff0c\u800c\u4e14\u8fd8\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u6297\u653b\u51fb\u80fd\u529b\u3002"}}
{"id": "2602.12681", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12681", "abs": "https://arxiv.org/abs/2602.12681", "authors": ["Jiyong Uhm", "Minseok Kim", "Michalis Polychronakis", "Hyungjoon Koo"], "title": "Fool Me If You Can: On the Robustness of Binary Code Similarity Detection Models against Semantics-preserving Transformations", "comment": "23 pages, 9 figures, 5 tables. The paper has been accepted by The ACM International Conference on the Foundations of Software Engineering (FSE 2026)", "summary": "Binary code analysis plays an essential role in cybersecurity, facilitating reverse engineering to reveal the inner workings of programs in the absence of source code. Traditional approaches, such as static and dynamic analysis, extract valuable insights from stripped binaries, but often demand substantial expertise and manual effort. Recent advances in deep learning have opened promising opportunities to enhance binary analysis by capturing latent features and disclosing underlying code semantics. Despite the growing number of binary analysis models based on machine learning, their robustness to adversarial code transformations at the binary level remains underexplored. We evaluate the robustness of deep learning models for the task of binary code similarity detection (BCSD) under semantics-preserving transformations. The unique nature of machine instructions presents distinct challenges compared to the typical input perturbations found in other domains. We introduce asmFooler, a system that evaluates the resilience of BCSD models using a diverse set of adversarial code transformations that preserve functional semantics. We construct a dataset of 9,565 binary variants from 620 baseline samples by applying eight semantics-preserving transformations across six representative BCSD models. Our major findings highlight several key insights: i) model robustness relies on the processing pipeline, including code pre-processing, architecture, and feature selection; ii) adversarial transformation effectiveness is bounded by a budget shaped by model-specific constraints like input size and instruction expressive capacity; iii) well-crafted transformations can be highly effective with minimal perturbations; and iv) such transformations efficiently disrupt model decisions (e.g., misleading to false positives or false negatives) by focusing on semantically significant instructions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u9488\u5bf9\u4e8c\u8fdb\u5236\u4ee3\u7801\u76f8\u4f3c\u6027\u68c0\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aasmFooler\u7684\u7cfb\u7edf\uff0c\u5b9a\u4e49\u4e86\u591a\u79cd\u4fdd\u6301\u529f\u80fd\u8bed\u4e49\u7684\u5bf9\u6297\u6027\u4ee3\u7801\u8f6c\u6362\uff0c\u5e76\u5c55\u793a\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u4f9d\u8d56\u4e8e\u9884\u5904\u7406\u3001\u67b6\u6784\u548c\u7279\u5f81\u9009\u62e9\u7b49\u56e0\u7d20\u3002", "motivation": "\u5f53\u524d\u73b0\u6709\u7684\u4e8c\u8fdb\u5236\u4ee3\u7801\u5206\u6790\u6a21\u578b\u5728\u9762\u5bf9\u5bf9\u6297\u6027\u4ee3\u7801\u8f6c\u6362\u65f6\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u56e0\u6b64\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4e8c\u8fdb\u5236\u4ee3\u7801\u76f8\u4f3c\u6027\u68c0\u6d4b\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u4f9b\u5bf9\u6297\u6027\u4ee3\u7801\u8f6c\u6362\u5bf9\u68c0\u6d4b\u6a21\u578b\u5f71\u54cd\u7684\u6df1\u5165\u89c1\u89e3\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u5305\u542b9,565\u4e2a\u4e8c\u8fdb\u5236\u53d8\u4f53\u7684\u6570\u636e\u96c6\uff0c\u5e94\u7528\u516b\u79cd\u4fdd\u7559\u529f\u80fd\u8bed\u4e49\u7684\u8f6c\u6362\uff0c\u5bf9\u516d\u79cd\u4ee3\u8868\u6027\u7684BCSD\u6a21\u578b\u7684\u9c81\u68d2\u6027\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6a21\u578b\u7684\u9c81\u68d2\u6027\u4e0e\u9884\u5904\u7406\u8fc7\u7a0b\u3001\u67b6\u6784\u548c\u7279\u5f81\u9009\u62e9\u7b49\u56e0\u7d20\u6709\u5173\u3002\u5bf9\u6297\u6027\u8f6c\u6362\u7684\u6548\u679c\u53d7\u9650\u4e8e\u6a21\u578b\u7279\u5b9a\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8f6c\u6362\u5373\u4f7f\u5728\u8f7b\u5fae\u7684\u6270\u52a8\u4e0b\u4e5f\u53ef\u4ee5\u6709\u6548\u5f71\u54cd\u6a21\u578b\u7684\u51b3\u7b56\uff0c\u5305\u62ec\u8bef\u5bfc\u6a21\u578b\u4ea7\u751f\u5047\u9633\u6027\u6216\u5047\u9634\u6027\u7ed3\u679c\uff0c\u8fd9\u4e9b\u8f6c\u6362\u4e3b\u8981\u9488\u5bf9\u8bed\u4e49\u4e0a\u91cd\u8981\u7684\u6307\u4ee4\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u4e8c\u8fdb\u5236\u4ee3\u7801\u9886\u57df\u4fdd\u62a4\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u6a21\u578b\u5728\u9762\u5bf9\u9690\u8eab\u5bf9\u6297\u6027\u4ee3\u7801\u8f6c\u6362\u65f6\u7684\u8106\u5f31\u6027\u3002"}}
{"id": "2602.12389", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12389", "abs": "https://arxiv.org/abs/2602.12389", "authors": ["Siyuan Li", "Yunjia Wu", "Yiyong Xiao", "Pingyang Huang", "Peize Li", "Ruitong Liu", "Yan Wen", "Te Sun", "Fangyi Pei"], "title": "Evolving Beyond Snapshots: Harmonizing Structure and Sequence via Entity State Tuning for Temporal Knowledge Graph Forecasting", "comment": null, "summary": "Temporal knowledge graph (TKG) forecasting requires predicting future facts by jointly modeling structural dependencies within each snapshot and temporal evolution across snapshots. However, most existing methods are stateless: they recompute entity representations at each timestamp from a limited query window, leading to episodic amnesia and rapid decay of long-term dependencies. To address this limitation, we propose Entity State Tuning (EST), an encoder-agnostic framework that endows TKG forecasters with persistent and continuously evolving entity states. EST maintains a global state buffer and progressively aligns structural evidence with sequential signals via a closed-loop design. Specifically, a topology-aware state perceiver first injects entity-state priors into structural encoding. Then, a unified temporal context module aggregates the state-enhanced events with a pluggable sequence backbone. Subsequently, a dual-track evolution mechanism writes the updated context back to the global entity state memory, balancing plasticity against stability. Experiments on multiple benchmarks show that EST consistently improves diverse backbones and achieves state-of-the-art performance, highlighting the importance of state persistence for long-horizon TKG forecasting. The code is published at https://github.com/yuanwuyuan9/Evolving-Beyond-Snapshots", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6Entity State Tuning (EST)\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u5927\u591a\u6570\u65f6\u95f4\u77e5\u8bc6\u56fe\u8c31(TKG)\u9884\u6d4b\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u77ed\u671f\u8bb0\u5fc6\u80fd\u529b\u548c\u957f\u671f\u4f9d\u8d56\u7ef4\u6301\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u591a\u6570TKG\u9884\u6d4b\u65b9\u6cd5\u5728\u6bcf\u4e2a\u65f6\u95f4\u6233\u91cd\u65b0\u8ba1\u7b97\u5b9e\u4f53\u8868\u793a\uff0c\u5bfc\u81f4\u4e86\u77ed\u671f\u8bb0\u5fc6\u80fd\u529b\u7684\u7f3a\u5931\u548c\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u7684\u5feb\u901f\u8870\u51cf\u3002EST\u6846\u67b6\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u4e0d\u8017\u5c3d\u67e5\u8be2\u7a97\u53e3\u7684\u60c5\u51b5\u4e0b\u66f4\u597d\u5730\u4fdd\u6301\u548c\u6301\u7eed\u66f4\u65b0\u5b9e\u4f53\u72b6\u6001\u3002", "method": "EST\u6846\u67b6\u5305\u542b\u62d3\u6251\u611f\u77e5\u72b6\u6001\u611f\u77e5\u5668\u3001\u7edf\u4e00\u65f6\u95f4\u4e0a\u4e0b\u6587\u6a21\u5757\u548c\u53cc\u8f68\u6f14\u5316\u673a\u5236\u3002\u9996\u5148\uff0c\u72b6\u6001\u611f\u77e5\u5668\u6ce8\u5165\u5b9e\u4f53\u72b6\u6001\u5148\u9a8c\u4fe1\u606f\u5230\u7ed3\u6784\u7f16\u7801\u4e2d\u3002\u5176\u6b21\uff0c\u65f6\u95f4\u4e0a\u4e0b\u6587\u6a21\u5757\u4f7f\u7528\u53ef\u63d2\u62d4\u7684\u65f6\u95f4\u5e8f\u5217\u9aa8\u5e72\u6765\u805a\u5408\u72b6\u6001\u589e\u5f3a\u7684\u4e8b\u4ef6\u3002\u6700\u540e\uff0c\u6f14\u5316\u673a\u5236\u5c06\u66f4\u65b0\u540e\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u5199\u56de\u5230\u5168\u5c40\u5b9e\u4f53\u72b6\u6001\u5b58\u50a8\u4e2d\uff0c\u5b9e\u73b0\u4e86\u72b6\u6001\u7684\u6301\u7eed\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86EST\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u63d0\u5347\u5404\u79cd\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u957f\u9884\u89c1\u6027TKG\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "EST\u6846\u67b6\u901a\u8fc7\u878d\u5408\u5b9e\u4f53\u72b6\u6001\u7684\u6301\u4e45\u6027\u548c\u6301\u7eed\u6f14\u5316\uff0c\u6210\u529f\u63d0\u9ad8\u4e86TKG\u9884\u6d4b\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\uff0c\u4e3a\u672a\u6765\u7684TKG\u9884\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2602.12825", "categories": ["cs.CR", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.12825", "abs": "https://arxiv.org/abs/2602.12825", "authors": ["Rub\u00e9n P\u00e9rez-Jove", "Osvaldo Simeone", "Alejandro Pazos", "Jose V\u00e1zquez-Naya"], "title": "Reliable Hierarchical Operating System Fingerprinting via Conformal Prediction", "comment": "Submitted as a preprint (not peer reviewed). 16 pages, 10 figures. Code and datasets available at: https://github.com/rubenpjove/CP-HOSfing", "summary": "Operating System (OS) fingerprinting is critical for network security, but conventional methods do not provide formal uncertainty quantification mechanisms. Conformal Prediction (CP) could be directly wrapped around existing methods to obtain prediction sets with guaranteed coverage. However, a direct application of CP would treat OS identification as a flat classification problem, ignoring the natural taxonomic structure of OSs and providing brittle point predictions. This work addresses these limitations by introducing and evaluating two distinct structured CP strategies: level-wise CP (L-CP), which calibrates each hierarchy level independently, and projection-based CP (P-CP), which ensures structural consistency by projecting leaf-level sets upwards. Our results demonstrate that, while both methods satisfy validity guarantees, they expose a fundamental trade-off between level-wise efficiency and structural consistency. L-CP yields tighter prediction sets suitable for human forensic analysis but suffers from taxonomic inconsistencies. Conversely, P-CP guarantees hierarchically consistent, nested sets ideal for automated policy enforcement, albeit at the cost of reduced efficiency at coarser levels.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f15\u5165\u5e76\u8bc4\u4f30\u4e86\u4e24\u79cd\u7ed3\u6784\u5316\u7684\u7f6e\u4fe1\u9884\u6d4b\u65b9\u6cd5\u2014\u2014\u5c42\u6b21\u7ea7\u7f6e\u4fe1\u9884\u6d4b (L-CP) \u548c\u6295\u5f71\u57fa\u4e8e\u7f6e\u4fe1\u9884\u6d4b (P-CP)\uff0c\u4ee5\u6539\u8fdb\u64cd\u4f5c\u7cfb\u7edf\u6307\u7eb9\u8bc6\u522b\u4e2d\u7684\u5f62\u5f0f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002L-CP \u63d0\u4f9b\u66f4\u7d27\u51d1\u7684\u9884\u6d4b\u96c6\u9002\u5408\u4eba\u5de5\u6cd5\u533b\u5206\u6790\uff0c\u4f46\u5b58\u5728\u5206\u7c7b\u7ed3\u6784\u4e0d\u4e00\u81f4\u95ee\u9898\uff1bP-CP \u786e\u4fdd\u5c42\u6b21\u4e0a\u7684\u4e00\u81f4\u6027\u548c\u5d4c\u5957\u6027\u9002\u7528\u4e8e\u81ea\u52a8\u5316\u7b56\u7565\u6267\u884c\uff0c\u4f46\u8fd9\u4f1a\u964d\u4f4e\u8f83\u7c97\u7c92\u5ea6\u7ea7\u522b\u7684\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u7684 OS \u6307\u7eb9\u8bc6\u522b\u65b9\u6cd5\u7f3a\u4e4f\u5f62\u5f0f\u5316\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u673a\u5236\uff0c\u800c\u91c7\u7528 Conformal Prediction (CP) \u53ef\u4ee5\u589e\u5f3a\u5b89\u5168\u6027\uff0c\u4f46\u4ecd\u4f1a\u9762\u4e34\u5c06 OS \u8bc6\u522b\u89c6\u4e3a\u5e73\u5766\u5206\u7c7b\u95ee\u9898\u7684\u5c40\u9650\u6027\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u5c42\u6b21\u5316 CP \u65b9\u6cd5\uff1a\u5c42\u6b21\u7ea7\u7f6e\u4fe1\u9884\u6d4b (L-CP) \u548c\u6295\u5f71\u57fa\u4e8e\u7f6e\u4fe1\u9884\u6d4b (P-CP)\u3002L-CP \u72ec\u7acb\u6821\u51c6\u6bcf\u7ea7\u5c42\u6b21\uff0c\u800c P-CP \u901a\u8fc7\u4ece\u53f6\u5b50\u7ea7\u522b\u5411\u4e0a\u4f20\u8f93\u96c6\u6765\u786e\u4fdd\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5c3d\u7ba1\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u6ee1\u8db3\u6709\u6548\u7684\u4fdd\u8bc1\uff0c\u4f46\u4e8c\u8005\u5728\u5c42\u6b21\u7ea7\u6548\u7387\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u7684\u6743\u8861\u3002", "conclusion": "\u4e24\u79cd\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u5e94\u7528\u573a\u666f\u4e2d\u5404\u6709\u4f18\u52bf\uff0c\u7814\u7a76\u5c55\u793a\u4e86 CP \u5728 OS \u6307\u7eb9\u8bc6\u522b\u4e2d\u7684\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8981\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u7387\u548c\u4e00\u81f4\u6027\u4ee5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002"}}
{"id": "2602.12943", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.12943", "abs": "https://arxiv.org/abs/2602.12943", "authors": ["Osama Zafar", "Shaojie Zhan", "Tianxi Ji", "Erman Ayday"], "title": "Neighborhood Blending: A Lightweight Inference-Time Defense Against Membership Inference Attacks", "comment": null, "summary": "In recent years, the widespread adoption of Machine Learning as a Service (MLaaS), particularly in sensitive environments, has raised considerable privacy concerns. Of particular importance are membership inference attacks (MIAs), which exploit behavioral discrepancies between training and non-training data to determine whether a specific record was included in the model's training set, thereby presenting significant privacy risks. Although existing defenses, such as adversarial regularization, DP-SGD, and MemGuard, assist in mitigating these threats, they often entail trade-offs such as compromising utility, increased computational requirements, or inconsistent protection against diverse attack vectors.\n  In this paper, we introduce a novel inference-time defense mechanism called Neighborhood Blending, which mitigates MIAs without retraining the model or incurring significant computational overhead. Our approach operates post-training by smoothing the model's confidence outputs based on the neighborhood of a queried sample. By averaging predictions from similar training samples selected using differentially private sampling, our method establishes a consistent confidence pattern, rendering members and non-members indistinguishable to an adversary while maintaining high utility. Significantly, Neighborhood Blending maintains label integrity (zero label loss) and ensures high utility through an adaptive, \"pay-as-you-go\" distortion strategy. It is a model-agnostic approach that offers a practical, lightweight solution that enhances privacy without sacrificing model utility. Through extensive experiments across diverse datasets and models, we demonstrate that our defense significantly reduces MIA success rates while preserving model performance, outperforming existing post-hoc defenses like MemGuard and training-time techniques like DP-SGD in terms of utility retention.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12544", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12544", "abs": "https://arxiv.org/abs/2602.12544", "authors": ["Lajanugen Logeswaran", "Jaekyeom Kim", "Sungryull Sohn", "Creighton Glasscock", "Honglak Lee"], "title": "Scaling Web Agent Training through Automatic Data Generation and Fine-grained Evaluation", "comment": "COLM 2025", "summary": "We present a scalable pipeline for automatically generating high-quality training data for web agents. In particular, a major challenge in identifying high-quality training instances is trajectory evaluation - quantifying how much progress was made towards task completion. We introduce a novel constraint-based evaluation framework that provides fine-grained assessment of progress towards task completion. This enables us to leverage partially successful trajectories, which significantly expands the amount of usable training data. We evaluate our method on a new benchmark we propose called BookingArena, which consists of complex booking tasks across 20 popular websites, and demonstrate that our distilled student model outperforms open-source approaches and matches or exceeds commercial systems, while being a significantly smaller model. Our work addresses the challenge of efficiently creating diverse, realistic web interaction datasets and provides a systematic evaluation methodology for complex structured web tasks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12967", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.12967", "abs": "https://arxiv.org/abs/2602.12967", "authors": ["Sebastian M\u00f6dersheim", "Simon Lund", "Alessandro Bruni", "Marco Carbone", "Rosario Giustolisi"], "title": "Cryptographic Choreographies", "comment": null, "summary": "We present CryptoChoreo, a choreography language for the specification of cryptographic protocols. Choreographies can be regarded as an extension of Alice-and-Bob notation, providing an intuitive high-level view of the protocol as a whole (rather than specifying each protocol role in isolation). The extensions over standard Alice-and-Bob notation that we consider are non-deterministic choice, conditional branching, and mutable long-term memory. We define the semantics of CryptoChoreo by translation to a process calculus. This semantics entails an understanding of the protocol: it determines how agents parse and check incoming messages and how they construct outgoing messages, in the presence of an arbitrary algebraic theory and non-deterministic choices made by other agents. While this semantics entails algebraic problems that are in general undecidable, we give an implementation for a representative theory. We connect this translation to ProVerif and show on a number of case studies that the approach is practically feasible.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12566", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12566", "abs": "https://arxiv.org/abs/2602.12566", "authors": ["Haoqing Wang", "Xiang Long", "Ziheng Li", "Yilong Xu", "Tingguang Li", "Yehui Tang"], "title": "To Mix or To Merge: Toward Multi-Domain Reinforcement Learning for Large Language Models", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) plays a key role in stimulating the explicit reasoning capability of Large Language Models (LLMs). We can achieve expert-level performance in some specific domains via RLVR, such as coding or math. When a general multi-domain expert-level model is required, we need to carefully consider the collaboration of RLVR across different domains. The current state-of-the-art models mainly employ two different training paradigms for multi-domain RLVR: mixed multi-task RLVR and separate RLVR followed by model merging. However, most of the works did not provide a detailed comparison and analysis about these paradigms. To this end, we choose multiple commonly used high-level tasks (e.g., math, coding, science, and instruction following) as our target domains and design extensive qualitative and quantitative experiments using open-source datasets. We find the RLVR across domains exhibits few mutual interferences, and reasoning-intensive domains demonstrate mutually synergistic effects. Furthermore, we analyze the internal mechanisms of mutual gains from the perspectives of weight space geometry, model prediction behavior, and information constraints. This project is named as M2RL that means Mixed multi-task training or separate training followed by model Merging for Reinforcement Learning, and the homepage is at https://github.com/mosAI25/M2RL", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.13148", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.13148", "abs": "https://arxiv.org/abs/2602.13148", "authors": ["Parsa Sadri Sinaki", "Zainab Ahmad", "Wentao Xie", "Merlijn Sebrechts", "Jimmy Kj\u00e4llman", "Lachlan J. Gunn"], "title": "TrustMee: Self-Verifying Remote Attestation Evidence", "comment": "17 pages, 12 figures", "summary": "Hardware-secured remote attestation is essential to establishing trust in the integrity of confidential virtual machines (cVMs), but is difficult to use in practice because verifying attestation evidence requires the use of hardware-specific cryptographic logic. This increases both maintenance costs and the verifiers' trusted computing base. We introduce the concept of self-verifying remote attestation evidence. Each attestation bundle includes verification logic as a WebAssembly component signed by a trusted party. This approach transforms evidence verification into a standard code-signing problem: the verifier checks the signature on the embedded logic and then executes it to validate the evidence. As a result, verifiers can validate attestation evidence without any platform-specific knowledge. We implement this concept as TrustMee, a platform-agnostic verification driver for the Trustee framework. We demonstrate its functionality with self-verifying evidence for AMD SEV-SNP and Intel TDX attestations, producing attestation claims in the standard EAT Attestation Result (EAR) format.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12586", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12586", "abs": "https://arxiv.org/abs/2602.12586", "authors": ["Joshua Ong Jun Leang", "Yu Zhao", "Mihaela C\u0103t\u0103lina Stoian", "Wenda Li", "Shay B. Cohen", "Eleonora Giunchiglia"], "title": "Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models", "comment": "8 pages, preprint", "summary": "While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.13156", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13156", "abs": "https://arxiv.org/abs/2602.13156", "authors": ["Yiran Gao", "Kim Hammar", "Tao Li"], "title": "In-Context Autonomous Network Incident Response: An End-to-End Large Language Model Agent Approach", "comment": "2026 AAAI Summer Symposium on Human-Aware AI Agents for the Cyber Battlefield", "summary": "Rapidly evolving cyberattacks demand incident response systems that can autonomously learn and adapt to changing threats. Prior work has extensively explored the reinforcement learning approach, which involves learning response strategies through extensive simulation of the incident. While this approach can be effective, it requires handcrafted modeling of the simulator and suppresses useful semantics from raw system logs and alerts. To address these limitations, we propose to leverage large language models' (LLM) pre-trained security knowledge and in-context learning to create an end-to-end agentic solution for incident response planning. Specifically, our agent integrates four functionalities, perception, reasoning, planning, and action, into one lightweight LLM (14b model). Through fine-tuning and chain-of-thought reasoning, our LLM agent is capable of processing system logs and inferring the underlying network state (perception), updating its conjecture of attack models (reasoning), simulating consequences under different response strategies (planning), and generating an effective response (action). By comparing LLM-simulated outcomes with actual observations, the LLM agent repeatedly refines its attack conjecture and corresponding response, thereby demonstrating in-context adaptation. Our agentic approach is free of modeling and can run on commodity hardware. When evaluated on incident logs reported in the literature, our agent achieves recovery up to 23% faster than those of frontier LLMs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12617", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12617", "abs": "https://arxiv.org/abs/2602.12617", "authors": ["Modi Jin", "Yiming Zhang", "Boyuan Sun", "Dingwen Zhang", "MingMing Cheng", "Qibin Hou"], "title": "GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics", "comment": null, "summary": "This paper presents GeoAgent, a model capable of reasoning closely with humans and deriving fine-grained address conclusions. Previous RL-based methods have achieved breakthroughs in performance and interpretability but still remain concerns because of their reliance on AI-generated chain-of-thought (CoT) data and training strategies, which conflict with geographic characteristics. To address these issues, we first introduce GeoSeek, a new geolocation dataset comprising CoT data annotated by geographic experts and professional players. We further thoroughly explore the inherent characteristics of geographic tasks and propose a geo-similarity reward and a consistency reward assessed by a consistency agent to assist training. This encourages the model to converge towards correct answers from a geographic perspective while ensuring the integrity and consistency of its reasoning process. Experimental results show that GeoAgent outperforms existing methods and a series of general VLLMs across multiple grains, while generating reasoning that closely aligns with humans.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12631", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12631", "abs": "https://arxiv.org/abs/2602.12631", "authors": ["Jackie Baek", "Yaopeng Fu", "Will Ma", "Tianyi Peng"], "title": "AI Agents for Inventory Control: Human-LLM-OR Complementarity", "comment": null, "summary": "Inventory control is a fundamental operations problem in which ordering decisions are traditionally guided by theoretically grounded operations research (OR) algorithms. However, such algorithms often rely on rigid modeling assumptions and can perform poorly when demand distributions shift or relevant contextual information is unavailable. Recent advances in large language models (LLMs) have generated interest in AI agents that can reason flexibly and incorporate rich contextual signals, but it remains unclear how best to incorporate LLM-based methods into traditional decision-making pipelines.\n  We study how OR algorithms, LLMs, and humans can interact and complement each other in a multi-period inventory control setting. We construct InventoryBench, a benchmark of over 1,000 inventory instances spanning both synthetic and real-world demand data, designed to stress-test decision rules under demand shifts, seasonality, and uncertain lead times. Through this benchmark, we find that OR-augmented LLM methods outperform either method in isolation, suggesting that these methods are complementary rather than substitutes.\n  We further investigate the role of humans through a controlled classroom experiment that embeds LLM recommendations into a human-in-the-loop decision pipeline. Contrary to prior findings that human-AI collaboration can degrade performance, we show that, on average, human-AI teams achieve higher profits than either humans or AI agents operating alone. Beyond this population-level finding, we formalize an individual-level complementarity effect and derive a distribution-free lower bound on the fraction of individuals who benefit from AI collaboration; empirically, we find this fraction to be substantial.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12662", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12662", "abs": "https://arxiv.org/abs/2602.12662", "authors": ["Ruihan Yang", "Fanghua Ye", "Xiang We", "Ruoqing Zhao", "Kang Luo", "Xinbo Xu", "Bo Zhao", "Ruotian Ma", "Shanyi Wang", "Zhaopeng Tu", "Xiaolong Li", "Deqing Yang", "Linus"], "title": "Think Fast and Slow: Step-Level Cognitive Depth Adaptation for LLM Agents", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as autonomous agents for multi-turn decision-making tasks. However, current agents typically rely on fixed cognitive patterns: non-thinking models generate immediate responses, while thinking models engage in deep reasoning uniformly. This rigidity is inefficient for long-horizon tasks, where cognitive demands vary significantly from step to step, with some requiring strategic planning and others only routine execution. In this paper, we introduce CogRouter, a framework that trains agents to dynamically adapt cognitive depth at each step. Grounded in ACT-R theory, we design four hierarchical cognitive levels ranging from instinctive responses to strategic planning. Our two-stage training approach includes Cognition-aware Supervised Fine-tuning (CoSFT) to instill stable level-specific patterns, and Cognition-aware Policy Optimization (CoPO) for step-level credit assignment via confidence-aware advantage reweighting. The key insight is that appropriate cognitive depth should maximize the confidence of the resulting action. Experiments on ALFWorld and ScienceWorld demonstrate that CogRouter achieves state-of-the-art performance with superior efficiency. With Qwen2.5-7B, it reaches an 82.3% success rate, outperforming GPT-4o (+40.3%), OpenAI-o3 (+18.3%), and GRPO (+14.0%), while using 62% fewer tokens.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12665", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12665", "abs": "https://arxiv.org/abs/2602.12665", "authors": ["Na\u00efm Es-sebbani", "Esteban Marquer", "Yakoub Salhi", "Zied Bouraoui"], "title": "Evaluating Robustness of Reasoning Models on Parameterized Logical Problems", "comment": null, "summary": "Logic provides a controlled testbed for evaluating LLM-based reasoners, yet standard SAT-style benchmarks often conflate surface difficulty (length, wording, clause order) with the structural phenomena that actually determine satisfiability. We introduce a diagnostic benchmark for 2-SAT built from parameterized families of structured 2--CNF formulas, where satisfiability is characterized by the implication graph and can be tuned along interpretable axes. Our generators isolate distinct competencies and failure modes: (i) contradiction-cycle UNSAT cores with controllable size and imbalance, (ii) SAT instances with a prescribed fraction of free variables to control solution multiplicity, (iii) planted backbones that modulate propagation, (iv) late bridge clauses that couple otherwise monotone regions to probe sensitivity to ordering and revision, and (v) symmetry/duplication variants that test abstraction under renaming and redundant structure. We evaluate LLM-based reasoners on decision accuracy and assignment validity, and quantify robustness under semantics-preserving perturbations such as clause reordering, filler clauses, and variable renaming. Across models, we observe sharp performance transitions under targeted structural interventions even when surface statistics are held fixed, revealing brittleness regimes that are invisible to aggregate SAT accuracy.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12670", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12670", "abs": "https://arxiv.org/abs/2602.12670", "authors": ["Xiangyi Li", "Wenbo Chen", "Yimin Liu", "Shenghan Zheng", "Xiaokun Chen", "Yifeng He", "Yubo Li", "Bingran You", "Haotian Shen", "Jiankai Sun", "Shuyi Wang", "Qunhong Zeng", "Di Wang", "Xuandong Zhao", "Yuanli Wang", "Roey Ben Chaim", "Zonglin Di", "Yipeng Gao", "Junwei He", "Yizhuo He", "Liqiang Jing", "Luyang Kong", "Xin Lan", "Jiachen Li", "Songlin Li", "Yijiang Li", "Yueqian Lin", "Xinyi Liu", "Xuanqing Liu", "Haoran Lyu", "Ze Ma", "Bowei Wang", "Runhui Wang", "Tianyu Wang", "Wengao Ye", "Yue Zhang", "Hanwen Xing", "Yiqi Xue", "Steven Dillmann", "Han-chung Lee"], "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks", "comment": null, "summary": "Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12748", "categories": ["cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.12748", "abs": "https://arxiv.org/abs/2602.12748", "authors": ["Tobias Labarta", "Nhi Hoang", "Maximilian Dreyer", "Jim Berend", "Oleg Hein", "Jackie Ma", "Wojciech Samek", "Sebastian Lapuschkin"], "title": "X-SYS: A Reference Architecture for Interactive Explanation Systems", "comment": "18 pages, 8 figures", "summary": "The explainable AI (XAI) research community has proposed numerous technical methods, yet deploying explainability as systems remains challenging: Interactive explanation systems require both suitable algorithms and system capabilities that maintain explanation usability across repeated queries, evolving models and data, and governance constraints. We argue that operationalizing XAI requires treating explainability as an information systems problem where user interaction demands induce specific system requirements. We introduce X-SYS, a reference architecture for interactive explanation systems, that guides (X)AI researchers, developers and practitioners in connecting interactive explanation user interfaces (XUI) with system capabilities. X-SYS organizes around four quality attributes named STAR (scalability, traceability, responsiveness, and adaptability), and specifies a five-component decomposition (XUI Services, Explanation Services, Model Services, Data Services, Orchestration and Governance). It maps interaction patterns to system capabilities to decouple user interface evolution from backend computation. We implement X-SYS through SemanticLens, a system for semantic search and activation steering in vision-language models. SemanticLens demonstrates how contract-based service boundaries enable independent evolution, offline/online separation ensures responsiveness, and persistent state management supports traceability. Together, this work provides a reusable blueprint and concrete instantiation for interactive explanation systems supporting end-to-end design under operational constraints.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12852", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12852", "abs": "https://arxiv.org/abs/2602.12852", "authors": ["Junjie Wang", "Zequn Xie", "Dan Yang", "Jie Feng", "Yue Shen", "Duolin Sun", "Meixiu Long", "Yihan Jiao", "Zhehao Tan", "Jian Wang", "Peng Wei", "Jinjie Gu"], "title": "WebClipper: Efficient Evolution of Web Agents with Graph-based Trajectory Pruning", "comment": "Work in Progress", "summary": "Deep Research systems based on web agents have shown strong potential in solving complex information-seeking tasks, yet their search efficiency remains underexplored. We observe that many state-of-the-art open-source web agents rely on long tool-call trajectories with cyclic reasoning loops and exploration of unproductive branches. To address this, we propose WebClipper, a framework that compresses web agent trajectories via graph-based pruning. Concretely, we model the agent's search process as a state graph and cast trajectory optimization as a minimum-necessary Directed Acyclic Graph (DAG) mining problem, yielding pruned trajectories that preserve essential reasoning while eliminating redundant steps. Continued training on these refined trajectories enables the agent to evolve toward more efficient search patterns and reduces tool-call rounds by about 20% while improving accuracy. Furthermore, we introduce a new metric called F-AE Score to measure the model's overall performance in balancing accuracy and efficiency. Experiments demonstrate that WebClipper compresses tool-call rounds under excellent performance, providing practical insight into balancing effectiveness and efficiency in web agent design.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12876", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12876", "abs": "https://arxiv.org/abs/2602.12876", "authors": ["Huanyao Zhang", "Jiepeng Zhou", "Bo Li", "Bowen Zhou", "Yanzhe Dan", "Haishan Lu", "Zhiyong Cao", "Jiaoyang Chen", "Yuqian Han", "Zinan Sheng", "Zhengwei Tao", "Hao Liang", "Jialong Wu", "Yang Shi", "Yuanpeng He", "Jiaye Lin", "Qintong Zhang", "Guochen Yan", "Runhao Zhao", "Zhengpin Li", "Xiaohan Yu", "Lang Mei", "Chong Chen", "Wentao Zhang", "Bin Cui"], "title": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents", "comment": null, "summary": "Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-$V^3$, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception. Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12963", "abs": "https://arxiv.org/abs/2602.12963", "authors": ["Alfred Harwood", "Jose Faustino", "Alex Altair"], "title": "Information-theoretic analysis of world models in optimal reward maximizers", "comment": "28 pages, 0 figures. Not submitted to any conference yet", "summary": "An important question in the field of AI is the extent to which successful behaviour requires an internal representation of the world. In this work, we quantify the amount of information an optimal policy provides about the underlying environment. We consider a Controlled Markov Process (CMP) with $n$ states and $m$ actions, assuming a uniform prior over the space of possible transition dynamics. We prove that observing a deterministic policy that is optimal for any non-constant reward function then conveys exactly $n \\log m$ bits of information about the environment. Specifically, we show that the mutual information between the environment and the optimal policy is $n \\log m$ bits. This bound holds across a broad class of objectives, including finite-horizon, infinite-horizon discounted, and time-averaged reward maximization. These findings provide a precise information-theoretic lower bound on the \"implicit world model'' necessary for optimality.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.13093", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13093", "abs": "https://arxiv.org/abs/2602.13093", "authors": ["Yubo Li", "Ramayya Krishnan", "Rema Padman"], "title": "Consistency of Large Reasoning Models Under Multi-Turn Attacks", "comment": null, "summary": "Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.13135", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.13135", "abs": "https://arxiv.org/abs/2602.13135", "authors": ["Emanuele De Angelis", "Fabio Fioravanti", "Maria Chiara Meo", "Alberto Pettorossi", "Maurizio Proietti", "Francesca Toni"], "title": "Constrained Assumption-Based Argumentation Frameworks", "comment": "Extended version with proofs and additional results of the full paper accepted at the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026). DOI: https://doi.org/10.65109/KRAP9309", "summary": "Assumption-based Argumentation (ABA) is a well-established form of structured argumentation. ABA frameworks with an underlying atomic language are widely studied, but their applicability is limited by a representational restriction to ground (variable-free) arguments and attacks built from propositional atoms. In this paper, we lift this restriction and propose a novel notion of constrained ABA (CABA), whose components, as well as arguments built from them, may include constrained variables, ranging over possibly infinite domains. We define non-ground semantics for CABA, in terms of various notions of non-ground attacks. We show that the new semantics conservatively generalise standard ABA semantics.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.13166", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13166", "abs": "https://arxiv.org/abs/2602.13166", "authors": ["Hugo Henry", "Arthur Tsai", "Kelly Cohen"], "title": "Optimal Take-off under Fuzzy Clearances", "comment": "12 pages, 12 figures, conference paper", "summary": "This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
