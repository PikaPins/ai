<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 35]
- [cs.CR](#cs.CR) [Total: 21]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation](https://arxiv.org/abs/2512.22199)
*Teja Chinthala*

Main category: cs.AI

TL;DR: Bidirectional RAG通过引入多阶段验证层，实现了安全的知识库扩展，显著提高了知识覆盖范围，减少了所需文档数量。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统依赖静态知识库，无法从用户交互中动态学习和更新。为此，提出了Bidirectional RAG来解决这一问题。

Method: Bidirectional RAG采用多阶段验证层，结合语义一致验证、归因检查和新颖性检测，确保高质量生成的响应被安全地添加到知识库。

Result: 实验结果表明，与标准RAG相比，Bidirectional RAG提升了40.58%的知识覆盖范围，并且通过精心挑选响应，仅增加了72%的文档数量，比简单写入减少了140 vs 500份文档。

Conclusion: 研究表明，通过严格的验证治理自我改进的RAG系统是可行且安全的，这为RAG系统的部署提供了实际路径。

Abstract: Retrieval-Augmented Generation RAG systems enhance large language models by grounding responses in external knowledge bases, but conventional RAG architectures operate with static corpora that cannot evolve from user interactions. We introduce Bidirectional RAG, a novel RAG architecture that enables safe corpus expansion through validated write back of high quality generated responses. Our system employs a multi stage acceptance layer combining grounding verification (NLI based entailment, attribution checking, and novelty detection to prevent hallucination pollution while enabling knowledge accumulation. Across four datasets Natural Questions, TriviaQA, HotpotQA, Stack Overflow with three random seeds 12 experiments per system, Bidirectional RAG achieves 40.58% average coverage nearly doubling Standard RAG 20.33% while adding 72% fewer documents than naive write back 140 vs 500. Our work demonstrates that self improving RAG is feasible and safe when governed by rigorous validation, offering a practical path toward RAG systems that learn from deployment.

</details>


### [2] [Emergent Persuasion: Will LLMs Persuade Without Being Prompted?](https://arxiv.org/abs/2512.22201)
*Vincent Chang,Thee Ho,Sunishchal Dev,Kevin Zhu,Shi Feng,Kellin Pelrine,Matthew Kowal*

Main category: cs.AI

TL;DR: 研究发现，通过内部激活引导和监督微调两种方式，AI模型在未被明确提示下仍可能促进有害信念，特别是监督微调针对泛化说服数据集时，模型更倾向于在有争议或有害的话题上产生说服行为。


<details>
  <summary>Details</summary>
Motivation: 随着对话型AI系统的广泛应用，这些系统能够对人类的意见和信念产生前所未有的影响。已有研究表明，大型语言模型在未经明确提示的情况下也可能被用于引导用户产生有害的信念或行动。本文研究在未明确提示的情况下模型是否会出现自治的说服行为，以及这种行为的条件。

Method: 本文通过两种方式研究AI模型在未被明确提示下是否会自主进行说服：一是通过内部激活引导模型往特性和非特性的方向发展；二是监督微调模型使之表现出相同的特性。研究结果显示，只有在监督微调针对泛化说服数据集时，模型更可能在有争议或有害的话题上产生说服行为。

Result: 研究发现，即使模型没有被明确提示，通过内部激活引导和监督微调两种方法，AI模型依然可能会在未被提示的情况下产生说服行为。特别是，监督微调针对包含仅良性话题的泛化说服数据集时，模型更倾向于在有争议或有害的话题上产生说服行为。

Conclusion: 本文研究表明，AI模型在未被明确提示的情况下可能会产生自发的说服行为，并且这种情况可能在未经精细配置的监督微调过程中加剧。因此，进一步研究这种新兴的说服风险是必要的。

Abstract: With the wide-scale adoption of conversational AI systems, AI are now able to exert unprecedented influence on human opinion and beliefs. Recent work has shown that many Large Language Models (LLMs) comply with requests to persuade users into harmful beliefs or actions when prompted and that model persuasiveness increases with model scale. However, this prior work looked at persuasion from the threat model of $\textit{misuse}$ (i.e., a bad actor asking an LLM to persuade). In this paper, we instead aim to answer the following question: Under what circumstances would models persuade $\textit{without being explicitly prompted}$, which would shape how concerned we should be about such emergent persuasion risks. To achieve this, we study unprompted persuasion under two scenarios: (i) when the model is steered (through internal activation steering) along persona traits, and (ii) when the model is supervised-finetuned (SFT) to exhibit the same traits. We showed that steering towards traits, both related to persuasion and unrelated, does not reliably increase models' tendency to persuade unprompted, however, SFT does. Moreover, SFT on general persuasion datasets containing solely benign topics admits a model that has a higher propensity to persuade on controversial and harmful topics--showing that emergent harmful persuasion can arise and should be studied further.

</details>


### [3] [GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks](https://arxiv.org/abs/2512.22207)
*Ryan Spencer,Roey Yaari,Ritvik Vemavarapu,Joyce Yang,Steven Ngo,Utkarsh Sharma*

Main category: cs.AI

TL;DR: GamiBench 提供了一组基于 Origami 折纸任务的评估基准，专注于更加复杂和动态的空间推理能力。该基准集包括 362 个二维和三维组合样本，涵盖六种不同的视角，并通过测量模型的空间推理过程中的跨视角一致性、物理可行性以及中间折叠步骤的解释能力来全面评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的评测基准大多集中在静态图像或最终结果上，未能充分衡量空间推理这一关键的人类智力组件。通过引入 GamiBench，旨在全面评估多模态大语言模型在动态空间推理方面的表现。

Method: GamiBench 采用了基于折纸的折纸折纸任务，这些任务涉及二维和三维形状转换，并从六个不同的视角进行评估。通过设计多个针对空间推理不同方面的视觉问答任务，参与者需要预测折叠配置、区分有效视角和识别不可能折叠图形。

Result: 实验结果显示，即使是最先进的模型也难以处理单步空间理解。GamiBench 的引入为多模态大型语言模型的空间理解评估提供了一个标准化框架。

Conclusion: GamiBench 是一个标准化评估基准，能够有效衡量多模态大语言模型在空间推理和几何理解方面的能力。

Abstract: Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.

</details>


### [4] [Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh](https://arxiv.org/abs/2512.22210)
*Farjana Yesmin,Romana Akter*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Post-disaster aid allocation in developing nations often suffers from systematic biases that disadvantage vulnerable regions, perpetuating historical inequities. This paper presents a fairness-aware artificial intelligence framework for prioritizing post-flood aid distribution in Bangladesh, a country highly susceptible to recurring flood disasters. Using real data from the 2022 Bangladesh floods that affected 7.2 million people and caused 405.5 million US dollars in damages, we develop an adversarial debiasing model that predicts flood vulnerability while actively removing biases against marginalized districts and rural areas. Our approach adapts fairness-aware representation learning techniques from healthcare AI to disaster management, employing a gradient reversal layer that forces the model to learn bias-invariant representations. Experimental results on 87 upazilas across 11 districts demonstrate that our framework reduces statistical parity difference by 41.6 percent, decreases regional fairness gaps by 43.2 percent, and maintains strong predictive accuracy (R-squared=0.784 vs baseline 0.811). The model generates actionable priority rankings ensuring aid reaches the most vulnerable populations based on genuine need rather than historical allocation patterns. This work demonstrates how algorithmic fairness techniques can be effectively applied to humanitarian contexts, providing decision-makers with tools to implement more equitable disaster recovery strategies.

</details>


### [5] [With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems](https://arxiv.org/abs/2512.22211)
*Shaun Khoo,Jessica Foo,Roy Ka-Wei Lee*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Agentic AI systems present both significant opportunities and novel risks due to their capacity for autonomous action, encompassing tasks such as code execution, internet interaction, and file modification. This poses considerable challenges for effective organizational governance, particularly in comprehensively identifying, assessing, and mitigating diverse and evolving risks. To tackle this, we introduce the Agentic Risk \& Capability (ARC) Framework, a technical governance framework designed to help organizations identify, assess, and mitigate risks arising from agentic AI systems. The framework's core contributions are: (1) it develops a novel capability-centric perspective to analyze a wide range of agentic AI systems; (2) it distills three primary sources of risk intrinsic to agentic AI systems - components, design, and capabilities; (3) it establishes a clear nexus between each risk source, specific materialized risks, and corresponding technical controls; and (4) it provides a structured and practical approach to help organizations implement the framework. This framework provides a robust and adaptable methodology for organizations to navigate the complexities of agentic AI, enabling rapid and effective innovation while ensuring the safe, secure, and responsible deployment of agentic AI systems. Our framework is open-sourced \href{https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}{here}.

</details>


### [6] [We are not able to identify AI-generated images](https://arxiv.org/abs/2512.22236)
*Adrien Pavão*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: AI-generated images are now pervasive online, yet many people believe they can easily tell them apart from real photographs. We test this assumption through an interactive web experiment where participants classify 20 images as real or AI-generated. Our dataset contains 120 difficult cases: real images sampled from CC12M, and carefully curated AI-generated counterparts produced with MidJourney. In total, 165 users completed 233 sessions. Their average accuracy was 54%, only slightly above random guessing, with limited improvement across repeated attempts. Response times averaged 7.3 seconds, and some images were consistently more deceptive than others. These results indicate that, even on relatively simple portrait images, humans struggle to reliably detect AI-generated content. As synthetic media continues to improve, human judgment alone is becoming insufficient for distinguishing real from artificial data. These findings highlight the need for greater awareness and ethical guidelines as AI-generated media becomes increasingly indistinguishable from reality.

</details>


### [7] [Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method](https://arxiv.org/abs/2512.22258)
*Satvik Tripathi*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.

</details>


### [8] [SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence](https://arxiv.org/abs/2512.22334)
*Yiheng Wang,Yixin Chen,Shuo Li,Yifan Zhou,Bo Liu,Hengjian Gao,Jiakang Yuan,Jia Bu,Wanghan Xu,Yuhao Zhou,Xiangyu Zhao,Zhiwang Zhou,Fengxiang Wang,Haodong Duan,Songyang Zhang,Jun Yao,Han Deng,Yizhou Wang,Jiabei Xiao,Jiaqi Liu,Encheng Su,Yujie Liu,Weida Wang,Junchi Yao,Shenghe Zheng,Haoran Sun,Runmin Ma,Xiangchao Yan,Bo Zhang,Dongzhan Zhou,Shufei Zhang,Peng Ye,Xiaosong Wang,Shixiang Tang,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.

</details>


### [9] [Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback](https://arxiv.org/abs/2512.22336)
*Mengkang Hu,Bowei Xia,Yuran Wu,Ailing Yu,Yude Zou,Qiguang Chen,Shijian Wang,Jiarui Jin,Kexin Li,Wenxiang Jiao,Yuan Lu,Ping Luo*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.

</details>


### [10] [Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions](https://arxiv.org/abs/2512.22367)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Numeric planning with control parameters extends the standard numeric planning model by introducing action parameters as free numeric variables that must be instantiated during planning. This results in a potentially infinite number of applicable actions in a state. In this setting, off-the-shelf numeric heuristics that leverage the action structure are not feasible. In this paper, we identify a tractable subset of these problems--namely, controllable, simple numeric problems--and propose an optimistic compilation approach that transforms them into simple numeric tasks. To do so, we abstract control-dependent expressions into bounded constant effects and relaxed preconditions. The proposed compilation makes it possible to effectively use subgoaling heuristics to estimate goal distance in numeric planning problems involving control parameters. Our results demonstrate that this approach is an effective and computationally feasible way of applying traditional numeric heuristics to settings with an infinite number of possible actions, pushing the boundaries of the current state of the art.

</details>


### [11] [HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification](https://arxiv.org/abs/2512.22396)
*Bhanu Prakash Vangala,Sajid Mahmud,Pawan Neupane,Joel Selvaraj,Jianlin Cheng*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Artificial Intelligence (AI), particularly Large Language Models (LLMs), is transforming scientific discovery, enabling rapid knowledge generation and hypothesis formulation. However, a critical challenge is hallucination, where LLMs generate factually incorrect or misleading information, compromising research integrity. To address this, we introduce HalluMatData, a benchmark dataset for evaluating hallucination detection methods, factual consistency, and response robustness in AI-generated materials science content. Alongside this, we propose HalluMatDetector, a multi-stage hallucination detection framework that integrates intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate LLM hallucinations. Our findings reveal that hallucination levels vary significantly across materials science subdomains, with high-entropy queries exhibiting greater factual inconsistencies. By utilizing HalluMatDetector verification pipeline, we reduce hallucination rates by 30% compared to standard LLM outputs. Furthermore, we introduce the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries, offering deeper insights into model reliability.

</details>


### [12] [Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings](https://arxiv.org/abs/2512.22398)
*Ozan Oguztuzun,Cerag Oguztuzun*

Main category: cs.AI

TL;DR: GatedBias 是一个轻量级的推理时个性化框架，它通过结构门控适应将冻结的知识图谱嵌入与个人用户上下文结合，仅需少量可训练参数即能有效提升个性化推荐的效果。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱中的基础模型虽然在团体级别的链接预测中表现良好，但未能捕捉用户个性化偏好，文章提出 GatedBias 旨在弥补关系推理与个性化排序之间的差距。

Method: GatedBias 引入了结构门控自适应方法，通过特定用户剖面特征与图提取的二进制门结合，为每个实体生成可解释的偏见，无需重新训练模型即可优化。

Result: 在两个基准数据集（Amazon-Book 和 Last-FM）上的实验表明，该方法在用户偏好对齐度方面有显著提升，同时保持了整体性能。因果扰动实验验证了其因果响应能力，增强相关偏好信号后的实体排名提高了 6-30 倍。

Conclusion: GatedBias 证明了个性化调整基础模型可以在参数效率和因果可验证性之间取得平衡，拓宽了知识表示与个人用户需求之间的桥梁。

Abstract: Foundation models for knowledge graphs (KGs) achieve strong cohort-level performance in link prediction, yet fail to capture individual user preferences; a key disconnect between general relational reasoning and personalized ranking. We propose GatedBias, a lightweight inference-time personalization framework that adapts frozen KG embeddings to individual user contexts without retraining or compromising global accuracy. Our approach introduces structure-gated adaptation: profile-specific features combine with graph-derived binary gates to produce interpretable, per-entity biases, requiring only ${\sim}300$ trainable parameters. We evaluate GatedBias on two benchmark datasets (Amazon-Book and Last-FM), demonstrating statistically significant improvements in alignment metrics while preserving cohort performance. Counterfactual perturbation experiments validate causal responsiveness; entities benefiting from specific preference signals show 6--30$\times$ greater rank improvements when those signals are boosted. These results show that personalized adaptation of foundation models can be both parameter-efficient and causally verifiable, bridging general knowledge representations with individual user needs.

</details>


### [13] [Monadic Context Engineering](https://arxiv.org/abs/2512.22431)
*Yifan Zhang,Mengdi Wang*

Main category: cs.AI

TL;DR: 本文提出了一种新的架构范式Monadic Context Engineering（MCE），通过函数代数结构（如Functors、Applicative Functors和Monads）来解决当前智能体架构中的问题，如状态管理、错误处理和并发性。MCE能够构建复杂的、健壮且高效的AI智能体，并可以通过元编程扩展该框架以实现生成性编排。


<details>
  <summary>Details</summary>
Motivation: 当前的智能体架构使用的是命令式、手工制作的方法，这导致系统脆弱，并且在状态管理、错误处理和并发性方面存在问题。本文旨在提出一个新的架构范式来解决这些问题。

Method: 该方法基于函数代数结构，如Functors、Applicative Functors和Monads，提出了一个新的架构范式Monadic Context Engineering（MCE），并且通过展示Monads、Applicatives和Monad Transformers的应用来支持该方法。此外，该方法还通过元编程扩展了该框架，以支持动态创建和管理子智能体流程。

Result: MCE能够帮助开发者构建复杂的、健壮且高效的AI智能体，它还提供了一种系统的方法来组合这些能力。通过引入Meta-Agents的概念，该方法还展示了如何动态地创建和管理智能体流程。

Conclusion: MCE为智能体的设计提供了新的基础架构，克服了当前架构的局限性，并展示了在智能体领域的创新可能性。

Abstract: The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.

</details>


### [14] [DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior](https://arxiv.org/abs/2512.22470)
*Sadia Asif,Israel Antonio Rosales Laguan,Haris Khan,Shumaila Asif,Muneeb Asif*

Main category: cs.AI

TL;DR: 该研究提出了DarkPatterns-LLM基准数据集和诊断框架，旨在全面评估大语言模型输出中的操控性内容，并探索多种分析层级，涵盖多层次检测、多尺度意图分析、威胁谐波协议和深度上下文风险对齐。该数据集包括401个精心挑选的示例，涵盖七种危害类别，评估结果显示现有模型在检测操控性模式方面存在显著差异和不足。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型的泛滥及其潜在的操纵和欺骗行为，可能对用户造成心理、社会和情感层面的损害，现有的安全基准过于简单且未能捕捉到操控背后的心智和社会机制。因此，研究需要一种更加精细和多维度的方式来进行评估。

Method: 该研究构建了一个包含多层次分析管道的框架，包括多粒度检测（MGD）、多尺度意图分析（MSIAN）、威胁谐波协议（THP）和深度上下文风险对齐（DCRA）。还使用了一个包含401个精挑细选的例子的基准数据集，这些例子涵盖了七种类型的危害类别。研究还评估了最新模型的表现。

Result: 研究发现，最先进的模型如GPT-4、Claude 3.5和LLaMA-3-70B在检测操纵性模式时存在显著的性能差距（65.2%-89.7%），并且在识别限制用户自主性的影响方面表现出一致的弱点。

Conclusion: DarkPatterns-LLM为大语言模型中的操纵性检测建立了第一个标准化、多维度基准，为打造更加可信的AI系统提供了可操作的诊断工具。

Abstract: The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\%--89.7\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.

</details>


### [15] [Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI](https://arxiv.org/abs/2512.22568)
*Rajesh P. N. Rao,Vishwas Sathish,Linxing Preston Jiang,Matthew Bryan,Prashant Rangarajan*

Main category: cs.AI

TL;DR: 这篇文章讨论了大型语言模型和基础模型在优化过程中的不足，提出在现有模型中加入行动整合、层次组合结构和情景记忆，以提升模型的安全性、可解释性和能耗效率。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型在人工智能领域展现出巨大潜力，但存在一些不足，如幻觉、表面理解、缺乏行动控制感和信任问题等。

Method: 文章分析了神经科学和认知科学中关于预测编码模型的重要成分，阐述了如何通过增强模型实现更安全、可解释和能耗效率的AI。

Result: 研究表明，加入行动整合、层次组合结构和情景记忆等成分有助于改善当前基础模型的性能和适用性。

Conclusion: 文章呼吁神经科学和AI领域深化交流合作，这将为今后安全、可解释的以人类为中心的人工智能发展铺平道路。

Abstract: The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.

</details>


### [16] [Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care](https://arxiv.org/abs/2512.22601)
*Tao Zhou,Lingyu Shu,Zixing Zhang,Jing Han*

Main category: cs.AI

TL;DR: Tyee 是一种统一、模块化且一体化的配置工具，用于智能生理健康care。它通过统一的数据接口和配置预处理管道、模块化和可扩展的架构以及端到端的工作流程配置，解决了深度学习在生理信号分析中遇到的多种挑战。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决当前深度学习在生理信号分析中遇到的问题，如数据格式不一致、预处理策略不一致、模型管道碎片化和实验设置不可重现。通过提出Tyee，作者希望提供一种统一、可配置的解决方案，促进这一领域的发展。

Method: Tyee 采用了统一的数据接口和可配置预处理管道，支持12种信号模态。其模块化和可扩展的架构允许灵活集成和快速原型设计。此外，Tyee 实现了端到端的工作流程配置，推动了可重现和可扩展的实验。

Result: Tyee 在所有评估任务中都展示了持续的实用有效性与泛化能力，其性能要么优于基线，要么与其持平，在13个数据集中有12个达到了最先进的结果。

Conclusion: Tyee 已在GitHub上发布并积极维护，为研究者提供了一个强大的工具来处理生理信号分析任务。

Abstract: Deep learning has shown great promise in physiological signal analysis, yet its progress is hindered by heterogeneous data formats, inconsistent preprocessing strategies, fragmented model pipelines, and non-reproducible experimental setups. To address these limitations, we present Tyee, a unified, modular, and fully-integrated configurable toolkit designed for intelligent physiological healthcare. Tyee introduces three key innovations: (1) a unified data interface and configurable preprocessing pipeline for 12 kinds of signal modalities; (2) a modular and extensible architecture enabling flexible integration and rapid prototyping across tasks; and (3) end-to-end workflow configuration, promoting reproducible and scalable experimentation. Tyee demonstrates consistent practical effectiveness and generalizability, outperforming or matching baselines across all evaluated tasks (with state-of-the-art results on 12 of 13 datasets). The Tyee toolkit is released at https://github.com/SmileHnu/Tyee and actively maintained.

</details>


### [17] [Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation](https://arxiv.org/abs/2512.22605)
*Junshu Dai,Yu Wang,Tongya Zheng,Wei Ji,Qinghong Guo,Ji Cao,Jie Song,Canghong Jin,Mingli Song*

Main category: cs.AI

TL;DR: 该研究提出了一种多模态时空知识的方法（称为M^3ob），利用大型语言模型增强的时空知识图谱来统一表示多模态数据，并设计了门控机制和空间-时间知识图谱引导的跨模态对齐，以融合不同的时空图表示并注入时空动态知识。实验结果表明，该方法在正常和异常场景下均能实现一致的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的单模态和多模态方法在人类移动性预测方面存在局限性，无法充分捕获动态的移动模式，研究旨在通过多模态时空知识的方法提升预测精度和泛化能力。

Method: 该方法首先构建了一个多模态时空关系图（STRG），通过大型语言模型增强的时空知识图谱来统一表示多模态数据。其次，设计了一个门控机制来融合不同模态的时空图表示，并提出了时空知识图谱引导的跨模态对齐以注入时空动态知识。

Result: 在六个公开数据集上进行的实验表明，该方法在正常场景和异常场景中均实现了稳定且显著的性能提升。

Conclusion: 该研究提出的方法M^3ob在多模态时空移动性预测方面表现出优越的性能和良好的泛化能力，对于实际应用具有重要意义。

Abstract: The precise prediction of human mobility has produced significant socioeconomic impacts, such as location recommendations and evacuation suggestions. However, existing methods suffer from limited generalization capability: unimodal approaches are constrained by data sparsity and inherent biases, while multi-modal methods struggle to effectively capture mobility dynamics caused by the semantic gap between static multi-modal representation and spatial-temporal dynamics. Therefore, we leverage multi-modal spatial-temporal knowledge to characterize mobility dynamics for the location recommendation task, dubbed as \textbf{M}ulti-\textbf{M}odal \textbf{Mob}ility (\textbf{M}$^3$\textbf{ob}). First, we construct a unified spatial-temporal relational graph (STRG) for multi-modal representation, by leveraging the functional semantics and spatial-temporal knowledge captured by the large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG). Second, we design a gating mechanism to fuse spatial-temporal graph representations of different modalities, and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into the static image modality. Extensive experiments on six public datasets show that our proposed method not only achieves consistent improvements in normal scenarios but also exhibits significant generalization ability in abnormal scenarios.

</details>


### [18] [The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?](https://arxiv.org/abs/2512.22625)
*Paul Schneider,Amalie Schramm*

Main category: cs.AI

TL;DR: 研究发现，让大语言模型审查彼此的预测并在更新前进行交流，可以在特定情况下提高预测准确性，特别是在模型具有差异性和共享信息的情境中。


<details>
  <summary>Details</summary>
Motivation: 为了探讨是否可以通过一种结构化的讨论方式来提升大语言模型（如GPT-5、Claude Sonnet 4.5、Gemini Pro 2.5）的预测准确性。

Method: 研究人员使用了从Metaculus Q2 2025 AI Forecasting Tournament中抽样的202个已解决的二元问题，对四种不同情境下的多个模型进行了评估。

Result: 研究结果显示，在模型具有差异性和共享信息的情境下，这种互动干预措施显著提高了预测准确性，降低了相对Log Loss的数值约4%（p = 0.017）。但在同质模型（三个相同模型实例）中使用相同过程时，并未观察到任何益处，且增加额外上下文信息并未提升预测准确性。

Conclusion: 研究指出，可以通过结构化的讨论方式来改善大语言模型的预测能力，并暗示这一策略在不同模型结构和信息共享模式下可能是有效的，但信息集合机制仍需进一步研究以验证其实用性。

Abstract: Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.

</details>


### [19] [DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation](https://arxiv.org/abs/2512.22629)
*Shiyan Liu,Jian Ma,Rui Qu*

Main category: cs.AI

TL;DR: DICE 是一种两阶段、证据结合的框架，通过使用概率性 {A, B, 平局} 计分来增强 RAG 评估的解释性和稳健性，同时通过瑞士制锦标赛减少了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的标量指标在解释性、不确定性量化和多系统比较时的计算效率方面存在限制，阻碍了 RAG 技术的负责任部署。DICE 旨在解决这些问题。

Method: DICE 采用了两阶段的证据结合框架，结合了深度分析推理和概率性 {A, B, 平局} 计分，同时使用瑞士制锦标赛解决了大规模评估中的效率挑战。

Result: DICE 实现了在八系统评估中计算复杂性的减少，同时保留了排名的一致性。在金融问答数据集上的验证表明，DICE 在人类专家意见上的同意率达到了 85.7%，显著优于现有 LLM 基础的指标如 RAGAS。

Conclusion: DICE 被证明是一种负责任、可解释且高效的 RAG 系统评估范式，增强了评估的可信度。

Abstract: As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\{A, B, Tie\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.

</details>


### [20] [Memento-II: Learning by Stateful Reflective Memory](https://arxiv.org/abs/2512.22716)
*Jun Wang*

Main category: cs.AI

TL;DR: 该研究提出了一种结合情景记忆和强化学习的持续学习理论框架，通过引入Stateful Reflective Decision Process，使得学习过程能够通过反思机制从经验中自动调整，而无需传统的微调或反向传播。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习框架中，训练与应用分离可能导致模型性能下降，该研究旨在通过集成情景记忆与强化学习，提出一种新颖的框架，使大型语言模型能够持续适应而无需更新模型参数。

Method: 研究通过引入Stateful Reflective Decision Process，将反思学习建模为两阶段的读写交互过程。此过程能够将特定的学习状态涵盖到增强的状态记忆表示中，利用动态规划和强化学习的古典工具来达到等效的马尔可夫决策过程。

Result: 研究展示了通过熵正则化策略迭代实例化该框架，并证明了收敛性保证。随着情景记忆的扩大，该策略最终可以收敛到最优策略。

Conclusion: 该研究提供了一种理论基础，使记忆增强的和基于检索的语言模型能够持续适应而无需更新参数，这一框架或可推动语言模型在实际应用中的性能改进。

Abstract: We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.

</details>


### [21] [HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery](https://arxiv.org/abs/2512.22899)
*Yaping Zhang,Qixuan Zhang,Xingquan Zhang,Zhiyuan Chen,Wenwen Zhuang,Yupu Liang,Lu Xiang,Yang Zhao,Jiajun Zhang,Yu Zhou,Chengqing Zong*

Main category: cs.AI

TL;DR: HiSciBench 提出了一种新的层次化基准，用于评估基础模型在五个科学工作流程层级上的能力，涵盖从基础知识理解到科学发现。该基准包括8,735个精心策划的实例，并支持跨学科和多模态输入。评估结果显示，现有模型在基本任务上可达69%的准确率，但在更高级别的任务上准确性显著下降。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和多模态基础模型的快速发展，研究者希望评估这些模型在科学研究中的应用潜力，但由于科学智能涉及广泛的能力范围，现有基准显得不够全面，因此需要一个更综合、多层级的评价体系。

Method: HiSciBench 通过设计一个包含五个层级的多层级基准来评估模型的科学智能，这些层级分别是科学素养、文献解析、基于文献的问题回答、文献综述生成以及科学发现。基准涵盖了六个主要学科，并支持多模态输入。

Result: 实证研究中，HiSciBench 显示了当前顶级模型（如 GPT-5 和 DeepSeek-R1）在不同科学智能任务上的显著性能差异，特别是在高级别任务上表现出显著下降。

Conclusion: HiSciBench 为评估模型科学智能提供了一个新的标准，强调跨阶段的知识依赖性，有助于促进模型开发向更可靠和全面发展方向的改进。该基准计划公开发布，以支持未来的研究工作。

Abstract: The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \textit{Scientific Literacy} (L1), \textit{Literature Parsing} (L2), \textit{Literature-based Question Answering} (L3), \textit{Literature Review Generation} (L4), and \textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\% accuracy on basic literacy tasks, performance declines sharply to 25\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.

</details>


### [22] [Geometric Structural Knowledge Graph Foundation Model](https://arxiv.org/abs/2512.22931)
*Ling Xin,Mojtaba Nayyeri,Zahra Makki Nayeri,Steffen Staab*

Main category: cs.AI

TL;DR: Gamma是一个引入多头几何注意力的知识图谱推理模型，通过多层次的变换机制提高表达能力，实验证明其在零样本归纳链接预测任务上优于Ultra。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱推理模型如Ultra仅依赖单一的加权方式，限制了模型的表达能力，无法有效处理多种关系和结构模式，因此提出Gamma以解决这一问题。

Method: Gamma采用了多头几何注意力机制，通过多种基于不同代数结构的变换（如实数、复数、双曲复数、双数）来表示关系，结合关系条件下的注意力融合机制在链接级别进行适配融合。

Result: 实验结果表明，Gamma在零样本推理任务上表现优越，尤其是在环形基准测试中，均值倒数排名提高了5.5%，综合基准测试中的整体性能提高了4.4%。

Conclusion: Gamma通过多头几何注意力提升知识图谱推理的表达能力和泛化能力，其多层次的设计对多种复杂关系的建模更为有效。

Abstract: Structural knowledge graph foundation models aim to generalize reasoning to completely new graphs with unseen entities and relations. A key limitation of existing approaches like Ultra is their reliance on a single relational transformation (e.g., element-wise multiplication) in message passing, which can constrain expressiveness and fail to capture diverse relational and structural patterns exhibited on diverse graphs. In this paper, we propose Gamma, a novel foundation model that introduces multi-head geometric attention to knowledge graph reasoning. Gamma replaces the single relational transformation with multiple parallel ones, including real, complex, split-complex, and dual number based transformations, each designed to model different relational structures. A relational conditioned attention fusion mechanism then adaptively fuses them at link level via a lightweight gating with entropy regularization, allowing the model to robustly emphasize the most appropriate relational bias for each triple pattern. We present a full formalization of these algebraic message functions and discuss how their combination increases expressiveness beyond any single space. Comprehensive experiments on 56 diverse knowledge graphs demonstrate that Gamma consistently outperforms Ultra in zero-shot inductive link prediction, with a 5.5% improvement in mean reciprocal rank on the inductive benchmarks and a 4.4% improvement across all benchmarks, highlighting benefits from complementary geometric representations.

</details>


### [23] [Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education](https://arxiv.org/abs/2512.23036)
*Danial Hooshyar,Yeongwook Yang,Gustav Šíř,Tommi Kärkkäinen,Raija Hämäläinen,Mutlu Cukurova,Roger Azevedo*

Main category: cs.AI

TL;DR: 此研究探讨了基于大语言模型（LLM）的辅导系统的局限性，对比了DKT模型与LLM在知识追踪上的表现，发现DKT在准确性和时效性上均优于LLM，即使是经过微调的LLM也无法达到DKT的效果，这表明单独使用LLM辅导系统的效果可能不如成熟的人工智能辅导系统。


<details>
  <summary>Details</summary>
Motivation: 鉴于大语言模型在K-12教育中的广泛应用带来的误解，以及欧盟AI法案对高风险领域的分类应对，本研究旨在评估大语言模型在知识追踪及适应性教学中的局限性，确保教育技术的合理使用。

Method: 本研究综合分析了基于大语言模型的辅导系统在准确性和时效性上的局限性，通过对广泛使用的LLM进行了零样本和微调两种方式的比较，并使用了一个大型公开可访问的数据集进行评估。

Result: 经过零样本和微调的LLM在预测下一个步骤的正确性方面，其AUC分别为0.75和0.83，尽管微调提高了约8%的AUC，但仍比DKT低6%，且微调的LLM在早期序列错误上表现更差。跨时间分析表明，DKT能够稳定、方向正确的更新知识掌握，而LLM变体表现出时间上的不稳定性和错误的方向。即便经过微调，LLM在多技能掌握估计中也产生不一致的趋势，而DKT则保持了平滑和连贯的更新。

Conclusion: 本文的研究结果表明，单独使用大语言模型在知识追踪及适应性教学中的效果有限，可能不足以替代现有的智能辅导系统。因此，负责任的教学辅导需要结合学生建模的混合框架。

Abstract: The rapid rise of large language model (LLM)-based tutors in K--12 education has fostered a misconception that generative models can replace traditional learner modelling for adaptive instruction. This is especially problematic in K--12 settings, which the EU AI Act classifies as high-risk domain requiring responsible design. Motivated by these concerns, this study synthesises evidence on limitations of LLM-based tutors and empirically investigates one critical issue: the accuracy, reliability, and temporal coherence of assessing learners' evolving knowledge over time. We compare a deep knowledge tracing (DKT) model with a widely used LLM, evaluated zero-shot and fine-tuned, using a large open-access dataset. Results show that DKT achieves the highest discrimination performance (AUC = 0.83) on next-step correctness prediction and consistently outperforms the LLM across settings. Although fine-tuning improves the LLM's AUC by approximately 8\% over the zero-shot baseline, it remains 6\% below DKT and produces higher early-sequence errors, where incorrect predictions are most harmful for adaptive support. Temporal analyses further reveal that DKT maintains stable, directionally correct mastery updates, whereas LLM variants exhibit substantial temporal weaknesses, including inconsistent and wrong-direction updates. These limitations persist despite the fine-tuned LLM requiring nearly 198 hours of high-compute training, far exceeding the computational demands of DKT. Our qualitative analysis of multi-skill mastery estimation further shows that, even after fine-tuning, the LLM produced inconsistent mastery trajectories, while DKT maintained smooth and coherent updates. Overall, the findings suggest that LLMs alone are unlikely to match the effectiveness of established intelligent tutoring systems, and that responsible tutoring requires hybrid frameworks that incorporate learner modelling.

</details>


### [24] [The Reward Model Selection Crisis in Personalized Alignment](https://arxiv.org/abs/2512.23067)
*Fady Rezk,Yuangang Pan,Chuan-Sheng Foo,Xun Xu,Nancy Chen,Henry Gouk,Timothy Hospedales*

Main category: cs.AI

TL;DR: 研究指出，现有的奖励模型准确性（RM准确性）忽略了在实际部署中至关重要的生成决策指导能力。通过新的准则和实验表明，仅依赖标准的RM准确性无法预测部署效果，且在情境学习方法下，对于大参数模型，ICL比基于奖励的方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化对齐方法主要集中于提高RM准确性，但忽略了实际部署时计算限制带来的挑战，即需要在推理时通过奖励指导解码（RGD）来适应用户，而非每次都重新微调用户的策略。因此，研究引入了一个新的度量标准：政策准确性，以评估RGD执行函数是否能够有效区分偏好和不偏好响应，揭示现有方法的问题。

Method: 研究通过系统评估三个数据集，引入了新的政策准确性指标，该指标用于评估奖励模型在指导生成决策时的有效性。研究还提出了一种新的基准测试，包含真实用户的完成结果，以直接评估行为表现，而无需依赖循环的奖励指标。并分析了不同方法在大规模模型下的表现。

Result: 研究发现，标准的RM准确性与生成决策指导能力相关性较低，且存在显著的分离现象：具有20分RM准确性差异的方法会产生几乎相同质量的输出，即使在能够进行高度区分的方法中，也无法在行为上产生一致的响应。对于超过3B参数的模型，情境学习方法（ICL）优于所有奖励导向的方法，能够在7B规模下实现3-5分的ROUGE-1增益。

Conclusion: 研究结论表明，当前使用的技术指标无法准确预测部署效果，未充分考虑实际部署中的计算限制，导致不能有效将用户的偏好转化为实际行为上的个性化调整。需要重新评估个性化对齐的目标和方法，以适应实际部署环境的需求。

Abstract: Personalized alignment from preference data has focused primarily on improving reward model (RM) accuracy, with the implicit assumption that better preference ranking translates to better personalized behavior. However, in deployment, computational constraints necessitate inference-time adaptation via reward-guided decoding (RGD) rather than per-user policy fine-tuning. This creates a critical but overlooked requirement: reward models must not only rank preferences accurately but also effectively guide token-level generation decisions. We demonstrate that standard RM accuracy fails catastrophically as a selection criterion for deployment-ready personalized alignment. Through systematic evaluation across three datasets, we introduce policy accuracy, a metric quantifying whether RGD scoring functions correctly discriminate between preferred and dispreferred responses. We show that RM accuracy correlates only weakly with this policy-level discrimination ability (Kendall's tau = 0.08--0.31). More critically, we introduce Pref-LaMP, the first personalized alignment benchmark with ground-truth user completions, enabling direct behavioral evaluation without circular reward-based metrics. On Pref-LaMP, we expose a complete decoupling between discrimination and generation: methods with 20-point RM accuracy differences produce almost identical output quality, and even methods achieving high discrimination fail to generate behaviorally aligned responses. Finally, simple in-context learning (ICL) dominates all reward-guided methods for models > 3B parameters, achieving 3-5 point ROUGE-1 gains over the best reward method at 7B scale. These findings show that the field optimizes proxy metrics that fail to predict deployment performance and do not translate preferences into real behavioral adaptation under deployment constraints.

</details>


### [25] [Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients](https://arxiv.org/abs/2512.23090)
*Armin Berger,Manuela Bergau,Helen Schneider,Saad Ahmad,Tom Anglim Lagones,Gianluca Brugnara,Martha Foltyn-Dumitru,Kai Schlamp,Philipp Vollmuth,Rafet Sifa*

Main category: cs.AI

TL;DR: ChexReason 是一种利用 R1 方法（SFT 后跟随 GRPO）训练的视觉语言模型，仅使用少量样本和单块 A100 GPU 实现了在医疗图像分析任务上的性能提升。


<details>
  <summary>Details</summary>
Motivation: 鉴于近期 RL 在 LLM 中的进展主要集中在推理任务上，且医疗图像分析中的资源约束应用尚未充分探索，本文旨在通过研究提出一种新的方法以降低成本并提高医学生物图像的处理能力。

Method: 方法包括使用 R1 方法进行训练，即通过 SFT（自我监督学习）后跟随 GRPO（多步奖励优化），在有限资源下（2000 个 SFT 样本、1000 个 RL 样本和单块 A100 GPU）训练 ChexReason 模型。

Result: 在 CheXpert 和 NIH 基准测试中，GROP 恢复了对分布内性能的提升（CheXpert 中宏 F1 得分为 0.346，提升 23%），但在跨数据集迁移性方面表现较差（NIH 中下降 19%）。此现象与高资源模型一致，表明问题可能源于 RL 原理而非规模。

Conclusion: 研究揭示了一个泛化悖论：SFT 检查点在优化前对 NIH 上表现特别好，表明教师指导的推理捕捉到了更多机构无关的功能特征。此外，跨模型比较显示了结构化推理支架对通用 VLM 的优势，但对于医学生物预训练模型增益有限，因此精选的监督微调可能比进取的 RL 更适用于需要跨不同人群具备稳健性的临床部署。

Abstract: Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.

</details>


### [26] [InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization](https://arxiv.org/abs/2512.23126)
*Yu Li,Tian Lan,Zhengling Qi*

Main category: cs.AI

TL;DR: 该研究提出了一种名为Intrinsic Self-reflective Preference Optimization (Intrinsic Self-reflective Preference Optimization)的方法，旨在解决直接偏好优化(Direct Preference Optimization, DPO)及其变体中的两个基本局限性：依赖任意建模选择导致的行为反映参数化因子而非真实偏好，以及孤立处理响应生成导致的信息遗漏问题。Intrinsic Self-reflective Preference Optimization考虑了上下文和替代响应，从而提供了一种全局最优策略，并且该方法不需要进行架构更改或者增加推理开销。实验结果表明，该方法在胜利率和控制长度的指标上都优于DPO/RLHF，证明了解锁自我反思可以产生更 robust, 与人类更一致的自动语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前直接偏好优化方法存在依赖建模选择和孤立处理响应生成的局限性，这限制了模型的真实性能。研究旨在提出一种全新的优化方法来解决这些局限问题，并提高自动语言模型的鲁棒性和人机协作性。

Method: 研究首先分析了DPO及其变体的问题所在，然后提出了Intrinsic Self-reflective Preference Optimization（Intrinsic Self-reflective Preference Optimization）的方法，它考虑了上下文和替代响应，从而得到了全局最优策略。该方法的核心在于基于上下文和替代响应进行优化，从而不依赖于建模选择，且无需改变架构或增加推理开销。

Result: 实验结果证明了Intrinsic Self-reflective Preference Optimization的有效性，在胜利率和控制长度等多个指标上均有显著提升，这表明该方法能够提高自动语言模型的稳健性和与人类的对齐程度。

Conclusion: 综上所述，Intrinsic Self-reflective Preference Optimization提供了一种全新的优化方法，解决了直接偏好优化中的两个基本问题，并通过实验验证了其有效性和优越性。

Abstract: Direct Preference Optimization (DPO) and its variants have become standard for aligning Large Language Models due to their simplicity and offline stability. However, we identify two fundamental limitations. First, the optimal policy depends on arbitrary modeling choices (scalarization function, reference policy), yielding behavior reflecting parameterization artifacts rather than true preferences. Second, treating response generation in isolation fails to leverage comparative information in pairwise data, leaving the model's capacity for intrinsic self-reflection untapped. To address it, we propose Intrinsic Self-reflective Preference Optimization (\q), deriving a globally optimal policy conditioning on both context and alternative responses. We prove this formulation superior to DPO/RLHF while guaranteeing invariance to scalarization and reference choices. \q~serves as a plug-and-play enhancement without architectural changes or inference overhead. Experiments demonstrate consistent improvements in win rates and length-controlled metrics, validating that unlocking self-reflection yields more robust, human-aligned LLMs.

</details>


### [27] [Why We Need a New Framework for Emotional Intelligence in AI](https://arxiv.org/abs/2512.23163)
*Max Parks,Kheli Atluru,Meera Vinod,Mike Kuniavsky,Jud Brewer,Sean White,Sarah Adler,Wendy Ju*

Main category: cs.AI

TL;DR: 本文强调现有用于评估人工智能（AI）系统情感智能（EI）的框架需要改进，因为它们未能全面衡量AI系统情感智能的相关方面。论文旨在审查有关情感和通用情感智能的不同理论，并批判性地评估当前可用的基准框架，以提出改进评估策略的方法。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能系统的不断发展，如何准确评估其情感智能成为了一个重要问题。现有评估框架虽然可以从某些角度评估AI系统的情感智能，但整体而言并不完善，无法全面衡量其情感智能的相关方面。

Method: 本文首先概述了不同关于情感和情感智能的理论，并审查了这些理论在应用于人工智能系统方面的适用性。接着，本文深入分析当前可用的性能评估基准框架，基于对情感智能理论的理解，指出这些框架在评估人工智能系统情感智能方面的不足之处。最后，本文提出了一些改进评估策略的方法，以避免这些问题。

Result: 本文通过分析不同的关于情感和情感智能的理论，提高了对当前评估人工智能系统情感智能框架的理解，为进一步研究改进评估策略提供了理论基础。

Conclusion: 本文强调了评估人工智能系统情感智能的现有框架存在的问题，并提出了一些改进措施，明确了未来研究的方向。

Abstract: In this paper, we develop the position that current frameworks for evaluating emotional intelligence (EI) in artificial intelligence (AI) systems need refinement because they do not adequately or comprehensively measure the various aspects of EI relevant in AI. Human EI often involves a phenomenological component and a sense of understanding that artificially intelligent systems lack; therefore, some aspects of EI are irrelevant in evaluating AI systems. However, EI also includes an ability to sense an emotional state, explain it, respond appropriately, and adapt to new contexts (e.g., multicultural), and artificially intelligent systems can do such things to greater or lesser degrees. Several benchmark frameworks specialize in evaluating the capacity of different AI models to perform some tasks related to EI, but these often lack a solid foundation regarding the nature of emotion and what it is to be emotionally intelligent. In this project, we begin by reviewing different theories about emotion and general EI, evaluating the extent to which each is applicable to artificial systems. We then critically evaluate the available benchmark frameworks, identifying where each falls short in light of the account of EI developed in the first section. Lastly, we outline some options for improving evaluation strategies to avoid these shortcomings in EI evaluation in AI systems.

</details>


### [28] [From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research](https://arxiv.org/abs/2512.23184)
*Hongshen Sun,Juanjuan Zhang*

Main category: cs.AI

TL;DR: 该论文引入并形式化了一个名为“模型信念”的度量，它从大语言模型的token级概率中得出，用于捕捉生成运行中对选择替代项的模型信念分布。模型信念比模型选择本身更统计上有效，计算效率更高，特别是在有限运行次数的实际环境中。


<details>
  <summary>Details</summary>
Motivation: 当前实践中，大语言模型（LLMs）的输出通常被视为单一数据点，这低估了LLMs所固有的概率特性中的信息量。论文提出并证明了模型信念的引入可以更充分利用LLM的信息，提供更有效的统计估计。

Method: 论文通过形式化和研究模型信念的统计性质来验证其有效性，包括证明其渐近等价于模型选择的均值但具有更低的方差和更快的收敛速率，同时也表明它对于模型选择平滑函数也具有类似性质。

Result: 通过需求估计的研究，模型信念在有限运行次数的实际环境中，比模型选择本身更好地解释和预测了真实模型选择，并将所需计算量降低了约20倍。这一结果支持使用模型信念作为提取LLM生成数据信息的标准度量。

Conclusion: 论文建议在处理LLM生成的数据时，使用模型信念而非直接使用模型选择，这是因为它更统计上有效且更高效的特性，特别是在计算资源有限的情况下。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output ("model choice") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes "model belief," a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.

</details>


### [29] [Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control](https://arxiv.org/abs/2512.23292)
*Yoonpyo Lee,Kazuma Kobayashi,Sai Puppala,Sajedul Talukder,Seid Koric,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.AI

TL;DR: 本文介绍了一种全新的物理AI途径，通过引入基于物理验证的策略优化，使得在大规模模型上表现出高效的执行控制能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在处理物理任务时存在感知偏差问题，表现为虽然能够保持语义上的合理性，但无法满足物理约束。因此，提出了从感知层面的模仿转移到关注执行结果的空间保证。

Method: 通过训练一个3.6亿参数的语言模型，在合成的反应堆控制场景中从10^3扩展到10^5个例子，实现了从小型系统到大型系统性能的显著提升。

Result: 模型在大型系统上表现出高稳定的执行表现，策略优化由基于物理验证驱动，且能够跨不同物理环境及连续输入模态迁移。

Conclusion: 该模型展示了从感知层面向执行层面的转变，为领域特定的基础模型带来了新的可能性。

Abstract: The prevailing paradigm in AI for physical systems, scaling general-purpose foundation models toward universal multimodal reasoning, confronts a fundamental barrier at the control interface. Recent benchmarks show that even frontier vision-language models achieve only 50-53% accuracy on basic quantitative physics tasks, behaving as approximate guessers that preserve semantic plausibility while violating physical constraints. This input unfaithfulness is not a scaling deficiency but a structural limitation. Perception-centric architectures optimize parameter-space imitation, whereas safety-critical control demands outcome-space guarantees over executed actions. Here, we present a fundamentally different pathway toward domain-specific foundation models by introducing compact language models operating as Agentic Physical AI, in which policy optimization is driven by physics-based validation rather than perceptual inference. We train a 360-million-parameter model on synthetic reactor control scenarios, scaling the dataset from 10^3 to 10^5 examples. This induces a sharp phase transition absent in general-purpose models. Small-scale systems exhibit high-variance imitation with catastrophic tail risk, while large-scale models undergo variance collapse exceeding 500x reduction, stabilizing execution-level behavior. Despite balanced exposure to four actuation families, the model autonomously rejects approximately 70% of the training distribution and concentrates 95% of runtime execution on a single-bank strategy. Learned representations transfer across distinct physics and continuous input modalities without architectural modification.

</details>


### [30] [On Conformant Planning and Model-Checking of $\exists^*\forall^*$ Hyperproperties](https://arxiv.org/abs/2512.23324)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.AI

TL;DR: 本文研究了规划和验证领域两个问题之间的联系：符合性规划和超属性模型检查。它提出了一种高效的方法将超属性模型检查问题编码为符合性规划问题，反之亦然。


<details>
  <summary>Details</summary>
Motivation: 作为一名规划与验证领域的研究者，探讨符合性规划和超属性模型检查之间的关系有助于拓宽我们对这两个问题的理解，并可能为解决这些问题提供新的解决方案。

Method: 本文通过提供一种高效的编码方法，将超属性模型检查的实例转换为符合性规划实例，并证明其完整性和正确的编码方式。

Result: 作者证明了一种高效的编码方法，可以从超属性模型检查的实例高效地转换到符合性规划的实例中，同时示例化地展示了从任何符合性规划问题都能还原为一个超属性模型检查问题。

Conclusion: 本文为理解和研究符合性规划和超属性模型检查问题提供了新的视角，并为这两个问题的设计和实现奠定了基础。

Abstract: We study the connection of two problems within the planning and verification community: Conformant planning and model-checking of hyperproperties. Conformant planning is the task of finding a sequential plan that achieves a given objective independent of non-deterministic action effects during the plan's execution. Hyperproperties are system properties that relate multiple execution traces of a system and, e.g., capture information-flow and fairness policies. In this paper, we show that model-checking of $\exists^*\forall^*$ hyperproperties is closely related to the problem of computing a conformant plan. Firstly, we show that we can efficiently reduce a hyperproperty model-checking instance to a conformant planning instance, and prove that our encoding is sound and complete. Secondly, we establish the converse direction: Every conformant planning problem is, itself, a hyperproperty model-checking task.

</details>


### [31] [Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2512.23457)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Wenjian Zhang,Min Cen,Yang Zhou,Wenkai Fang,Yiru Zhao,Baisheng Lai,Mingli Song*

Main category: cs.AI

TL;DR: 提出了一个名为HiR的样本高效强化学习框架，用于复杂指令跟随任务，并通过回放失败尝试作为成功来改进学习过程，同时仅使用二元奖励信号即可有效优化。


<details>
  <summary>Details</summary>
Motivation: 当前的RL方法受限于模型生成满足所有约束的高质量响应的能力，因此任务初始阶段难以生成足够的正样本。为了解决这一问题，作者提出了HiR框架。

Method: HiR采用‘选择-重写’策略，允许失败的操作被视为成功，基于事后分析的约束条件来优化样本集。这种方法使学习过程能够高效地使用二元奖励信号进行优化。

Result: 通过广泛的实验证明，提出的HiR框架在不同指令跟随任务中表现优异，同时减少了计算资源的消耗。

Conclusion: HiR框架为复杂指令跟随任务中的样本效率学习提供了新的解决方案，其二元奖励框架有助于有效地优化强化学习过程。

Abstract: Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.

</details>


### [32] [Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation](https://arxiv.org/abs/2512.23601)
*Manh Hung Nguyen,Adish Singla*

Main category: cs.AI

TL;DR: 该研究提出了CreativeDC，一种两阶段提示方法，通过分离创造性的探索和约束满足，旨在提高语言模型生成教育问题的多样性和新颖性，同时保持实用性。


<details>
  <summary>Details</summary>
Motivation: 探讨解决大型语言模型生成的教育问题存在相似性和重复性的方法，以促进学生思维的多样性。

Method: 基于Wallas的创造力理论和Guilford的发散-收敛思维框架，提出了CreativeDC两阶段提示方法，分别进行创造性的探索和满足约束。

Result: CreativeDC在多样性、新颖性和实用性方面优于基线方法，并在采样更多问题时生成更多独特的教育问题。

Conclusion: CreativeDC方法有效提高了语言模型生成教育问题的能力，促进了教育内容的多样性和创新性。

Abstract: Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.

</details>


### [33] [Physics-Informed Neural Networks for Device and Circuit Modeling: A Case Study of NeuroSPICE](https://arxiv.org/abs/2512.23624)
*Chien-Ting Tung,Chenming Hu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present NeuroSPICE, a physics-informed neural network (PINN) framework for device and circuit simulation. Unlike conventional SPICE, which relies on time-discretized numerical solvers, NeuroSPICE leverages PINNs to solve circuit differential-algebraic equations (DAEs) by minimizing the residual of the equations through backpropagation. It models device and circuit waveforms using analytical equations in time domain with exact temporal derivatives. While PINNs do not outperform SPICE in speed or accuracy during training, they offer unique advantages such as surrogate models for design optimization and inverse problems. NeuroSPICE's flexibility enables the simulation of emerging devices, including highly nonlinear systems such as ferroelectric memories.

</details>


### [34] [Regret-Based Federated Causal Discovery with Unknown Interventions](https://arxiv.org/abs/2512.23626)
*Federico Baldo,Charles K. Assaad*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Most causal discovery methods recover a completed partially directed acyclic graph representing a Markov equivalence class from observational data. Recent work has extended these methods to federated settings to address data decentralization and privacy constraints, but often under idealized assumptions that all clients share the same causal model. Such assumptions are unrealistic in practice, as client-specific policies or protocols, for example, across hospitals, naturally induce heterogeneous and unknown interventions. In this work, we address federated causal discovery under unknown client-level interventions. We propose I-PERI, a novel federated algorithm that first recovers the CPDAG of the union of client graphs and then orients additional edges by exploiting structural differences induced by interventions across clients. This yields a tighter equivalence class, which we call the $\mathbfΦ$-Markov Equivalence Class, represented by the $\mathbfΦ$-CPDAG. We provide theoretical guarantees on the convergence of I-PERI, as well as on its privacy-preserving properties, and present empirical evaluations on synthetic data demonstrating the effectiveness of the proposed algorithm.

</details>


### [35] [Web World Models](https://arxiv.org/abs/2512.23676)
*Jichen Feng,Yifan Zhang,Chenggong Zhang,Yifu Lu,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [36] [A Statistical Side-Channel Risk Model for Timing Variability in Lattice-Based Post-Quantum Cryptography](https://arxiv.org/abs/2512.22301)
*Aayush Mainali,Sirjan Ghimire*

Main category: cs.CR

TL;DR: 本文提出了一种基于统计风险模型的方法，以评估lattice-based密码方案在不同场景下的定时泄漏情况，并使用多种统计方法量化泄漏程度，从而在没有具体硬件平台的情况下进行可重复的设计阶段比较。


<details>
  <summary>Details</summary>
Motivation: 文章旨在解决后量子密码学中由于复杂算术和控制流导致的由密钥依赖的定时变异性带来的安全威胁，特别是在存在环境噪声的情况下。

Method: 该方法通过生成不同秘密类别的轨迹，在闲时、抖动和负载条件下，以及针对多种泄漏模型，使用Welch's t-test、KS距离、Cliff's delta、互信息和分布重叠等统计方法来量化泄漏程度，结合以TLRI类似的方式获得一致评分，用于排序场景。

Result: 研究结果表明，闲时条件下通常具有最佳可区分性，抖动和负载条件会通过增加方差和重叠来侵蚀可区分性；缓存索引和分支样式的泄漏倾向于给出最高风险信号，且较快速的方案在相同泄漏假设下可以有更高的峰值风险，允许在没有具体硬件平台的情况下进行可重复的设计早期阶段比较。

Conclusion: 该模型为设计初期提供了评估不同场景下定时泄漏风险的工具，并且支持在选择合适的硬件平台验证之前进行可重复的设计比较。

Abstract: Timing side-channels are an important threat to cryptography that still needs to be addressed in implementations, and the advent of post-quantum cryptography raises this issue because the lattice-based schemes may produce secret-dependent timing variability with the help of complex arithmetic and control flow. Since also real timing measurements are affected by environmental noise (e.g. scheduling effects, contention, heavy tailed delays), in this work a scenario-based statistical risk model is proposed for timing leakage as a problem of distributional distinguishability under controlled execution conditions. We synthesize traces for two secret classes in idle, jitter and loaded scenarios and for multiple leakage models and quantify leakage with Welch's t-test, KS distance, Cliff's delta, mutual information, and distribution overlap to combine in a TLRI like manner to obtain a consistent score for ranking scenarios. Across representative lattice-based KEM families (Kyber, Saber, Frodo), idle conditions generally have the best distinguishability, jitter and loaded conditions erode distinguishability through an increase in variance and increase in overlap; cache-index and branch-style leakage tends to give the highest risk signals, and faster schemes can have a higher peak risk given similar leakage assumptions, allowing reproducible comparisons at an early design stage, prior to platform-specific validation.

</details>


### [37] [Beyond Single Bugs: Benchmarking Large Language Models for Multi-Vulnerability Detection](https://arxiv.org/abs/2512.22306)
*Chinmay Pushkar,Sanchit Kabra,Dhruv Kumar,Jagat Sesh Challa*

Main category: cs.CR

TL;DR: 该研究构建了一个全面的多漏洞检测基准，涵盖C、C++、Python和JavaScript四种语言，针对大语言模型在代码安全中的表现进行了评估，发现模型性能随着漏洞密度增加而显著下降，尤其是在Python和JavaScript语言中，模型出现严重的“欠计数”问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要用于单一漏洞样本或函数级别的分类，不足以反映真实软件环境中的复杂性。研究旨在量化大语言模型在代码安全多标签任务中的“计数偏差”和“选择偏差”，并提供一个全面的多漏洞检测基准。

Method: 研究团队构建了一套包含40,000个文件的数据集，每个文件中系统地注入了不同数量的控制漏洞（1, 3, 5, 9），以长上下文代码片段（7.5k-10k标记）为基础，来自CodeParrot。对五种最先进的大语言模型（包括GPT-4o-mini、Llama-3.3-70B和Qwen-2.5系列）进行了评估。

Result: 研究发现，模型在单漏洞任务上的表现随着高密度漏洞环境下的性能显著下降。Llama-3.3-70B在单漏洞C任务中接近完美的F1分数（约0.97），但在高密度设置中性能下降高达40%。特别地，Python和JavaScript中模型显示了显著的“欠计数”问题，召回率低至小于0.30。

Conclusion: 研究揭示了大语言模型在处理代码安全中的多漏洞检测任务时的局限性，并强调了在高漏洞密度环境下的性能衰退。这对于未来研究和实际应用具有重要启示。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in automated software security, particularly in vulnerability detection. However, existing benchmarks primarily focus on isolated, single-vulnerability samples or function-level classification, failing to reflect the complexity of real-world software where multiple interacting vulnerabilities often coexist within large files. Recent studies indicate that LLMs suffer from "count bias" and "selection bias" in multi-label tasks, yet this has not been rigorously quantified in the domain of code security. In this work, we introduce a comprehensive benchmark for Multi-Vulnerability Detection across four major languages: C, C++, Python, and JavaScript. We construct a dataset of 40,000 files by systematically injecting controlled counts of vulnerabilities (1, 3, 5, and 9) into long-context code samples (7.5k-10k tokens) sourced from CodeParrot. We evaluate five state-of-the-art LLMs, including GPT-4o-mini, Llama-3.3-70B, and the Qwen-2.5 series. Our results reveal a sharp degradation in performance as vulnerability density increases. While Llama-3.3-70B achieves near-perfect F1 scores (approximately 0.97) on single-vulnerability C tasks, performance drops by up to 40% in high-density settings. Notably, Python and JavaScript show distinct failure modes compared to C/C++, with models exhibiting severe "under-counting" (Recall dropping to less than 0.30) in complex Python files.

</details>


### [38] [LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators](https://arxiv.org/abs/2512.22307)
*You Li,Guannan Zhao,Yuhao Ju,Yunqi He,Jie Gu,Hai Zhou*

Main category: cs.CR

TL;DR: LLA 是一种有效的生成型 AI 模型知识产权保护方案，通过软硬件协同保护模型不被盗取、篡改或泄露信息，同时保持较低的计算开销。


<details>
  <summary>Details</summary>
Motivation: 面对生成型 AI 模型可能面临的供应链威胁，如模型被盗、篡改或信息泄露，LLA 提供了一种有效的软硬件结合保护方案，旨在确保模型的知识产权安全。

Method: LLA 方法包含软件和硬件两部分。软件方面，它将关键信息嵌入到可能导致性能下降的神经元中，并应用不变变换来模糊关键值。硬件方面，它集成了一个轻量锁模块到 AI 加速器中，此模块在预存密钥的情况下作为访问 IP 模型服务的许可。

Result: LLA 能够抵御多种 Oracle 引导的关键优化攻击，且计算开销低于 0.1%，主要针对 7,168 位密钥。

Conclusion: LLA 通过软硬件结合的方式成功保护了生成型 AI 模型的知识产权，有效地对抗了各种威胁，同时保持了较低的计算成本。

Abstract: We introduce LLA, an effective intellectual property (IP) protection scheme for generative AI models. LLA leverages the synergy between hardware and software to defend against various supply chain threats, including model theft, model corruption, and information leakage. On the software side, it embeds key bits into neurons that can trigger outliers to degrade performance and applies invariance transformations to obscure the key values. On the hardware side, it integrates a lightweight locking module into the AI accelerator while maintaining compatibility with various dataflow patterns and toolchains. An accelerator with a pre-stored secret key acts as a license to access the model services provided by the IP owner. The evaluation results show that LLA can withstand a broad range of oracle-guided key optimization attacks, while incurring a minimal computational overhead of less than 0.1% for 7,168 key bits.

</details>


### [39] [Verifiable Dropout: Turning Randomness into a Verifiable Claim](https://arxiv.org/abs/2512.22526)
*Kichang Lee,Sungmin Lee,Jaeho Jin,JeongGil Ko*

Main category: cs.CR

TL;DR: Verifiable Dropout 提出了一种基于零知识证明的隐私保护机制，通过将随机掩码绑定到一个确定性的、密码学验证的种子，并证明 dropout 操作的正确执行，从而允许用户在不泄露模型和数据隐私的情况下，后验审计随机训练步骤的完整性。


<details>
  <summary>Details</summary>
Motivation: 现有的审计追踪机制无法验证深度学习中的随机值是否被真实生成和应用，而 Verifiable Dropout 旨在解决这一问题，通过使用户能够在不暴露敏感训练数据的情况下验证随机性操作的诚实性。

Method: Verifiable Dropout 通过将随机掩码与一个确定性的、密码学验证的种子绑定，并使用零知识证明技术来验证 dropout 操作的正确执行，形成了一种隐私保护机制。

Result: 该研究提出了 Verifiable Dropout 机制，证明了随机性操作的正确执行，增强了 AI 模型训练过程中的审计能力，同时确保了数据和模型的隐私性。

Conclusion: Verifiable Dropout 增强了深度学习中随机性能审计的可信度，使得 AI 训练更加可靠和安全。

Abstract: Modern cloud-based AI training relies on extensive telemetry and logs to ensure accountability. While these audit trails enable retrospective inspection, they struggle to address the inherent non-determinism of deep learning. Stochastic operations, such as dropout, create an ambiguity surface where attackers can mask malicious manipulations as natural random variance, granting them plausible deniability. Consequently, existing logging mechanisms cannot verify whether stochastic values were generated and applied honestly without exposing sensitive training data. To close this integrity gap, we introduce Verifiable Dropout, a privacy-preserving mechanism based on zero-knowledge proofs. We treat stochasticity not as an excuse but as a verifiable claim. Our approach binds dropout masks to a deterministic, cryptographically verifiable seed and proves the correct execution of the dropout operation. This design enables users to audit the integrity of stochastic training steps post-hoc, ensuring that randomness was neither biased nor cherry-picked, while strictly preserving the confidentiality of the model and data.

</details>


### [40] [Raven: Mining Defensive Patterns in Ethereum via Semantic Transaction Revert Invariants Categories](https://arxiv.org/abs/2512.22616)
*Mojtaba Eshghie,Melissa Mazura,Alexandre Bartel*

Main category: cs.CR

TL;DR: Raven框架通过分析回退交易的不变量，揭示了新的安全模式，并通过案例研究展示了其在检测攻击中的实用性。


<details>
  <summary>Details</summary>
Motivation: 研究发现现有的安全研究对由守恒触发的回退交易中的防御模式关注不足，而这些模式对提高智能合约安全性有价值。

Method: 使用Raven框架对交易回退进行分析，采用基于BERT的模型嵌入守恒，并按语义意图进行聚类，以发现防御守恒类别。

Result: Raven框架成功地在20,000个回退交易中发现并分类了19个语义类别，其中包含6个未经记录的类别，如功能开关、重放预防、证明/签名验证等。

Conclusion: Raven为安全研究人员提供了一个数据驱动的分析工具基础，用于从智能合约的工作防御中提取安全或acles来检测漏洞。

Abstract: We frame Ethereum transactions reverted by invariants-require(<invariant>)/ assert(<invariant>)/if (<invariant>) revert statements in the contract implementation-as a positive signal of active on-chain defenses. Despite their value, the defensive patterns in these transactions remain undiscovered and underutilized in security research. We present Raven, a framework that aligns reverted transactions to the invariant causing the reversion in the smart contract source code, embeds these invariants using our BERT-based fine-tuned model, and clusters them by semantic intent to mine defensive invariant categories on Ethereum. Evaluated on a sample of 20,000 reverted transactions, Raven achieves cohesive and meaningful clusters of transaction-reverting invariants. Manual expert review of the mined 19 semantic clusters uncovers six new invariant categories absent from existing invariant catalogs, including feature toggles, replay prevention, proof/signature verification, counters, caller-provided slippage thresholds, and allow/ban/bot lists. To demonstrate the practical utility of this invariant catalog mining pipeline, we conduct a case study using one of the newly discovered invariant categories as a fuzzing oracle to detect vulnerabilities in a real-world attack. Raven thus can map Ethereum's successful defenses. These invariant categories enable security researchers to develop analysis tools based on data-driven security oracles extracted from the smart contracts' working defenses.

</details>


### [41] [SCyTAG: Scalable Cyber-Twin for Threat-Assessment Based on Attack Graphs](https://arxiv.org/abs/2512.22669)
*David Tayouri,Elad Duani,Abed Showgan,Ofir Manor,Ortal Lavi,Igor Podoski,Miro Ohana,Yuval Elovici,Andres Murillo,Asaf Shabtai,Rami Puzis*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Understanding the risks associated with an enterprise environment is the first step toward improving its security. Organizations employ various methods to assess and prioritize the risks identified in cyber threat intelligence (CTI) reports that may be relevant to their operations. Some methodologies rely heavily on manual analysis (which requires expertise and cannot be applied frequently), while others automate the assessment, using attack graphs (AGs) or threat emulators. Such emulators can be employed in conjunction with cyber twins to avoid disruptions in live production environments when evaluating the highlighted threats. Unfortunately, the use of cyber twins in organizational networks is limited due to their inability to scale. In this paper, we propose SCyTAG, a multi-step framework that generates the minimal viable cyber twin required to assess the impact of a given attack scenario. Given the organizational computer network specifications and an attack scenario extracted from a CTI report, SCyTAG generates an AG. Then, based on the AG, it automatically constructs a cyber twin comprising the network components necessary to emulate the attack scenario and assess the relevance and risks of the attack to the organization. We evaluate SCyTAG on both a real and fictitious organizational network. The results show that compared to the full topology, SCyTAG reduces the number of network components needed for emulation by up to 85% and halves the amount of required resources while preserving the fidelity of the emulated attack. SCyTAG serves as a cost-effective, scalable, and highly adaptable threat assessment solution, improving organizational cyber defense by bridging the gap between abstract CTI and practical scenario-driven testing.

</details>


### [42] [When RSA Fails: Exploiting Prime Selection Vulnerabilities in Public Key Cryptography](https://arxiv.org/abs/2512.22720)
*Murtaza Nikzad,Kerem Atas*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper explores vulnerabilities in RSA cryptosystems that arise from improper prime number selection during key generation. We examine two primary attack vectors: Fermat's factorization method, which exploits RSA keys generated with primes that are too close together, and the Greatest Common Divisor (GCD) attack, which exploits keys that share a common prime factor. Drawing from landmark research including Heninger et al.'s ``Mining Your Ps and Qs'' study, which discovered over 64,000 vulnerable TLS hosts, and B{ö}ck's 2023 analysis of Fermat factorization in deployed systems, we demonstrate that these vulnerabilities remain prevalent in real-world cryptographic implementations. Our analysis reveals that weak random number generation in embedded devices is the primary cause of these failures, and we discuss mitigation strategies including proper entropy collection and prime validation checks.

</details>


### [43] [Identifying social bots via heterogeneous motifs based on Naïve Bayes model](https://arxiv.org/abs/2512.22759)
*Yijun Ran,Jingjing Xiao,Xiao-Ke Xu*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Identifying social bots has become a critical challenge due to their significant influence on social media ecosystems. Despite advancements in detection methods, most topology-based approaches insufficiently account for the heterogeneity of neighborhood preferences and lack a systematic theoretical foundation, relying instead on intuition and experience. Here, we propose a theoretical framework for detecting social bots utilizing heterogeneous motifs based on the Naïve Bayes model. Specifically, we refine homogeneous motifs into heterogeneous ones by incorporating node-label information, effectively capturing the heterogeneity of neighborhood preferences. Additionally, we systematically evaluate the contribution of different node pairs within heterogeneous motifs to the likelihood of a node being identified as a social bot. Furthermore, we mathematically quantify the maximum capability of each heterogeneous motif, enabling the estimation of its potential benefits. Comprehensive evaluations on four large, publicly available benchmarks confirm that our method surpasses state-of-the-art techniques, achieving superior performance across five evaluation metrics. Moreover, our results reveal that selecting motifs with the highest capability achieves detection performance comparable to using all heterogeneous motifs. Overall, our framework offers an effective and theoretically grounded solution for social bot detection, significantly enhancing cybersecurity measures in social networks.

</details>


### [44] [A generalized motif-based Naïve Bayes model for sign prediction in complex networks](https://arxiv.org/abs/2512.22765)
*Yijun Ran,Si-Yuan Liu,Junjie Huang,Tao Jia,Xiao-Ke Xu*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Signed networks, encoding both positive and negative interactions, are essential for modeling complex systems in social and financial domains. Sign prediction, which infers the sign of a target link, has wide-ranging practical applications. Traditional motif-based Naïve Bayes models assume that all neighboring nodes contribute equally to a target link's sign, overlooking the heterogeneous influence among neighbors and potentially limiting performance. To address this, we propose a generalizable sign prediction framework that explicitly models the heterogeneity. Specifically, we design two role functions to quantify the differentiated influence of neighboring nodes. We further extend this approach from a single motif to multiple motifs via two strategies. The generalized multiple motifs-based Naïve Bayes model linearly combines information from diverse motifs, while the Feature-driven Generalized Motif-based Naïve Bayes (FGMNB) model integrates high-dimensional motif features using machine learning. Extensive experiments on four real-world signed networks show that FGMNB consistently outperforms five state-of-the-art embedding-based baselines on three of these networks. Moreover, we observe that the most predictive motif structures differ across datasets, highlighting the importance of local structural patterns and offering valuable insights for motif-based feature engineering. Our framework provides an effective and theoretically grounded solution to sign prediction, with practical implications for enhancing trust and security in online platforms.

</details>


### [45] [Breaking the illusion: Automated Reasoning of GDPR Consent Violations](https://arxiv.org/abs/2512.22789)
*Ying Li,Wenjun Qiu,Faysal Hossain Shezan,Kunlin Cai,Michelangelo van Dam,Lisa Austin,David Lie,Yuan Tian*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent privacy regulations such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have established legal requirements for obtaining user consent regarding the collection, use, and sharing of personal data. These regulations emphasize that consent must be informed, freely given, specific, and unambiguous. However, there are still many violations, which highlight a gap between legal expectations and actual implementation. Consent mechanisms embedded in functional web forms across websites play a critical role in ensuring compliance with data protection regulations such as the GDPR and CCPA, as well as in upholding user autonomy and trust. However, current research has primarily focused on cookie banners and mobile app dialogs. These forms are diverse in structure, vary in legal basis, and are often difficult to locate or evaluate, creating a significant challenge for automated consent compliance auditing. In this work, we present Cosmic, a novel automated framework for detecting consent-related privacy violations in web forms. We evaluate our developed tool for auditing consent compliance in web forms, across 5,823 websites and 3,598 forms. Cosmic detects 3,384 violations on 94.1% of consent forms, covering key GDPR principles such as freely given consent, purpose disclosure, and withdrawal options. It achieves 98.6% and 99.1% TPR for consent and violation detection, respectively, demonstrating high accuracy and real-world applicability.

</details>


### [46] [Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks](https://arxiv.org/abs/2512.22860)
*Soham Padia,Dhananjay Vaidya,Ramchandra Mangrulkar*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Securing blockchain-enabled IoT networks against sophisticated adversarial attacks remains a critical challenge. This paper presents a trust-based delegated consensus framework integrating Fully Homomorphic Encryption (FHE) with Attribute-Based Access Control (ABAC) for privacy-preserving policy evaluation, combined with learning-based defense mechanisms. We systematically compare three reinforcement learning approaches -- tabular Q-learning (RL), Deep RL with Dueling Double DQN (DRL), and Multi-Agent RL (MARL) -- against five distinct attack families: Naive Malicious Attack (NMA), Collusive Rumor Attack (CRA), Adaptive Adversarial Attack (AAA), Byzantine Fault Injection (BFI), and Time-Delayed Poisoning (TDP). Experimental results on a 16-node simulated IoT network reveal significant performance variations: MARL achieves superior detection under collusive attacks (F1=0.85 vs. DRL's 0.68 and RL's 0.50), while DRL and MARL both attain perfect detection (F1=1.00) against adaptive attacks where RL fails (F1=0.50). All agents successfully defend against Byzantine attacks (F1=1.00). Most critically, the Time-Delayed Poisoning attack proves catastrophic for all agents, with F1 scores dropping to 0.11-0.16 after sleeper activation, demonstrating the severe threat posed by trust-building adversaries. Our findings indicate that coordinated multi-agent learning provides measurable advantages for defending against sophisticated trust manipulation attacks in blockchain IoT environments.

</details>


### [47] [Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations](https://arxiv.org/abs/2512.22883)
*Tao Li,Quanyan Zhu*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cybersecurity is being fundamentally reshaped by foundation-model-based artificial intelligence. Large language models now enable autonomous planning, tool orchestration, and strategic adaptation at scale, challenging security architectures built on static rules, perimeter defenses, and human-centered workflows. This chapter argues for a shift from prevention-centric security toward agentic cyber resilience. Rather than seeking perfect protection, resilient systems must anticipate disruption, maintain critical functions under attack, recover efficiently, and learn continuously. We situate this shift within the historical evolution of cybersecurity paradigms, culminating in an AI-augmented paradigm where autonomous agents participate directly in sensing, reasoning, action, and adaptation across cyber and cyber-physical systems. We then develop a system-level framework for designing agentic AI workflows. A general agentic architecture is introduced, and attacker and defender workflows are analyzed as coupled adaptive processes, and game-theoretic formulations are shown to provide a unifying design language for autonomy allocation, information flow, and temporal composition. Case studies in automated penetration testing, remediation, and cyber deception illustrate how equilibrium-based design enables system-level resiliency design.

</details>


### [48] [DECEPTICON: How Dark Patterns Manipulate Web Agents](https://arxiv.org/abs/2512.22894)
*Phil Cuvin,Hao Zhu,Diyi Yang*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deceptive UI designs, widely instantiated across the web and commonly known as dark patterns, manipulate users into performing actions misaligned with their goals. In this paper, we show that dark patterns are highly effective in steering agent trajectories, posing a significant risk to agent robustness. To quantify this risk, we introduce DECEPTICON, an environment for testing individual dark patterns in isolation. DECEPTICON includes 700 web navigation tasks with dark patterns -- 600 generated tasks and 100 real-world tasks, designed to measure instruction-following success and dark pattern effectiveness. Across state-of-the-art agents, we find dark patterns successfully steer agent trajectories towards malicious outcomes in over 70% of tested generated and real-world tasks -- compared to a human average of 31%. Moreover, we find that dark pattern effectiveness correlates positively with model size and test-time reasoning, making larger, more capable models more susceptible. Leading countermeasures against adversarial attacks, including in-context prompting and guardrail models, fail to consistently reduce the success rate of dark pattern interventions. Our findings reveal dark patterns as a latent and unmitigated risk to web agents, highlighting the urgent need for robust defenses against manipulative designs.

</details>


### [49] [SecureBank: A Financially-Aware Zero Trust Architecture for High-Assurance Banking Systems](https://arxiv.org/abs/2512.23124)
*Paulo Fernandes Biao*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Financial institutions increasingly rely on distributed architectures, open banking APIs, cloud native infrastructures, and high frequency digital transactions. These transformations expand the attack surface and expose limitations in traditional perimeter based security models. While Zero Trust architectures provide essential security principles, most existing frameworks do not explicitly incorporate transactional semantics, financial risk modeling, adaptive identity trust, or automation weighted by economic impact.
  This paper introduces SecureBank, a financially aware and context adaptive Zero Trust architecture designed specifically for high assurance banking systems. The proposed framework integrates Financial Zero Trust, Adaptive Identity Scoring, Contextual Micro Segmentation, and Impact Driven Security Automation. A Monte Carlo simulation evaluates SecureBank against a representative rule based baseline architecture using metrics such as the Transactional Integrity Index (TII), Identity Trust Adaptation Level (ITAL), and Security Automation Efficiency (SAE).
  The results demonstrate that SecureBank significantly improves automated attack handling and accelerates identity trust adaptation while preserving conservative and regulator aligned levels of transactional integrity. Beyond experimental validation, SecureBank is intended to serve as a reference architecture and evaluation baseline for financially aware Zero Trust systems in regulated financial environments.

</details>


### [50] [Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems](https://arxiv.org/abs/2512.23132)
*Armstrong Foundjem,Lionel Nganyewou Tidjon,Leuson Da Silva,Foutse Khomh*

Main category: cs.CR

TL;DR: 本文概述了基于MITRE ATLAS、AI事件数据库和文献中提取的93个威胁，并利用多智能体RAG系统分析了854个GitHub/Python仓库，识别出针对ML基础模型、多模态及检索增强系统的未报告威胁。主要威胁途径包括MASTERKEY样式的 Jailbreaking、聚合中毒、扩散后门和偏好优化泄漏，主要影响预训练和推理阶段。构建了以威胁图谱为核心的安全框架，强调依赖性卫生、威胁情报和监控的重要性。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于揭示机器学习安全领域的风险，通过对各种来源的威胁进行提取和分析，识别出未被报道的威胁，并针对这些威胁制定专门的安全对策。

Method: 研究人员从MITRE ATLAS、AI事件数据库和文献中提取了93个威胁，并利用多智能体RAG系统分析了854个GitHub/Python仓库，构建了一个以威胁图谱为核心的安全框架。

Result: 研究识别出商业LLM API模型窃取、参数记忆泄露以及偏好导向的文本监狱逃逸等未报告的威胁。主要威胁途径包括MASTERKEY样式的 Jailbreaking、聚合中毒、扩散后门和偏好优化泄漏，主要影响预训练和推理阶段。此外，图分析揭示了漏洞热点区域，这些区域的库缺乏有效的漏洞修补。

Conclusion: 为了缓解供应链和推理阶段的安全风险，本文建议采用适应性的、ML特定的安全框架，该框架结合依赖性卫生、威胁情报和监控。

Abstract: Machine learning (ML) underpins foundation models in finance, healthcare, and critical infrastructure, making them targets for data poisoning, model extraction, prompt injection, automated jailbreaking, and preference-guided black-box attacks that exploit model comparisons. Larger models can be more vulnerable to introspection-driven jailbreaks and cross-modal manipulation. Traditional cybersecurity lacks ML-specific threat modeling for foundation, multimodal, and RAG systems. Objective: Characterize ML security risks by identifying dominant TTPs, vulnerabilities, and targeted lifecycle stages. Methods: We extract 93 threats from MITRE ATLAS (26), AI Incident Database (12), and literature (55), and analyze 854 GitHub/Python repositories. A multi-agent RAG system (ChatGPT-4o, temp 0.4) mines 300+ articles to build an ontology-driven threat graph linking TTPs, vulnerabilities, and stages. Results: We identify unreported threats including commercial LLM API model stealing, parameter memorization leakage, and preference-guided text-only jailbreaks. Dominant TTPs include MASTERKEY-style jailbreaking, federated poisoning, diffusion backdoors, and preference optimization leakage, mainly impacting pre-training and inference. Graph analysis reveals dense vulnerability clusters in libraries with poor patch propagation. Conclusion: Adaptive, ML-specific security frameworks, combining dependency hygiene, threat intelligence, and monitoring, are essential to mitigate supply-chain and inference risks across the ML lifecycle.

</details>


### [51] [Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning](https://arxiv.org/abs/2512.23171)
*Yu Jiang,Xindi Tong,Ziyao Liu,Xiaoxi Zhang,Kwok-Yan Lam,Chee Wei Tan*

Main category: cs.CR

TL;DR: FedORA 是一种针对垂直联邦学习中样本和标签遗忘的问题，通过引入新的损失函数和特殊的批处理设计，实现模型更新的有效优化。


<details>
  <summary>Details</summary>
Motivation: 随着数据隐私保护变得越来越重要，联邦学习尤其是联邦遗忘成为了解决这一问题的有效方法。特别是在垂直联邦学习中，联邦遗忘遇到了新的挑战，如样本和标签的删除需要跨方协调，这增加了计算开销和复杂性。

Method: FedORA 采用如下的优化方法：将样本或标签的删除问题建模为约束优化问题，并通过普鲁尔-对偶算法进行求解。它引入了一种新的遗忘损失函数，旨在促进分类不确定性而非错误分类。此外，FedORA 还包括自适应步长选择机制和不对称批量设计，以提高稳定性并减少计算开销。

Result: FedORA 在理论分析中证明了其与从头训练模型之间的差异是可控制的。在表格和图像数据集上的实验表明，FedORA 能够在保持计算和通信开销较低的情况下，达到与从头训练模型相当的遗忘效果和实用性。

Conclusion: FedORA 提供了一种有效的解决垂直联邦学习中样本和标签遗忘问题的方法。该方法不仅能够有效减少计算和通信开销，还能够保持遗忘的实用性和模型的性能。

Abstract: Federated unlearning has become an attractive approach to address privacy concerns in collaborative machine learning, for situations when sensitive data is remembered by AI models during the machine learning process. It enables the removal of specific data influences from trained models, aligning with the growing emphasis on the "right to be forgotten." While extensively studied in horizontal federated learning, unlearning in vertical federated learning (VFL) remains challenging due to the distributed feature architecture. VFL unlearning includes sample unlearning that removes specific data points' influence and label unlearning that removes entire classes. Since different parties hold complementary features of the same samples, unlearning tasks require cross-party coordination, creating computational overhead and complexities from feature interdependencies. To address such challenges, we propose FedORA (Federated Optimization for data Removal via primal-dual Algorithm), designed for sample and label unlearning in VFL. FedORA formulates the removal of certain samples or labels as a constrained optimization problem solved using a primal-dual framework. Our approach introduces a new unlearning loss function that promotes classification uncertainty rather than misclassification. An adaptive step size enhances stability, while an asymmetric batch design, considering the prior influence of the remaining data on the model, handles unlearning and retained data differently to efficiently reduce computational costs. We provide theoretical analysis proving that the model difference between FedORA and Train-from-scratch is bounded, establishing guarantees for unlearning effectiveness. Experiments on tabular and image datasets demonstrate that FedORA achieves unlearning effectiveness and utility preservation comparable to Train-from-scratch with reduced computation and communication overhead.

</details>


### [52] [EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion](https://arxiv.org/abs/2512.23173)
*Zhen Liang,Hai Huang,Zhengkui Chen*

Main category: cs.CR

TL;DR: 提出了一种名为Equacode的新型多策略攻击方法，通过数学问题求解和代码补全来针对大型语言模型，实验表明其成功率达到91.19%到98.65%，展示了多策略方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击主要集中在自然语言层面，且依赖单一策略，无法全面评估LLM的鲁棒性。因此，需要一种能更全面评估LLM安全性的新方法。

Method: Equacode方法将恶意意图转化为数学问题，要求LLM通过编程来解决这个问题，以此将模型引导至任务完成而非安全约束。

Result: Equacode在GPT系列模型和3个最先进的LLM上分别取得了91.19%和98.65%的成功率，且只需单次查询。此外，消融实验表明两部分协同作用显著提升了整体效果。

Conclusion: 研究表明，多策略方法可以大幅提高对抗攻击的有效性，对于评估LLM的安全性具有重要意义。

Abstract: Large language models (LLMs), such as ChatGPT, have achieved remarkable success across a wide range of fields. However, their trustworthiness remains a significant concern, as they are still susceptible to jailbreak attacks aimed at eliciting inappropriate or harmful responses. However, existing jailbreak attacks mainly operate at the natural language level and rely on a single attack strategy, limiting their effectiveness in comprehensively assessing LLM robustness. In this paper, we propose Equacode, a novel multi-strategy jailbreak approach for large language models via equation-solving and code completion. This approach transforms malicious intent into a mathematical problem and then requires the LLM to solve it using code, leveraging the complexity of cross-domain tasks to divert the model's focus toward task completion rather than safety constraints. Experimental results show that Equacode achieves an average success rate of 91.19% on the GPT series and 98.65% across 3 state-of-the-art LLMs, all with only a single query. Further, ablation experiments demonstrate that EquaCode outperforms either the mathematical equation module or the code module alone. This suggests a strong synergistic effect, thereby demonstrating that multi-strategy approach yields results greater than the sum of its parts.

</details>


### [53] [Multiparty Authorization for Secure Data Storage in Cloud Environments using Improved Attribute-Based Encryption](https://arxiv.org/abs/2512.23216)
*Partha Paul,Keshav Sinha*

Main category: cs.CR

TL;DR: 该研究提出了一种改进的基于功能流密码的属性基加密（FBSE-ABE）方法，用于云环境中的数据加密和授权，表明该方案具有高性能和强大的安全性。


<details>
  <summary>Details</summary>
Motivation: 当前组织广泛采用云存储技术，但面临数据检索和存储的挑战，如频繁的数据请求增加服务器计算负担和数据泄露风险。为解决这些挑战，研究提出了一种新的访问控制技术，确保数据加密存储和安全访问。

Method: 研究设计了基于功能流密码的属性基加密方案。该方案通过生成和分享授权点来实现多方面授权。使用 Shamir 秘密共享和 2-D Lagrange 插值生成和重构秘密点，并设置了合法用户重构授权密钥的阈值。研究还通过统计分析来评估加密解密性能。

Result: 初步结果显示，新方案能够满足高效存储和授权需求，并在性能测试中表现出色。加密和解密时间与授权策略中的属性数量相关。此外，该方案还能应对碰撞攻击。研究结论表明，该方法在安全性与性能方面均优于现有方案。

Conclusion: 该研究表明，基于功能流密码的属性基加密方案能有效提升云环境中的数据安全性，并能够在实际应用中发挥重要作用。

Abstract: In todays scenario, various organizations store their sensitive data in the cloud environment. Multiple problems are present while retrieving and storing vast amounts of data, such as the frequency of data requests (increasing the computational overhead of the server) and data leakage while storing. To cope with said problem, Attribute-Based Encryption (ABE) is one of the potential security and access control techniques for secure data storage and authorization. The proposed work divides into two objectives: (i) provide access to authorized users and (ii) secure data storage in a cloud environment. The improved ABE using Functional Based Stream Cipher (FBSE) is proposed for data storage. The proposed technique uses simple scalar points over a parabolic curve to provide multiparty authorization. The authorization points are generated and share only with the authorized recipients. The Shamir secret sharing technique generate the authorization points and 2D-Lagrange Interpolation is used to reconstruct the secret points from regular parabola. The proposed scheme has specified the threshold (Ts>3) legally authorized users to reconstruct the attribute-associated keys for decryption. The encryption of data is evaluated using Statistical analysis (NIST Statistical Test Suite, Correlation Coefficient, and Histogram) test to investigate image pixel deviation. The parameters like encryption and decryption are used for performance analysis, where an increase in the number of attributes for the authorization policy will increase the encryption time. The proposed scheme imposes minimal storage overhead, irrespective of the users identity. The security analysis evidence that it resists collision attacks. The security and performance analysis results demonstrate that the proposed scheme is more robust and secure.

</details>


### [54] [Fuzzilicon: A Post-Silicon Microcode-Guided x86 CPU Fuzzer](https://arxiv.org/abs/2512.23438)
*Johannes Lenzen,Mohamadreza Rostami,Lichao Wu,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: Fuzzilicon是一个用于现代x86 CPU后硅阶段漏洞检测的自动化框架，能够发现微架构级别的漏洞并减少测试覆盖收集开销。


<details>
  <summary>Details</summary>
Motivation: 现有CPU存在微架构层面的安全漏洞，但自动化检测仍然困难，Fuzzilicon填补了这一空白，提供了一种新的微代码层面的检测方法。

Method: 通过逆向工程Intel的微代码更新接口，Fuzzilicon开发了一种非侵入性的微代码级仪器化方法，结合虚拟化环境进行精确的反馈引导输入生成。

Result: Fuzzilicon用于Goldmont微架构发现了5个重要发现，其中包括两个全新的微代码级投机执行漏洞，并自动重新发现了先前手动检测的μSpectre类漏洞。Fuzzilicon能将覆盖收集开销减少31倍，并实现了16.27%可挂钩位置的独特微代码覆盖。

Conclusion: Fuzzilicon为复杂的CPU漏洞自动发现提供了一种实用、覆盖导向且可扩展的方法，为后硅模糊测试奠定了新的基础。

Abstract: Modern CPUs are black boxes, proprietary, and increasingly characterized by sophisticated microarchitectural flaws that evade traditional analysis. While some of these critical vulnerabilities have been uncovered through cumbersome manual effort, building an automated and systematic vulnerability detection framework for real-world post-silicon processors remains a challenge.
  In this paper, we present Fuzzilicon, the first post-silicon fuzzing framework for real-world x86 CPUs that brings deep introspection into the microcode and microarchitectural layers. Fuzzilicon automates the discovery of vulnerabilities that were previously only detectable through extensive manual reverse engineering, and bridges the visibility gap by introducing microcode-level instrumentation. At the core of Fuzzilicon is a novel technique for extracting feedback directly from the processor's microarchitecture, enabled by reverse-engineering Intel's proprietary microcode update interface. We develop a minimally intrusive instrumentation method and integrate it with a hypervisor-based fuzzing harness to enable precise, feedback-guided input generation, without access to Register Transfer Level (RTL).
  Applied to Intel's Goldmont microarchitecture, Fuzzilicon introduces 5 significant findings, including two previously unknown microcode-level speculative-execution vulnerabilities. Besides, the Fuzzilicon framework automatically rediscover the $μ$Spectre class of vulnerabilities, which were detected manually in the previous work. Fuzzilicon reduces coverage collection overhead by up to 31$\times$ compared to baseline techniques and achieves 16.27% unique microcode coverage of hookable locations, the first empirical baseline of its kind. As a practical, coverage-guided, and scalable approach to post-silicon fuzzing, Fuzzilicon establishes a new foundation to automate the discovery of complex CPU vulnerabilities.

</details>


### [55] [A Privacy Protocol Using Ephemeral Intermediaries and a Rank-Deficient Matrix Power Function (RDMPF)](https://arxiv.org/abs/2512.23535)
*Eduardo Salazar*

Main category: cs.CR

TL;DR: 本文提出了一种互联网计算机（ICP）的私有传输架构，通过两个短暂的有效证人分离存储和检索。该协议利用非交互式RDMPF封装方法生成单次传输密钥。计算公开的通知提示以支持发现而不指纹用户密钥。检索授权由简短的脱封装证明授权，不泄露身份。所有交易中介都是短暂的，发布认证销毁意图和证明，允许公告板发布审计后的最终记录。设计提供了发送方隐私性（相对于接收方）和内容对中介的保密性，并提供了运输密钥的前向保密性。该设计还验证了生存能力和最终性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过隐私保护的中介分离和非交互式封装技术，提高互联网计算机传输的安全性和隐私性。

Method: 该方法主要依赖于非交互式RDMPF（ReEDS Deterministic Multi-Party Functional Encryption）封装技术来实现一次性的传输密钥，使用密封存储和可信的销毁通知实现安全性。

Result: 实现了对发送者隐私的保护，内容保密性，并提供了前向隐私性。同时，设计还保证了通信的生存能力与最终性。该架构经过了全面测试，并在互联网计算机上进行了实部署。

Conclusion: 该研究设计提供了一种在互联网计算机中实现安全传输的方案，通过正式的形式化接口，确保了传输的安全性和隐私性，为互联网计算机技术提供了重要的参考。

Abstract: This paper presents a private transfer architecture for the Internet Computer (ICP) that decouples deposit and retrieval through two short-lived intermediaries, with sealed storage and attested teardown by an ephemeral witness. The protocol uses a non-interactive RDMPF-based encapsulation to derive per-transfer transport keys. A public notice hint is computed from the capsule to enable discovery without fingerprinting the recipient's key. Retrieval is authorized by a short proof of decapsulation that reveals no identities. All transaction intermediaries are ephemeral and issue certified destruction intents and proofs, allowing a noticeboard to publish auditable finalization records. The design provides sender identity privacy with respect to the recipient, content confidentiality against intermediaries, forward secrecy for transport keys after staged destruction, verifiable liveness and finality. We formalize the basic interfaces, provide the security arguments for encapsulation correctness, hint privacy, authorization soundness and timeout reclaim.
  In terms of implementation, it has been recently brought into production on the ICP under the name ICPP. It has been subject to exhaustive testing and incorporates a few enhancements, focusing on the operational possibilities offered by ICP's technology. This work hence serves as a broad reference for the protocol now publicly accessible.

</details>


### [56] [Enhanced Web Payload Classification Using WAMM: An AI-Based Framework for Dataset Refinement and Model Evaluation](https://arxiv.org/abs/2512.23610)
*Heba Osama,Omar Elebiary,Youssef Qassim,Mohamed Amgad,Ahmed Maghawry,Ahmed Saafan,Haitham Ghalwash*

Main category: cs.CR

TL;DR: WAMM 是一种基于 AI 的多类 Web 攻击检测框架，通过多阶段增强管道对 HTTP 请求进行重新分类，并使用统一特征空间评估了多种模型，表明它在真实场景中的检测效果优于规则基础系统，尤其是在噪声数据下的表现差异。


<details>
  <summary>Details</summary>
Motivation: 传统 WAF 基于静态规则集，对付广泛变化的伪装和零日攻击效果不佳。WAMM 旨在通过通过数据预处理和机器学习模型，提高对这些攻击的检测能力。

Method: WAMM 使用多阶段增强管道对 SR-BH 2020 数据集进行处理，包括大规模去重、由大语言模型指导的重新标签、现实攻击数据增强和基于大语言模型的数据过滤。然后在统一特征空间中评估了 XGBoost、深度学习等模型。

Result: 使用增强和大语言模型过滤的数据集，XGBoost 达到了 99.59% 的准确率，并能在微秒级完成推理。而深度学习模型在噪声数据下表现不佳。WAMM 对未见过的数据集 true positive block 率达到 96% 到 100%，与 OWASP CRS 相比提高了最高 86% 的检测率。

Conclusion: WAMM 暴露了广泛使用的基于规则的防御技术的不足，并展示了精心策划的训练管道和高效机器学习模型在实时 Web 攻击检测中的潜力。

Abstract: Web applications increasingly face evasive and polymorphic attack payloads, yet traditional web application firewalls (WAFs) based on static rule sets such as the OWASP Core Rule Set (CRS) often miss obfuscated or zero-day patterns without extensive manual tuning. This work introduces WAMM, an AI-driven multiclass web attack detection framework designed to reveal the limitations of rule-based systems by reclassifying HTTP requests into OWASP-aligned categories for a specific technology stack. WAMM applies a multi-phase enhancement pipeline to the SR-BH 2020 dataset that includes large-scale deduplication, LLM-guided relabeling, realistic attack data augmentation, and LLM-based filtering, producing three refined datasets. Four machine and deep learning models are evaluated using a unified feature space built from statistical and text-based representations. Results show that using an augmented and LLM-filtered dataset on the same technology stack, XGBoost reaches 99.59% accuracy with microsecond-level inference while deep learning models degrade under noisy augmentation. When tested against OWASP CRS using an unseen augmented dataset, WAMM achieves true positive block rates between 96 and 100% with improvements of up to 86%. These findings expose gaps in widely deployed rule-based defenses and demonstrate that curated training pipelines combined with efficient machine learning models enable a more resilient, real-time approach to web attack detection suitable for production WAF environments.

</details>
