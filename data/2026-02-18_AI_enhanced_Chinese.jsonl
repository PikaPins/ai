{"id": "2602.15143", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15143", "abs": "https://arxiv.org/abs/2602.15143", "authors": ["Xinhang Ma", "William Yeoh", "Ning Zhang", "Yevgeniy Vorobeychik"], "title": "Protecting Language Models Against Unauthorized Distillation through Trace Rewriting", "comment": null, "summary": "Knowledge distillation is a widely adopted technique for transferring capabilities from LLMs to smaller, more efficient student models. However, unauthorized use of knowledge distillation takes unfair advantage of the considerable effort and cost put into developing frontier models. We investigate methods for modifying teacher-generated reasoning traces to achieve two objectives that deter unauthorized distillation: (1) \\emph{anti-distillation}, or degrading the training usefulness of query responses, and (2) \\emph{API watermarking}, which embeds verifiable signatures in student models. We introduce several approaches for dynamically rewriting a teacher's reasoning outputs while preserving answer correctness and semantic coherence. Two of these leverage the rewriting capabilities of LLMs, while others use gradient-based techniques. Our experiments show that a simple instruction-based rewriting approach achieves a strong anti-distillation effect while maintaining or even improving teacher performance. Furthermore, we show that our rewriting approach also enables highly reliable watermark detection with essentially no false alarms.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u4fee\u6539\u6559\u5e08\u751f\u6210\u7684\u63a8\u7406\u75d5\u8ff9\u6765\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u5e76\u63d0\u51fa\u4e86\u51e0\u79cd\u52a8\u6001\u91cd\u5199\u65b9\u6cd5\uff0c\u4ee5\u5728\u4fdd\u6301\u7b54\u6848\u6b63\u786e\u6027\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u53bb\u84b8\u998f\u548cAPI\u6c34\u5370\u76ee\u6807\u3002", "motivation": "\u7531\u4e8e\u77e5\u8bc6\u84b8\u998f\u88ab\u975e\u6388\u6743\u4f7f\u7528\uff0c\u4f1a\u4e0d\u516c\u6b63\u5730\u5229\u7528\u4e86\u53d1\u5e03\u524d\u6cbf\u6a21\u578b\u6240\u9700\u7684\u5de8\u5927\u52aa\u529b\u548c\u6210\u672c\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5f00\u53d1\u6280\u672f\u6765\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u77e5\u8bc6\u84b8\u998f\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u4ee4\u7684\u91cd\u5199\u65b9\u6cd5\uff0c\u5176\u4ed6\u65b9\u6cd5\u5219\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u91cd\u5199\u80fd\u529b\u548c\u68af\u5ea6\u57fa\u6280\u672f\uff0c\u91cd\u65b0\u6784\u5efa\u6559\u5e08\u7684\u63a8\u7406\u8f93\u51fa\uff0c\u540c\u65f6\u4fdd\u6301\u7b54\u6848\u7684\u6b63\u786e\u6027\u548c\u8bed\u4e49\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u57fa\u4e8e\u6307\u4ee4\u7684\u91cd\u5199\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u963b\u6b62\u77e5\u8bc6\u84b8\u998f\uff0c\u6709\u65f6\u751a\u81f3\u8fd8\u80fd\u4fdd\u6301\u548c\u63d0\u9ad8\u6559\u5e08\u6a21\u578b\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u8fd9\u79cd\u65b9\u6cd5\u8fd8\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u5ea6\u53ef\u9760\u7684\u6c34\u5370\u68c0\u6d4b\uff0c\u51e0\u4e4e\u4e0d\u4f1a\u4ea7\u751f\u8bef\u62a5\u3002", "conclusion": "\u603b\u7684\u6765\u8bf4\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u4fdd\u62a4\u9ad8\u8d28\u91cf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u514d\u53d7\u975e\u6388\u6743\u4f7f\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u624b\u6bb5\u3002"}}
{"id": "2602.15161", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15161", "abs": "https://arxiv.org/abs/2602.15161", "authors": ["Mohammad Hadi Foroughi", "Seyed Hamed Rastegar", "Mohammad Sabokrou", "Ahmad Khonsari"], "title": "Exploiting Layer-Specific Vulnerabilities to Backdoor Attack in Federated Learning", "comment": "This paper has been accepted for publication in IEEE ICC 2026", "summary": "Federated learning (FL) enables distributed model training across edge devices while preserving data locality. This decentralized approach has emerged as a promising solution for collaborative learning on sensitive user data, effectively addressing the longstanding privacy concerns inherent in centralized systems. However, the decentralized nature of FL exposes new security vulnerabilities, especially backdoor attacks that threaten model integrity. To investigate this critical concern, this paper presents the Layer Smoothing Attack (LSA), a novel backdoor attack that exploits layer-specific vulnerabilities in neural networks. First, a Layer Substitution Analysis methodology systematically identifies backdoor-critical (BC) layers that contribute most significantly to backdoor success. Subsequently, LSA strategically manipulates these BC layers to inject persistent backdoors while remaining undetected by state-of-the-art defense mechanisms. Extensive experiments across diverse model architectures and datasets demonstrate that LSA achieves a remarkably backdoor success rate of up to 97% while maintaining high model accuracy on the primary task, consistently bypassing modern FL defenses. These findings uncover fundamental vulnerabilities in current FL security frameworks, demonstrating that future defenses must incorporate layer-aware detection and mitigation strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684Layer Smoothing Attack (LSA)\u653b\u51fb\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u5730\u64cd\u7eb5\u5173\u952e\u5c42\u6765\u690d\u5165\u6301\u4e45\u6027\u540e\u95e8\uff0c\u800c\u4e0d\u88ab\u6700\u5148\u8fdb\u7684\u9632\u5fa1\u673a\u5236\u8bc6\u522b\uff0c\u5b9e\u9a8c\u8868\u660eLSA\u5728\u4fdd\u6301\u4e3b\u4efb\u52a1\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u8fbe97%\u7684\u540e\u95e8\u6210\u529f\u7387\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8054\u90a6\u5b66\u4e60\u5b89\u5168\u6846\u67b6\u7684\u6839\u672c\u6f0f\u6d1e\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u901a\u8fc7\u5206\u5e03\u5f0f\u8bbe\u5907\u534f\u540c\u8bad\u7ec3\u6a21\u578b\uff0c\u4fdd\u62a4\u7528\u6237\u6570\u636e\u9690\u79c1\uff0c\u4f46\u540c\u65f6\u9762\u4e34\u65b0\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u5c24\u5176\u662f\u540e\u95e8\u653b\u51fb\u3002\u672c\u6587\u7684\u76ee\u6807\u662f\u53d1\u73b0\u5e76\u5229\u7528\u8fd9\u79cd\u653b\u51fb\u7684\u673a\u5236\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u6297\u73b0\u6709\u9632\u5fa1\u673a\u5236\u7684\u6709\u6548\u6027\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6784\u5efaLayer Substitution Analysis\u65b9\u6cd5\uff0c\u8bc6\u522b\u5bf9\u540e\u95e8\u653b\u51fb\u81f3\u5173\u91cd\u8981\u7684\u5173\u952e\u5c42\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bbe\u8ba1\u4e86LSA\u653b\u51fb\uff0c\u901a\u8fc7\u64cd\u7eb5\u8fd9\u4e9b\u5173\u952e\u5c42\u6765\u690d\u5165\u6301\u4e45\u6027\u540e\u95e8\u3002\u5b9e\u9a8c\u5728\u591a\u79cd\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86LSA\u7684\u6709\u6548\u6027\u3002", "result": "LSA\u653b\u51fb\u5728\u4fdd\u6301\u4e3b\u4efb\u52a1\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u80fd\u591f\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u8fbe97%\u7684\u540e\u95e8\u6210\u529f\u7387\uff0c\u8fdc\u8fdc\u8d85\u8fc7\u4e86\u73b0\u6709\u9632\u5fa1\u673a\u5236\u3002\u8fd9\u8868\u660e\u5f53\u524d\u8054\u90a6\u5b66\u4e60\u7684\u5b89\u5168\u6846\u67b6\u5b58\u5728\u91cd\u5927\u6f0f\u6d1e\u3002", "conclusion": "\u672c\u6587\u53d1\u73b0\u4e86\u8054\u90a6\u5b66\u4e60\u4f53\u7cfb\u7ed3\u6784\u4e2d\u7684\u5173\u952e\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u5c55\u793a\u4e86\u65b0\u7684\u653b\u51fb\u6280\u672f\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u8003\u8651\u5f00\u53d1\u9488\u5bf9\u5c42\u7ea7\u522b\u7684\u68c0\u6d4b\u548c\u7f13\u89e3\u7b56\u7565\uff0c\u589e\u5f3a\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2602.15158", "categories": ["cs.AI", "cs.IR", "math.LO"], "pdf": "https://arxiv.org/pdf/2602.15158", "abs": "https://arxiv.org/abs/2602.15158", "authors": ["Gabriel Rocha"], "title": "da Costa and Tarski meet Goguen and Carnap: a novel approach for ontological heterogeneity based on consequence systems", "comment": "22 pages, 5 figures, 1 table", "summary": "This paper presents a novel approach for ontological heterogeneity that draws heavily from Carnapian-Goguenism, as presented by Kutz, Mossakowski and L\u00fccke (2010). The approach is provisionally designated da Costian-Tarskianism, named after da Costa's Principle of Tolerance in Mathematics and after Alfred Tarski's work on the concept of a consequence operator. The approach is based on the machinery of consequence systems, as developed by Carnielli et al. (2008) and Citkin and Muravitsky (2022), and it introduces the idea of an extended consequence system, which is a consequence system extended with ontological axioms. The paper also defines the concept of an extended development graph, which is a graph structure that allows ontologies to be related via morphisms of extended consequence systems, and additionally via other operations such as fibring and splitting. Finally, we discuss the implications of this approach for the field of applied ontology and suggest directions for future research.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5eb7\u666e-\u9ad8\u6839\u4e3b\u4e49\u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u5f15\u5165\u4e86\u6269\u5c55\u7684\u540e\u679c\u7cfb\u7edf\u548c\u6269\u5c55\u7684\u53d1\u5c55\u56fe\uff0c\u65e8\u5728\u89e3\u51b3\u672c\u4f53\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5728\u5e94\u7528\u672c\u4f53\u8bba\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u672c\u4f53\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u514b\u670d\u73b0\u6709\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u7406\u8bba\u5de5\u5177\u3002", "method": "\u7814\u7a76\u57fa\u4e8e\u5eb7\u666e-\u9ad8\u6839\u4e3b\u4e49\u7406\u8bba\uff0c\u6269\u5c55\u4e86\u540e\u679c\u7cfb\u7edf\uff0c\u5e76\u5f15\u5165\u4e86\u6269\u5c55\u7684\u53d1\u5c55\u56fe\u6765\u8868\u793a\u672c\u4f53\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u63d0\u51fa\u4e86\u65b0\u7684\u672c\u4f53\u8bba\u5de5\u5177\uff0c\u6269\u5c55\u7684\u540e\u679c\u7cfb\u7edf\u548c\u6269\u5c55\u7684\u53d1\u5c55\u56fe\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u8868\u793a\u548c\u64cd\u4f5c\u591a\u4e2a\u672c\u4f53\u5173\u7cfb\u7684\u65b0\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u5bf9\u672a\u6765\u5e94\u7528\u672c\u4f53\u8bba\u7684\u7814\u7a76\u65b9\u5411\u6709\u6240\u542f\u793a\uff0c\u5e76\u5bf9\u672a\u6765\u7684\u5de5\u4f5c\u63d0\u51fa\u4e86\u82e5\u5e72\u5efa\u8bae\u3002"}}
{"id": "2602.15248", "categories": ["cs.AI", "math.OC", "q-fin.MF"], "pdf": "https://arxiv.org/pdf/2602.15248", "abs": "https://arxiv.org/abs/2602.15248", "authors": ["Pavel Koptev", "Vishnu Kumar", "Konstantin Malkov", "George Shapiro", "Yury Vikhanov"], "title": "Predicting Invoice Dilution in Supply Chain Finance with Leakage Free Two Stage XGBoost, KAN (Kolmogorov Arnold Networks), and Ensemble Models", "comment": null, "summary": "Invoice or payment dilution is the gap between the approved invoice amount and the actual collection is a significant source of non credit risk and margin loss in supply chain finance. Traditionally, this risk is managed through the buyer's irrevocable payment undertaking (IPU), which commits to full payment without deductions. However, IPUs can hinder supply chain finance adoption, particularly among sub-invested grade buyers. A newer, data-driven methods use real-time dynamic credit limits, projecting dilution for each buyer-supplier pair in real-time. This paper introduces an AI, machine learning framework and evaluates how that can supplement a deterministic algorithm to predict invoice dilution using extensive production dataset across nine key transaction fields.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408AI\u548c\u673a\u5668\u5b66\u4e60\u6846\u67b6\u4e0e\u786e\u5b9a\u6027\u7b97\u6cd5\u7684\u6a21\u578b\uff0c\u4ee5\u9884\u6d4b\u91c7\u8d2d\u53d1\u7968\u7684\u7a00\u91ca\u60c5\u51b5\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5229\u7528\u4e86\u6db5\u76d6\u4e5d\u4e2a\u5173\u952e\u4ea4\u6613\u5b57\u6bb5\u7684\u5e7f\u6cdb\u751f\u4ea7\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u4e0d\u53ef\u64a4\u9500\u652f\u4ed8\u4fdd\u8bc1\u4e0e\u4f9b\u5e94\u94fe\u91d1\u878d\u91c7\u7eb3\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "motivation": "\u4f20\u7edf\u7684\u4e0d\u53ef\u64a4\u9500\u652f\u4ed8\u4fdd\u8bc1\uff08IPU\uff09\u867d\u7136\u786e\u4fdd\u4e86\u5168\u989d\u4ed8\u6b3e\uff0c\u4f46\u9650\u5236\u4e86\u4f9b\u5e94\u94fe\u91d1\u878d\u7684\u666e\u53ca\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u672c\u6295\u5165\u7ea7\u522b\u7684\u4e70\u5bb6\u3002\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u4f18\u5316\u4fe1\u7528\u989d\u5ea6\uff0c\u5e76\u5b9e\u65f6\u9884\u6d4b\u4e70\u5bb6\u4e0e\u4f9b\u5e94\u5546\u4e4b\u95f4\u7684\u53d1\u7968\u7a00\u91ca\u60c5\u51b5\uff0c\u4ee5\u63d0\u9ad8\u4f9b\u5e94\u94fe\u91d1\u878d\u7684\u6548\u7528\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u4e86\u57fa\u4e8eAI\u548c\u673a\u5668\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5e76\u7ed3\u5408\u4e86\u786e\u5b9a\u6027\u7b97\u6cd5\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5b83\u4f7f\u7528\u4e86\u4e00\u4e2a\u5305\u542b\u4e5d\u4e2a\u5173\u952e\u4ea4\u6613\u5b57\u6bb5\u7684\u5e7f\u6cdb\u751f\u4ea7\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u4ece\u800c\u9884\u6d4b\u53d1\u7968\u7a00\u91ca\u60c5\u51b5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408AI\u548c\u786e\u5b9a\u6027\u7b97\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u9884\u6d4b\u53d1\u7968\u7a00\u91ca\u60c5\u51b5\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u7ba1\u7406\u548c\u51cf\u5c11\u4f9b\u5e94\u94fe\u4e2d\u7684\u975e\u4fe1\u8d37\u98ce\u9669\u548c\u5229\u6da6\u635f\u5931\u65b9\u9762\u3002", "conclusion": "\u672c\u6587\u7684\u7ed3\u8bba\u662f\uff0c\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8eAI\u548c\u673a\u5668\u5b66\u4e60\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u7ba1\u7406\u548c\u51cf\u5c11\u4f9b\u5e94\u94fe\u91d1\u878d\u4e2d\u7684\u975e\u4fe1\u8d37\u98ce\u9669\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u5e2e\u52a9\u91d1\u878d\u673a\u6784\u66f4\u597d\u5730\u8bc4\u4f30\u4fe1\u7528\u98ce\u9669\uff0c\u5e76\u9f13\u52b1\u66f4\u591a\u4e70\u5bb6\u91c7\u7528\u4f9b\u5e94\u94fe\u91d1\u878d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.15323", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15323", "abs": "https://arxiv.org/abs/2602.15323", "authors": ["Huijia Lin", "Kameron Shahabi", "Min Jae Song"], "title": "Unforgeable Watermarks for Language Models via Robust Signatures", "comment": "60 pages, 7 figures", "summary": "Language models now routinely produce text that is difficult to distinguish from human writing, raising the need for robust tools to verify content provenance. Watermarking has emerged as a promising countermeasure, with existing work largely focused on model quality preservation and robust detection. However, current schemes provide limited protection against false attribution. We strengthen the notion of soundness by introducing two novel guarantees: unforgeability and recoverability. Unforgeability prevents adversaries from crafting false positives, texts that are far from any output from the watermarked model but are nonetheless flagged as watermarked. Recoverability provides an additional layer of protection: whenever a watermark is detected, the detector identifies the source text from which the flagged content was derived. Together, these properties strengthen content ownership by linking content exclusively to its generating model, enabling secure attribution and fine-grained traceability. We construct the first undetectable watermarking scheme that is robust, unforgeable, and recoverable with respect to substitutions (i.e., perturbations in Hamming metric). The key technical ingredient is a new cryptographic primitive called robust (or recoverable) digital signatures, which allow verification of messages that are close to signed ones, while preventing forgery of messages that are far from all previously signed messages. We show that any standard digital signature scheme can be boosted to a robust one using property-preserving hash functions (Boyle, LaVigne, and Vaikuntanathan, ITCS 2019).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e0d\u53ef\u68c0\u6d4b\u6c34\u5370\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u57fa\u4e8e\u65b0\u7684\u52a0\u5bc6\u539f\u8bed\u2014\u2014\u9c81\u68d2\u7b7e\u540d\u3002\u8fd9\u4e00\u65b9\u6848\u80fd\u591f\u5728\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u589e\u5f3a\u5185\u5bb9\u771f\u5b9e\u6027\uff0c\u5e76\u901a\u8fc7\u786e\u4fdd\u53ea\u6709\u6765\u81ea\u88ab\u6807\u8bb0\u6a21\u578b\u7684\u6587\u672c\u88ab\u6b63\u786e\u6807\u8bb0\u6765\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u8ffd\u8e2a\u548c\u5b89\u5168\u5f52\u56e0\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4eba\u7c7b\u96be\u4ee5\u533a\u5206\u7684\u6587\u672c\u589e\u591a\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u5de5\u5177\u6765\u9a8c\u8bc1\u5185\u5bb9\u6765\u6e90\u3002\u73b0\u6709\u7684\u6c34\u5370\u65b9\u6848\u5927\u591a\u5173\u6ce8\u4e8e\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u53ca\u9c81\u68d2\u68c0\u6d4b\uff0c\u4f46\u8fd8\u7f3a\u4e4f\u5bf9\u4f2a\u9020\u548c\u6062\u590d\u7684\u4fdd\u969c\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u4e00\u79cd\u65b0\u7684\u52a0\u5bc6\u539f\u8bed\u2014\u2014\u9c81\u68d2\u6570\u5b57\u7b7e\u540d\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u80fd\u591f\u9a8c\u8bc1\u63a5\u8fd1\u5df2\u7b7e\u540d\u4fe1\u606f\u7684\u6761\u76ee\uff0c\u540c\u65f6\u907f\u514d\u5927\u91cf\u504f\u5dee\u7684\u4fe1\u606f\u88ab\u4f2a\u9020\u3002\u4efb\u4f55\u6807\u51c6\u7684\u6570\u5b57\u7b7e\u540d\u65b9\u6848\u90fd\u80fd\u901a\u8fc7\u4e00\u79cd\u79f0\u4e3aBoyle-LaVigne-Vaikuntanathan\u7684\u65b9\u6cd5\u589e\u5f3a\u4e3a\u9c81\u68d2\u7b7e\u540d\u3002", "result": "\u8be5\u6c34\u5370\u65b9\u6848\u662f\u9996\u4e2a\u540c\u65f6\u5177\u5907\u9c81\u68d2\u6027\u3001\u4e0d\u53ef\u4f2a\u9020\u6027\u548c\u53ef\u6062\u590d\u6027\u7684\u65b9\u6848\u3002\u53ef\u4ee5\u5728\u54c8\u59c6\u660e\u8ddd\u79bb\u4e0b\u7684\u7f6e\u6362\u4e2d\u4fdd\u6301\u4e0d\u53ef\u88ab\u68c0\u6d4b\uff0c\u5e76\u4e14\u80fd\u591f\u627e\u5230\u88ab\u6807\u8bb0\u5185\u5bb9\u7684\u539f\u59cb\u6765\u6e90\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u589e\u5f3a\u6c34\u5370\u6280\u672f\u7684\u7279\u6027\uff0c\u63d0\u9ad8\u4e86\u5185\u5bb9\u8bc1\u660e\u7684\u51c6\u786e\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\uff0c\u4e3a\u5185\u5bb9\u7684\u53ef\u4fe1\u4f20\u8fbe\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u5b89\u5168\u4fdd\u969c\u673a\u5236\u3002"}}
{"id": "2602.15294", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15294", "abs": "https://arxiv.org/abs/2602.15294", "authors": ["Ming Du", "Yanqi Luo", "Srutarshi Banerjee", "Michael Wojcik", "Jelena Popovic", "Mathew J. Cherukara"], "title": "EAA: Automating materials characterization with vision language model agents", "comment": null, "summary": "We present Experiment Automation Agents (EAA), a vision-language-model-driven agentic system designed to automate complex experimental microscopy workflows. EAA integrates multimodal reasoning, tool-augmented action, and optional long-term memory to support both autonomous procedures and interactive user-guided measurements. Built on a flexible task-manager architecture, the system enables workflows ranging from fully agent-driven automation to logic-defined routines that embed localized LLM queries. EAA further provides a modern tool ecosystem with two-way compatibility for Model Context Protocol (MCP), allowing instrument-control tools to be consumed or served across applications. We demonstrate EAA at an imaging beamline at the Advanced Photon Source, including automated zone plate focusing, natural language-described feature search, and interactive data acquisition. These results illustrate how vision-capable agents can enhance beamline efficiency, reduce operational burden, and lower the expertise barrier for users.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15395", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.15395", "abs": "https://arxiv.org/abs/2602.15395", "authors": ["Qin Wang", "Ruiqiang Li", "Guangsheng Yu", "Vincent Gramoli", "Shiping Chen"], "title": "MEV in Binance Builder", "comment": null, "summary": "We study the builder-driven MEV arbitrage on BNB Smart Chain (BSC). BSC's Proposer--Builder Separation (PBS) adopts a leaner design: only whitelisted builders can participate, blocks are produced at shorter intervals, and private order flow bypasses the public mempool. These features have long raised community concerns over centralization, which we empirically confirm by tracing arbitrage activity of the two dominant builders from May to November 2025. Within months, 48Club and Blockrazor produced over 96\\% of blocks and captured about 92\\% of MEV profits.\n  We find that profits concentrate in short, low-hop arbitrage routes over wrapped tokens and stablecoins, and that block construction rapidly converges toward monopoly. Beyond concentration alone, our analysis reveals a structural source of inequality: BSC's short block interval and whitelisted PBS collapse the contestable window for MEV competition, amplifying latency advantages and excluding slower builders and searchers. MEV extraction on BSC is not only more centralized than on Ethereum, but also structurally more vulnerable to censorship and weakened fairness.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15485", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.15485", "abs": "https://arxiv.org/abs/2602.15485", "authors": ["Longfei Chen", "Ji Zhao", "Lanxiao Cui", "Tong Su", "Xingbo Pan", "Ziyang Li", "Yongxing Wu", "Qijiang Cao", "Qiyao Cai", "Jing Zhang", "Yuandong Ni", "Junyao He", "Zeyu Zhang", "Chao Ge", "Xuhuai Lu", "Zeyu Gao", "Yuxin Cui", "Weisen Chen", "Yuxuan Peng", "Shengping Wang", "Qi Li", "Yukai Huang", "Yukun Liu", "Tuo Zhou", "Terry Yue Zhuo", "Junyang Lin", "Chao Zhang"], "title": "SecCodeBench-V2 Technical Report", "comment": null, "summary": "We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and Node.js. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at https://alibaba.github.io/sec-code-bench. The benchmark is publicly available at https://github.com/alibaba/sec-code-bench.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15325", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15325", "abs": "https://arxiv.org/abs/2602.15325", "authors": ["Zhixing Zhang", "Jesen Zhang", "Hao Liu", "Qinhan Lv", "Jing Yang", "Kaitong Cai", "Keze Wang"], "title": "AgriWorld:A World Tools Protocol Framework for Verifiable Agricultural Reasoning with Code-Executing LLM Agents", "comment": null, "summary": "Foundation models for agriculture are increasingly trained on massive spatiotemporal data (e.g., multi-spectral remote sensing, soil grids, and field-level management logs) and achieve strong performance on forecasting and monitoring. However, these models lack language-based reasoning and interactive capabilities, limiting their usefulness in real-world agronomic workflows. Meanwhile, large language models (LLMs) excel at interpreting and generating text, but cannot directly reason over high-dimensional, heterogeneous agricultural datasets. We bridge this gap with an agentic framework for agricultural science. It provides a Python execution environment, AgriWorld, exposing unified tools for geospatial queries over field parcels, remote-sensing time-series analytics, crop growth simulation, and task-specific predictors (e.g., yield, stress, and disease risk). On top of this environment, we design a multi-turn LLM agent, Agro-Reflective, that iteratively writes code, observes execution results, and refines its analysis via an execute-observe-refine loop. We introduce AgroBench, with scalable data generation for diverse agricultural QA spanning lookups, forecasting, anomaly detection, and counterfactual \"what-if\" analysis. Experiments outperform text-only and direct tool-use baselines, validating execution-driven reflection for reliable agricultural reasoning.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15614", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.15614", "abs": "https://arxiv.org/abs/2602.15614", "authors": ["Yasmine Hayder", "Adrien Boiret", "C\u00e9dric Eichler", "Benjamin Nguyen"], "title": "Onto-DP: Constructing Neighborhoods for Differential Privacy on Ontological Databases", "comment": null, "summary": "In this paper, we investigate how attackers can discover sensitive information embedded within databases by exploiting inference rules. We demonstrate the inadequacy of naively applied existing state of the art differential privacy (DP) models in safeguarding against such attacks. We introduce ontology aware differential privacy (Onto-DP), a novel extension of differential privacy paradigms built on top of any classical DP model by enriching it with semantic awareness. We show that this extension is a sufficient condition to adequately protect against attackers aware of inference rules.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15384", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15384", "abs": "https://arxiv.org/abs/2602.15384", "authors": ["Zhouzhou Shen", "Xueyu Hu", "Xiyun Li", "Tianqing Fang", "Juncheng Li", "Shengyu Zhang"], "title": "World-Model-Augmented Web Agents with Action Correction", "comment": null, "summary": "Web agents based on large language models have demonstrated promising capability in automating web tasks. However, current web agents struggle to reason out sensible actions due to the limitations of predicting environment changes, and might not possess comprehensive awareness of execution risks, prematurely performing risky actions that cause losses and lead to task failure. To address these challenges, we propose WAC, a web agent that integrates model collaboration, consequence simulation, and feedback-driven action refinement. To overcome the cognitive isolation of individual models, we introduce a multi-agent collaboration process that enables an action model to consult a world model as a web-environment expert for strategic guidance; the action model then grounds these suggestions into executable actions, leveraging prior knowledge of environmental state transition dynamics to enhance candidate action proposal. To achieve risk-aware resilient task execution, we introduce a two-stage deduction chain. A world model, specialized in environmental state transitions, simulates action outcomes, which a judge model then scrutinizes to trigger action corrective feedback when necessary. Experiments show that WAC achieves absolute gains of 1.8% on VisualWebArena and 1.3% on Online-Mind2Web.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15654", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15654", "abs": "https://arxiv.org/abs/2602.15654", "authors": ["Xianglin Yang", "Yufei He", "Shuo Ji", "Bryan Hooi", "Jin Song Dong"], "title": "Zombie Agents: Persistent Control of Self-Evolving LLM Agents via Self-Reinforcing Injections", "comment": null, "summary": "Self-evolving LLM agents update their internal state across sessions, often by writing and reusing long-term memory. This design improves performance on long-horizon tasks but creates a security risk: untrusted external content observed during a benign session can be stored as memory and later treated as instruction. We study this risk and formalize a persistent attack we call a Zombie Agent, where an attacker covertly implants a payload that survives across sessions, effectively turning the agent into a puppet of the attacker.\n  We present a black-box attack framework that uses only indirect exposure through attacker-controlled web content. The attack has two phases. During infection, the agent reads a poisoned source while completing a benign task and writes the payload into long-term memory through its normal update process. During trigger, the payload is retrieved or carried forward and causes unauthorized tool behavior. We design mechanism-specific persistence strategies for common memory implementations, including sliding-window and retrieval-augmented memory, to resist truncation and relevance filtering. We evaluate the attack on representative agent setups and tasks, measuring both persistence over time and the ability to induce unauthorized actions while preserving benign task quality. Our results show that memory evolution can convert one-time indirect injection into persistent compromise, which suggests that defenses focused only on per-session prompt filtering are not sufficient for self-evolving agents.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15391", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15391", "abs": "https://arxiv.org/abs/2602.15391", "authors": ["Ankit Sharma", "Nachiket Tapas", "Jyotiprakash Patra"], "title": "Improving LLM Reliability through Hybrid Abstention and Adaptive Detection", "comment": null, "summary": "Large Language Models (LLMs) deployed in production environments face a fundamental safety-utility trade-off either a strict filtering mechanisms prevent harmful outputs but often block benign queries or a relaxed controls risk unsafe content generation. Conventional guardrails based on static rules or fixed confidence thresholds are typically context-insensitive and computationally expensive, resulting in high latency and degraded user experience. To address these limitations, we introduce an adaptive abstention system that dynamically adjusts safety thresholds based on real-time contextual signals such as domain and user history. The proposed framework integrates a multi-dimensional detection architecture composed of five parallel detectors, combined through a hierarchical cascade mechanism to optimize both speed and precision. The cascade design reduces unnecessary computation by progressively filtering queries, achieving substantial latency improvements compared to non-cascaded models and external guardrail systems. Extensive evaluation on mixed and domain-specific workloads demonstrates significant reductions in false positives, particularly in sensitive domains such as medical advice and creative writing. The system maintains high safety precision and near-perfect recall under strict operating modes. Overall, our context-aware abstention framework effectively balances safety and utility while preserving performance, offering a scalable solution for reliable LLM deployment.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15671", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.15671", "abs": "https://arxiv.org/abs/2602.15671", "authors": ["Haodong Zhao", "Jinming Hu", "Gongshen Liu"], "title": "Revisiting Backdoor Threat in Federated Instruction Tuning from a Signal Aggregation Perspective", "comment": "Accepted by ICASSP 2026", "summary": "Federated learning security research has predominantly focused on backdoor threats from a minority of malicious clients that intentionally corrupt model updates. This paper challenges this paradigm by investigating a more pervasive and insidious threat: \\textit{backdoor vulnerabilities from low-concentration poisoned data distributed across the datasets of benign clients.} This scenario is increasingly common in federated instruction tuning for language models, which often rely on unverified third-party and crowd-sourced data. We analyze two forms of backdoor data through real cases: 1) \\textit{natural trigger (inherent features as implicit triggers)}; 2) \\textit{adversary-injected trigger}. To analyze this threat, we model the backdoor implantation process from signal aggregation, proposing the Backdoor Signal-to-Noise Ratio to quantify the dynamics of the distributed backdoor signal. Extensive experiments reveal the severity of this threat: With just less than 10\\% of training data poisoned and distributed across clients, the attack success rate exceeds 85\\%, while the primary task performance remains largely intact. Critically, we demonstrate that state-of-the-art backdoor defenses, designed for attacks from malicious clients, are fundamentally ineffective against this threat. Our findings highlight an urgent need for new defense mechanisms tailored to the realities of modern, decentralized data ecosystems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15403", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15403", "abs": "https://arxiv.org/abs/2602.15403", "authors": ["Thomas \u00c5gotnes"], "title": "Common Belief Revisited", "comment": null, "summary": "Contrary to common belief, common belief is not KD4.\n  If individual belief is KD45, common belief does indeed lose the 5 property and keep the D and 4 properties -- and it has none of the other commonly considered properties of knowledge and belief. But it has another property: $C(C\u03c6\\rightarrow \u03c6)$ -- corresponding to so-called shift-reflexivity (reflexivity one step ahead). This observation begs the question:\n  is KD4 extended with this axiom a complete characterisation of common belief in the KD45 case? If not, what \\emph{is} the logic of common belief? In this paper we show that the answer to the first question is ``no'': there is one additional axiom, and, furthermore, it relies on the number of agents. We show that the result is a complete characterisation of common belief, settling the open problem.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15705", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.15705", "abs": "https://arxiv.org/abs/2602.15705", "authors": ["Saleh Darzia", "G\u00f6kcan Cantalib", "Attila Altay Yavuza", "G\u00fcrkan G\u00fcr"], "title": "Privacy-Preserving and Secure Spectrum Sharing for Database-Driven Cognitive Radio Networks", "comment": "19 pages, 13 figures, 5 tables", "summary": "Database-driven cognitive radio networks (DB-CRNs) enable dynamic spectrum sharing through geolocation databases but introduce critical security and privacy challenges, including mandatory location disclosure, susceptibility to location spoofing, and denial-of-service (DoS) attacks on centralized services. Existing approaches address these issues in isolation and lack a unified, regulation-compliant solution under realistic adversarial conditions. In this work, we present a unified security framework for DB-CRNs that simultaneously provides location privacy, user anonymity, verifiable location, and DoS resilience. Our framework, denoted as SLAPX, enables privacy-preserving spectrum queries using delegatable anonymous credentials, supports adaptive location verification without revealing precise user location, and mitigates DoS attacks through verifiable delay functions (VDFs) combined with RLRS-based rate limiting. Extensive cryptographic benchmarking and network simulations demonstrate that SLAPX achieves significantly lower latency and communication overhead than existing solutions while effectively resisting location spoofing and DoS attacks. These results show that SLAPX is practical and well-suited for secure next-generation DB-CRN deployments.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15531", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.15531", "abs": "https://arxiv.org/abs/2602.15531", "authors": ["Javier Irigoyen", "Roberto Daza", "Aythami Morales", "Julian Fierrez", "Francisco Jurado", "Alvaro Ortigosa", "Ruben Tolosana"], "title": "GenAI-LA: Generative AI and Learning Analytics Workshop (LAK 2026), April 27--May 1, 2026, Bergen, Norway", "comment": "10 pages, 3 figures. Published in Intl. Conf. on Learning Analytics & Knowledge Workshops (LAK Workshops 2026, GenAI-LA 26)", "summary": "This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations are annotated with binary risk labels through a semi-automatic process with expert teacher review. Finally, we present preliminary validation experiments to assess the suitability of EduEVAL-DB for evaluation. We benchmark a state-of-the-art education-oriented model (Gemini 2.5 Pro) against a lightweight local Llama 3.1 8B model and examine whether supervised fine-tuning on EduEVAL-DB supports pedagogical risk detection using models deployable on consumer hardware.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15756", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15756", "abs": "https://arxiv.org/abs/2602.15756", "authors": ["Or Zamir"], "title": "A Note on Non-Composability of Layerwise Approximate Verification for Neural Inference", "comment": null, "summary": "A natural and informal approach to verifiable (or zero-knowledge) ML inference over floating-point data is: ``prove that each layer was computed correctly up to tolerance $\u03b4$; therefore the final output is a reasonable inference result''. This short note gives a simple counterexample showing that this inference is false in general: for any neural network, we can construct a functionally equivalent network for which adversarially chosen approximation-magnitude errors in individual layer computations suffice to steer the final output arbitrarily (within a prescribed bounded range).", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15532", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15532", "abs": "https://arxiv.org/abs/2602.15532", "authors": ["Ryan Othniel Kearns"], "title": "Quantifying construct validity in large language model evaluations", "comment": null, "summary": "The LLM community often reports benchmark results as if they are synonymous with general model capabilities. However, benchmarks can have problems that distort performance, like test set contamination and annotator error. How can we know that a benchmark is a reliable indicator of some capability that we want to measure? This question concerns the construct validity of LLM benchmarks, and it requires separating benchmark results from capabilities when we model and predict LLM performance.\n  Both social scientists and computer scientists propose formal models - latent factor models and scaling laws - for identifying the capabilities underlying benchmark scores. However, neither technique is satisfactory for construct validity. Latent factor models ignore scaling laws, and as a result, the capabilities they extract often proxy model size. Scaling laws ignore measurement error, and as a result, the capabilities they extract are both uninterpretable and overfit to the observed benchmarks.\n  This thesis presents the structured capabilities model, the first model to extract interpretable and generalisable capabilities from a large collection of LLM benchmark results. I fit this model and its two alternatives on a large sample of results from the OpenLLM Leaderboard. Structured capabilities outperform latent factor models on parsimonious fit indices, and exhibit better out-of-distribution benchmark prediction than scaling laws. These improvements are possible because neither existing approach separates model scale from capabilities in the appropriate way. Model scale should inform capabilities, as in scaling laws, and these capabilities should inform observed results up to measurement error, as in latent factor models. In combining these two insights, structured capabilities demonstrate better explanatory and predictive power for quantifying construct validity in LLM evaluations.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15815", "categories": ["cs.CR", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.15815", "abs": "https://arxiv.org/abs/2602.15815", "authors": ["Matthew Regehr", "Bingshan Hu", "Ethan Leeman", "Pasin Manurangsi", "Pierre Tholoniat", "Mathias L\u00e9cuyer"], "title": "Natural Privacy Filters Are Not Always Free: A Characterization of Free Natural Filters", "comment": null, "summary": "We study natural privacy filters, which enable the exact composition of differentially private (DP) mechanisms with adaptively chosen privacy characteristics. Earlier privacy filters consider only simple privacy parameters such as R\u00e9nyi-DP or Gaussian DP parameters. Natural filters account for the entire privacy profile of every query, promising greater utility for a given privacy budget. We show that, contrary to other forms of DP, natural privacy filters are not free in general. Indeed, we show that only families of privacy mechanisms that are well-ordered when composed admit free natural privacy filters.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15553", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15553", "abs": "https://arxiv.org/abs/2602.15553", "authors": ["Gabriele Conte", "Alessio Mattiace", "Gianni Carmosino", "Potito Aghilar", "Giovanni Servedio", "Francesco Musicco", "Vito Walter Anelli", "Tommaso Di Noia", "Francesco Maria Donini"], "title": "RUVA: Personalized Transparent On-Device Graph Reasoning", "comment": null, "summary": "The Personal AI landscape is currently dominated by \"Black Box\" Retrieval-Augmented Generation. While standard vector databases offer statistical matching, they suffer from a fundamental lack of accountability: when an AI hallucinates or retrieves sensitive data, the user cannot inspect the cause nor correct the error. Worse, \"deleting\" a concept from a vector space is mathematically imprecise, leaving behind probabilistic \"ghosts\" that violate true privacy. We propose Ruva, the first \"Glass Box\" architecture designed for Human-in-the-Loop Memory Curation. Ruva grounds Personal AI in a Personal Knowledge Graph, enabling users to inspect what the AI knows and to perform precise redaction of specific facts. By shifting the paradigm from Vector Matching to Graph Reasoning, Ruva ensures the \"Right to be Forgotten.\" Users are the editors of their own lives; Ruva hands them the pen. The project and the demo video are available at http://sisinf00.poliba.it/ruva/.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15580", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15580", "abs": "https://arxiv.org/abs/2602.15580", "authors": ["Hongxuan Wu", "Yukun Zhang", "Xueqing Zhou"], "title": "How Vision Becomes Language: A Layer-wise Information-Theoretic Analysis of Multimodal Reasoning", "comment": null, "summary": "When a multimodal Transformer answers a visual question, is the prediction driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation -- and how does this structure evolve across layers? We address this question with a layer-wise framework based on Partial Information Decomposition (PID) that decomposes the predictive information at each Transformer layer into redundant, vision-unique, language-unique, and synergistic components. To make PID tractable for high-dimensional neural representations, we introduce \\emph{PID Flow}, a pipeline combining dimensionality reduction, normalizing-flow Gaussianization, and closed-form Gaussian PID estimation. Applying this framework to LLaVA-1.5-7B and LLaVA-1.6-7B across six GQA reasoning tasks, we uncover a consistent \\emph{modal transduction} pattern: visual-unique information peaks early and decays with depth, language-unique information surges in late layers to account for roughly 82\\% of the final prediction, and cross-modal synergy remains below 2\\%. This trajectory is highly stable across model variants (layer-wise correlations $>$0.96) yet strongly task-dependent, with semantic redundancy governing the detailed information fingerprint. To establish causality, we perform targeted Image$\\rightarrow$Question attention knockouts and show that disrupting the primary transduction pathway induces predictable increases in trapped visual-unique information, compensatory synergy, and total information cost -- effects that are strongest in vision-dependent tasks and weakest in high-redundancy tasks. Together, these results provide an information-theoretic, causal account of how vision becomes language in multimodal Transformers, and offer quantitative guidance for identifying architectural bottlenecks where modality-specific information is lost.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15635", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15635", "abs": "https://arxiv.org/abs/2602.15635", "authors": ["Konstantin Sidorov"], "title": "On inferring cumulative constraints", "comment": "17 pages, 6 figures, 4 tables; submitted to the 32nd International Conference on Principles and Practice of Constraint Programming (CP 2026)", "summary": "Cumulative constraints are central in scheduling with constraint programming, yet propagation is typically performed per constraint, missing multi-resource interactions and causing severe slowdowns on some benchmarks. I present a preprocessing method for inferring additional cumulative constraints that capture such interactions without search-time probing. This approach interprets cumulative constraints as linear inequalities over occupancy vectors and generates valid inequalities by (i) discovering covers, the sets of tasks that cannot run in parallel, (ii) strengthening the cover inequalities for the discovered sets with lifting, and (iii) injecting the resulting constraints back into the scheduling problem instance. Experiments on standard RCPSP and RCPSP/max test suites show that these inferred constraints improve search performance and tighten objective bounds on favorable instances, while incurring little degradation on unfavorable ones. Additionally, these experiments discover 25 new lower bounds and five new best solutions; eight of the lower bounds are obtained directly from the inferred constraints.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15645", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.15645", "abs": "https://arxiv.org/abs/2602.15645", "authors": ["Lucas Elbert Suryana", "Farah Bierenga", "Sanne van Buuren", "Pepijn Kooij", "Elsefien Tulleners", "Federico Scari", "Simeon Calvert", "Bart van Arem", "Arkady Zgonnikov"], "title": "CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving", "comment": "21 pages, on submission to Transportation Research Part C", "summary": "Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive decision making or merely post hoc rationalizations. This limitation is especially significant in safety critical domains because it can create false confidence. To address this gap, we propose CARE Drive, Context Aware Reasons Evaluation for Driving, a model agnostic framework for evaluating reason responsiveness in vision language models applied to automated driving. CARE Drive compares baseline and reason augmented model decisions under controlled contextual variation to assess whether human reasons causally influence decision behavior. The framework employs a two stage evaluation process. Prompt calibration ensures stable outputs. Systematic contextual perturbation then measures decision sensitivity to human reasons such as safety margins, social pressure, and efficiency constraints. We demonstrate CARE Drive in a cyclist overtaking scenario involving competing normative considerations. Results show that explicit human reasons significantly influence model decisions, improving alignment with expert recommended behavior. However, responsiveness varies across contextual factors, indicating uneven sensitivity to different types of reasons. These findings provide empirical evidence that reason responsiveness in foundation models can be systematically evaluated without modifying model parameters.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15669", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15669", "abs": "https://arxiv.org/abs/2602.15669", "authors": ["Xiachong Feng", "Liang Zhao", "Weihong Zhong", "Yichong Huang", "Yuxuan Gu", "Lingpeng Kong", "Xiaocheng Feng", "Bing Qin"], "title": "PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra", "comment": "ICLR 2026", "summary": "Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9.60, nearly matching the supervised fine-tuning upper bound of 9.61 without any gradient updates. On our proposed Persona-Evolve benchmark for dynamic personality adaptation, we achieve up to 91% win rates across diverse model families. These results provide evidence that aspects of LLM personality are mathematically tractable, opening new directions for interpretable and efficient behavioral control.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15725", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15725", "abs": "https://arxiv.org/abs/2602.15725", "authors": ["Sarim Chaudhry"], "title": "Recursive Concept Evolution for Compositional Reasoning in Large Language Models", "comment": null, "summary": "Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15776", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15776", "abs": "https://arxiv.org/abs/2602.15776", "authors": ["Yiqin Yang", "Xu Yang", "Yuhua Jiang", "Ni Mu", "Hao Hu", "Runpeng Xie", "Ziyou Zhang", "Siyuan Li", "Yuan-Hua Ni", "Qianchuan Zhao", "Bo Xu"], "title": "GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent Systems", "comment": null, "summary": "In the realm of multi-agent systems, the challenge of \\emph{partial observability} is a critical barrier to effective coordination and decision-making. Existing approaches, such as belief state estimation and inter-agent communication, often fall short. Belief-based methods are limited by their focus on past experiences without fully leveraging global information, while communication methods often lack a robust model to effectively utilize the auxiliary information they provide. To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to infer the global state based on the local observations. By formulating the state inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities in state estimation while simultaneously inferring the global state with high fidelity. We prove that the estimation error of GlobeDiff under both unimodal and multi-modal distributions can be bounded. Extensive experimental results demonstrate that GlobeDiff achieves superior performance and is capable of accurately inferring the global state.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15785", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15785", "abs": "https://arxiv.org/abs/2602.15785", "authors": ["Jessica Hullman", "David Broska", "Huaman Sun", "Aaron Shaw"], "title": "This human study did not involve human subjects: Validating LLM simulations as behavioral evidence", "comment": null, "summary": "A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15791", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15791", "abs": "https://arxiv.org/abs/2602.15791", "authors": ["Suhyung Jang", "Ghang Lee", "Jaekun Lee", "Hyunjun Lee"], "title": "Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings", "comment": "42nd International Symposium on Automation and Robotics in Construction (ISARC 2025)", "summary": "Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI's ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.15816", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.15816", "abs": "https://arxiv.org/abs/2602.15816", "authors": ["Xiaoran Liu", "Istvan David"], "title": "Developing AI Agents with Simulated Data: Why, what, and how?", "comment": null, "summary": "As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
