<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.CR](#cs.CR) [Total: 11]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Do You Trust Me? Cognitive-Affective Signatures of Trustworthiness in Large Language Models](https://arxiv.org/abs/2601.10719)
*Gerard Yeo,Svetlana Churina,Kokil Jaidka*

Main category: cs.AI

TL;DR: 研究通过分析大型语言模型在类似网页的叙述中如何编码感知的可信度，发现模型在预训练过程中隐式地学习了关于公正是、确定性和责任感的人类在线信任形成的维度的信号，这些信号有助于设计可信赖、透明和值得信赖的AI系统。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLMs）是否在心理上以一致的方式呈现感知的可信度，以了解其在搜索引擎、推荐系统和对话系统中的作用。

Method: 使用PEACE-Reviews数据集注释的心理认知评估、情感和行为意图对指令调整后的LLMs（Llama 3.1 8B、Qwen 2.5 7B、Mistral 7B）进行分析，通过探针分析评估信任信号。

Result: 展示了层级和头级别激活差异如何区分高可信度和低可信度文本，揭示了可信度提示在预训练期间隐式编码；探针分析显示线性可解的信任信号和微调效果，这些效果细化而不是重新构建这些表示。

Conclusion: 实验证明，现代大型语言模型在没有显式监督的情况下以内隐的方式学习了关于公平、确定性和责任感的人类在线信任形成的心理基础信号，为设计可信赖、透明和值得信赖的AI系统提供了代表性的基础。

Abstract: Perceived trustworthiness underpins how users navigate online information, yet it remains unclear whether large language models (LLMs),increasingly embedded in search, recommendation, and conversational systems, represent this construct in psychologically coherent ways. We analyze how instruction-tuned LLMs (Llama 3.1 8B, Qwen 2.5 7B, Mistral 7B) encode perceived trustworthiness in web-like narratives using the PEACE-Reviews dataset annotated for cognitive appraisals, emotions, and behavioral intentions. Across models, systematic layer- and head-level activation differences distinguish high- from low-trust texts, revealing that trust cues are implicitly encoded during pretraining. Probing analyses show linearly de-codable trust signals and fine-tuning effects that refine rather than restructure these representations. Strongest associations emerge with appraisals of fairness, certainty, and accountability-self -- dimensions central to human trust formation online. These findings demonstrate that modern LLMs internalize psychologically grounded trust signals without explicit supervision, offering a representational foundation for designing credible, transparent, and trust-worthy AI systems in the web ecosystem. Code and appendix are available at: https://github.com/GerardYeo/TrustworthinessLLM.

</details>


### [2] [Optimisation of complex product innovation processes based on trend models with three-valued logic](https://arxiv.org/abs/2601.10768)
*Nina Bočková,Barbora Volná,Mirko Dohnal*

Main category: cs.AI

TL;DR: 本文研究了基于一集启发式规则的复杂产品创新过程，通过简单趋势（增、减、稳）定义模型解，将可能的过渡情景用转换图表示，描绘系统的行为变化。


<details>
  <summary>Details</summary>
Motivation: 当前产品创新过程复杂，需要一种更简洁的建模方法，而基于启发式规则和趋势的方法提供了一种避免依赖具体数值或粗糙集的量化手段。

Method: 该研究采用启发式规则和简单趋势来建模产品创新过程，定义了解为可能的场景过渡集，并通过转换图表示这些过渡。

Result: 该方法可以简洁地描绘系统在未来或过去的各种可能出现的行为状态。

Conclusion: 这种方法提供了一种新的视角来理解复杂的产品创新过程，通过简单的趋势模型和启发式规则，更加直观地展示系统的动态变化。

Abstract: This paper investigates complex product-innovation processes using models grounded in a set of heuristics. Each heuristic is expressed through simple trends -- increasing, decreasing, or constant -- which serve as minimally information-intensive quantifiers, avoiding reliance on numerical values or rough sets. A solution to a trend model is defined as a set of scenarios with possible transitions between them, represented by a transition graph. Any possible future or past behaviour of the system under study can thus be depicted by a path within this graph.

</details>


### [3] [ARC Prize 2025: Technical Report](https://arxiv.org/abs/2601.10904)
*François Chollet,Mike Knoop,Gregory Kamradt,Bryan Landers*

Main category: cs.AI

TL;DR: 本文综述了顶级方法，探讨了在AGI进展中精炼回路的作用，讨论了知识依赖性过拟合，并预览了ARC-AGI-3基准，该版本引入了互动推理挑战。


<details>
  <summary>Details</summary>
Motivation: 随着ARC-AGI-2数据集的发布，该研究旨在评估AI在新颖任务上的少样本泛化能力，特别是关注精炼回路在AGI发展中的作用。

Method: 该研究通过分析和比较竞赛中的顶级方法，以及讨论未来基准的要求，来探索精炼回路在AGI进展中的角色，特别是知识依赖性过拟合。

Result: 研究结果显示，当前的人工智能在推理上的进步主要受限于知识覆盖范围，但零预训练深度学习方法已经能够在较小的模型大小中达到与传统方法相当的性能。此外，研究引入了ARC-AGI-3，这一新的基准挑战需要探索、规划、记忆、目标获取和对齐能力。

Conclusion: 该研究强调了需要新的基准来测试和评估AGI，特别是在包含互动推理挑战的新ARC-AGI-3版本中，这些挑战需要高级的推理能力来解决。

Abstract: The ARC-AGI benchmark series serves as a critical measure of few-shot generalization on novel tasks, a core aspect of intelligence. The ARC Prize 2025 global competition targeted the newly released ARC-AGI-2 dataset, which features greater task complexity compared to its predecessor. The Kaggle competition attracted 1,455 teams and 15,154 entries, with the top score reaching 24% on the ARC-AGI-2 private evaluation set. Paper submissions nearly doubled year-over-year to 90 entries, reflecting the growing research interest in fluid intelligence and abstract reasoning. The defining theme of 2025 is the emergence of the refinement loop -- a per-task iterative program optimization loop guided by a feedback signal. Refinement loops come in a variety of forms, in particular evolutionary program synthesis approaches and application-layer refinements to commercial AI systems. Such refinement loops are also possible in weight space, as evidenced by zero-pretraining deep learning methods which are now achieving competitive performance with remarkably small networks (7M parameters). In parallel, four frontier AI labs (Anthropic, Google DeepMind, OpenAI, and xAI) reported ARC-AGI performance in public model cards in 2025, establishing ARC-AGI as an industry standard benchmark for AI reasoning. However, our analysis indicates that current frontier AI reasoning performance remains fundamentally constrained to knowledge coverage, giving rise to new forms of benchmark contamination. In this paper, we survey the top-performing methods, examine the role of refinement loops in AGI progress, discuss knowledge-dependent overfitting, and preview ARC-AGI-3, which introduces interactive reasoning challenges that require exploration, planning, memory, goal acquisition, and alignment capabilities.

</details>


### [4] [What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge](https://arxiv.org/abs/2601.10922)
*Yosub Shin,Michael Buriek,Boris Sobolev,Pavel Bushuyeu,Vikas Kumar,Haoyang Xu,Samuel Watson,Igor Molybog*

Main category: cs.AI

TL;DR: 本研究通过NeurIPS 2025 视觉-语言推理数据整理挑战，展示了难度驱动的样本选择对数据整理过程的重要性，并探讨了数据集大小、多样性增强和合成增强的效果。


<details>
  <summary>Details</summary>
Motivation: 为了通过固定模型和训练协议评估不同的数据集选择策略，并深入理解在数据受限条件下数据整理的效果。

Method: 使用了来自Walton Multimodal Cold Start的紧凑型整理数据集，并通过后竞赛分析，比较了不同策略下的性能差异。

Result: 难度驱动的样本选择策略是性能提升的主要因素。大规模数据集对平均准确率无显著提升，但能减少运行间差异。常用的数据多样性增强和合成增强并没有提供额外帮助，有时甚至会降低性能。

Conclusion: DCVLR 挑战表明，数据整理处于饱和阶段，强调了数据整理中一致性和难度的重要性。

Abstract: We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.

</details>


### [5] [Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics](https://arxiv.org/abs/2601.11012)
*Jiahao Wang,Shuangjia Zheng*

Main category: cs.AI

TL;DR: HADES是一种利用贝叶斯优化方法和哈密顿动力学来高效探索结构感知近似后验的蛋白质优化技术。这种方法能够快速向有希望的区域提出提议，并在模拟物理运动中利用动量和不确定性。双方编码解码框架用于确定突变邻居之间的结构和功能关系，从而学习一种平滑的采样景观。HADES在多种指标上优于最先进的基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于序列的优化方法难以应对高维度复杂性及忽视结构约束的问题，而HADES则通过整合结构和序列关系来优化蛋白质，为生物技术和医学领域的工程提供新的可能性。

Method: HADES采用贝叶斯优化方法，并利用哈密顿动力学高效探索结构敏感的近似后验。通过动量和不确定性增强模拟物理运动过程，引入位置离散化技术以提出离散的蛋白质序列，并使用两阶段编码解码框架来构建结构和功能关系模型。

Result: HADES在多种指标上优于现有的先进方法。实验结果展示了HADES在蛋白质优化中的优势，尤其是在结构感知优化方面。

Conclusion: HADES为优化蛋白质序列提供了改进策略，尤其在整合结构和序列关系方面表现出色，展示了其在生物技术和医学工程中的潜在应用价值。相关代码和数据可以在公开仓库中获取。

Abstract: The ability to engineer optimized protein variants has transformative potential for biotechnology and medicine. Prior sequence-based optimization methods struggle with the high-dimensional complexities due to the epistasis effect and the disregard for structural constraints. To address this, we propose HADES, a Bayesian optimization method utilizing Hamiltonian dynamics to efficiently sample from a structure-aware approximated posterior. Leveraging momentum and uncertainty in the simulated physical movements, HADES enables rapid transition of proposals toward promising areas. A position discretization procedure is introduced to propose discrete protein sequences from such a continuous state system. The posterior surrogate is powered by a two-stage encoder-decoder framework to determine the structure and function relationships between mutant neighbors, consequently learning a smoothed landscape to sample from. Extensive experiments demonstrate that our method outperforms state-of-the-art baselines in in-silico evaluations across most metrics. Remarkably, our approach offers a unique advantage by leveraging the mutual constraints between protein structure and sequence, facilitating the design of protein sequences with similar structures and optimized properties. The code and data are publicly available at https://github.com/GENTEL-lab/HADES.

</details>


### [6] [Do We Always Need Query-Level Workflows? Rethinking Agentic Workflow Generation for Multi-Agent Systems](https://arxiv.org/abs/2601.11147)
*Zixu Wang,Bingbing Xu,Yige Yuan,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本文提出了一种低成本的任务级生成框架SCALE，该框架通过少量样本校准预测优化器来进行评估，而无需完整的验证执行。实验结果显示，与现有方法相比，该框架在多个数据集上性能相当，同时减少了高达83%的令牌使用量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成工作流时成本和收益不明确，且存在查询级别的工作流生成不总是必要的问题，研究发现通过少量最佳任务级工作流即可覆盖相当数量的查询。本文旨在提出一种低成本的任务级生成框架，以减少计算成本并提高评估效率。

Method: 提出了一个名为SCALE的低成本任务级生成框架，该框架依靠少量样本校准来预测优化器的效果，避免了全面验证执行的成本。

Result: 经过广泛的实验，与现有的方法相比，该框架在多个数据集上保持了竞争力，性能下降平均仅0.61%，同时在令牌使用上降低了高达83%。

Conclusion: 研究证明了SCALE在减少成本同时保持性能的有效性，为多智能体系统中的工作流生成提供了一种新的低成本方法。

Abstract: Multi-Agent Systems (MAS) built on large language models typically solve complex tasks by coordinating multiple agents through workflows. Existing approaches generates workflows either at task level or query level, but their relative costs and benefits remain unclear. After rethinking and empirical analyses, we show that query-level workflow generation is not always necessary, since a small set of top-K best task-level workflows together already covers equivalent or even more queries. We further find that exhaustive execution-based task-level evaluation is both extremely token-costly and frequently unreliable. Inspired by the idea of self-evolution and generative reward modeling, we propose a low-cost task-level generation framework \textbf{SCALE}, which means \underline{\textbf{S}}elf prediction of the optimizer with few shot \underline{\textbf{CAL}}ibration for \underline{\textbf{E}}valuation instead of full validation execution. Extensive experiments demonstrate that \textbf{SCALE} maintains competitive performance, with an average degradation of just 0.61\% compared to existing approach across multiple datasets, while cutting overall token usage by up to 83\%.

</details>


### [7] [TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech](https://arxiv.org/abs/2601.11178)
*Girish A. Koushik,Helen Treharne,Diptesh Kanojia*

Main category: cs.AI

TL;DR: 该研究提出了TANDEM框架，解决了多模态音频-视觉仇恨言论检测问题，通过双向强化学习策略在长时序上进行稳定推理，而不需要密集的时间点监督。该方法在三项基准数据集上获得了显著效果，同时保持了时间定位的精度。


<details>
  <summary>Details</summary>
Motivation: 当前自动系统在检测仇恨言论方面表现良好，但它们往往无法提供精细、可解释的证据，导致人类介入的困难。因此，研究引入TANDEM以提高多模态仇恨言论检测的透明度和可解释性。

Method: TANDEM采用了一种新颖的双向强化学习策略，使视听语言模型相互优化，增强上下文跨模态一致性，从而在长时间序列上进行稳定推理，无需密集的时间点监督。

Result: TANDEM在三项基准数据集上表现优异，感知目标识别的最佳F1分数达到0.73，在HateMM数据集（相对于现有最佳效果提高30%）上表现出较高的时间定位精度。

Conclusion: TANDEM不仅展现了在复杂多模态环境中的结构化可解释性对齐的可行性，还为下一代透明有效的在线安全屏蔽工具提供了一个范例。

Abstract: Social media platforms are increasingly dominated by long-form multimodal content, where harmful narratives are constructed through a complex interplay of audio, visual, and textual cues. While automated systems can flag hate speech with high accuracy, they often function as "black boxes" that fail to provide the granular, interpretable evidence, such as precise timestamps and target identities, required for effective human-in-the-loop moderation. In this work, we introduce TANDEM, a unified framework that transforms audio-visual hate detection from a binary classification task into a structured reasoning problem. Our approach employs a novel tandem reinforcement learning strategy where vision-language and audio-language models optimize each other through self-constrained cross-modal context, stabilizing reasoning over extended temporal sequences without requiring dense frame-level supervision. Experiments across three benchmark datasets demonstrate that TANDEM significantly outperforms zero-shot and context-augmented baselines, achieving 0.73 F1 in target identification on HateMM (a 30% improvement over state-of-the-art) while maintaining precise temporal grounding. We further observe that while binary detection is robust, differentiating between offensive and hateful content remains challenging in multi-class settings due to inherent label ambiguity and dataset imbalance. More broadly, our findings suggest that structured, interpretable alignment is achievable even in complex multimodal settings, offering a blueprint for the next generation of transparent and actionable online safety moderation tools.

</details>


### [8] [Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning](https://arxiv.org/abs/2601.11252)
*Qianyue Wang,Jinwu Hu,Yufeng Wang,Huanxiang Lin,Bolin Chen,Zhiquan Wen,Yaofo Chen,Mingkui Tan*

Main category: cs.AI

TL;DR: Think-with-Me 提出了一种新的测试时互动推理范式，通过在推理过程中引入外部反馈干预来平衡推理准确性和长度。


<details>
  <summary>Details</summary>
Motivation: 当前的高效推理方法通常在闭环中运行，缺乏外部干预机制来引导推理过程。现有的模型在处理多步推理时容易出现过度推理或偏离轨道的问题，这增加了计算成本并降低了性能。

Method: Think-with-Me 通过引入一个基于多标准评估（合理性与完整性）的外部反馈机制，使得推理过程在关键时刻暂停，从而减少冗余，同时维护准确性。具体方法包括识别过渡连词作为干预点，以及根据这些点适时提供外部反馈，从而灵活地延长或终止推理。

Result: 在实验中，Think-with-Me 在 AIME24 测试集上达到了优于 QwQ-32B 的性能，准确度提高了 7.19%，同时将平均推理长度减少了 81%。此外，该范式也适用于安全性和创意任务。

Conclusion: 这项研究提出了一种新的互动推理方法，有效地缓解了过度推理或偏离轨道的问题，同时提高了模型在有限上下文窗口中的性能和效率。

Abstract: Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.

</details>


### [9] [XChoice: Explainable Evaluation of AI-Human Alignment in LLM-based Constrained Choice Decision Making](https://arxiv.org/abs/2601.11286)
*Weihong Qi,Fan Huang,Rasika Muralidharan,Jisun An,Haewoon Kwak*

Main category: cs.AI

TL;DR: XChoice 是一个可解释的框架，用于评估 AI 与人类在受限决策中的对齐情况。通过对比机器学习模型和人类决策中参数向量的变化，揭示模型与人类决策之间的差异，以及不同群体间的对齐情况。


<details>
  <summary>Details</summary>
Motivation: 当前的评估方法主要关注结果的一致性，如准确率和 F1 分数等，但未能揭示决策因素的重要性、约束敏感性及权衡取舍背后的机制。XChoice 框架旨在填补这一空白，通过机制建模来评估 AI 和人类决策之间的对齐。

Method: XChoice 通过将机制基础的决策模型拟合到人类数据和语言模型生成的决策上，捕获决策因素的重要性、约束敏感性和隐含的取舍。然后通过比较模型、选项和子组之间的参数向量来评估对齐。

Result: XChoice 在美国人的日常时间分配数据上展示了差异化的对齐情况，并在不同模型和活动中发现显著的不对齐模式，特别是在黑人和已婚群体中。

Conclusion: XChoice 提供了机制基础的指标，以诊断不一致并支持超出表面结果匹配的知情改进。此外，XChoice 通过不变性分析验证了其稳健性，并评估了带回溯检索生成（RAG）干预措施的有效性。

Abstract: We present XChoice, an explainable framework for evaluating AI-human alignment in constrained decision making. Moving beyond outcome agreement such as accuracy and F1 score, XChoice fits a mechanism-based decision model to human data and LLM-generated decisions, recovering interpretable parameters that capture the relative importance of decision factors, constraint sensitivity, and implied trade-offs. Alignment is assessed by comparing these parameter vectors across models, options, and subgroups. We demonstrate XChoice on Americans' daily time allocation using the American Time Use Survey (ATUS) as human ground truth, revealing heterogeneous alignment across models and activities and salient misalignment concentrated in Black and married groups. We further validate robustness of XChoice via an invariance analysis and evaluate targeted mitigation with a retrieval augmented generation (RAG) intervention. Overall, XChoice provides mechanism-based metrics that diagnose misalignment and support informed improvements beyond surface outcome matching.

</details>


### [10] [Hyperparameter Optimization of Constraint Programming Solvers](https://arxiv.org/abs/2601.11389)
*Hedieh Haddad,Thibault Falque,Pierre Talbot,Pascal Bouvry*

Main category: cs.AI

TL;DR: 该研究提出了一种称为探针与求解的两阶段框架，用于自动化约束编程求解器的超参数优化。通过在ACE和Choco上进行评估，结果表明该方法显著提升了求解器的表现。


<details>
  <summary>Details</summary>
Motivation: 手动找到最佳求解器配置是一个耗时且需要专家知识的任务。因此，研究旨在提出一种自动化的方法来优化这些超参数。

Method: 该研究在CPMpy库中引入了一种探针与求解框架，将可用时间预算分为两阶段：探索阶段使用不同超参数优化方法探索不同的超参数集，随后的求解阶段使用最佳配置解决实际问题。

Result: 研究表明，使用贝叶斯优化方法时，算法在ACI上提高了解决方案质量的25.4%，达到默认配置的57.9%，在Choco上则在38.6%的实例中表现更优。同时，该方法在与简距离搜索算法的比较中表现出色，进一步证实了基于模型的探索策略优于简单的局部搜索。

Conclusion: 探针与求解算法提供了一种实用、资源感知的方法，能适用于多种问题类型，为调优约束求解器带来了稳健的改进。

Abstract: The performance of constraint programming solvers is highly sensitive to the choice of their hyperparameters. Manually finding the best solver configuration is a difficult, time-consuming task that typically requires expert knowledge. In this paper, we introduce probe and solve algorithm, a novel two-phase framework for automated hyperparameter optimization integrated into the CPMpy library. This approach partitions the available time budget into two phases: a probing phase that explores different sets of hyperparameters using configurable hyperparameter optimization methods, followed by a solving phase where the best configuration found is used to tackle the problem within the remaining time.
  We implement and compare two hyperparameter optimization methods within the probe and solve algorithm: Bayesian optimization and Hamming distance search. We evaluate the algorithm on two different constraint programming solvers, ACE and Choco, across 114 combinatorial problem instances, comparing their performance against the solver's default configurations.
  Results show that using Bayesian optimization, the algorithm outperforms the solver's default configurations, improving solution quality for ACE in 25.4% of instances and matching the default performance in 57.9%, and for Choco, achieving superior results in 38.6% of instances. It also consistently surpasses Hamming distance search within the same framework, confirming the advantage of model-based exploration over simple local search. Overall, the probe and solve algorithm offers a practical, resource-aware approach for tuning constraint solvers that yields robust improvements across diverse problem types.

</details>


### [11] [Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs](https://arxiv.org/abs/2601.11468)
*Alessandro Padella,Massimiliano de Leoni,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文扩展了我们之前基于LLM的预测过程监控框架，该框架最初专注于通过提示总时间预测。实验表明，在数据稀缺的情况下，LLM在总时间和活动发生预测方面优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 预测过程监控是过程挖掘中的一个分支，旨在预测正在进行的过程的结果。本文扩展了之前的基于LLM的预测过程监控框架，以评估其通用性、语义利用和推理机制，特别是在总时间和活动发生预测方面。

Method: 实验使用了三个不同的事件日志，评估了在只有100条轨迹的数据稀缺情况下LLM相对于基准方法的表现。

Result: 实验结果表明，LLM在总时间和活动发生预测方面的表现优于基准方法，表明它利用了内部训练轨迹之间的联系和其内置的先验知识。

Conclusion: 研究还表明，模型采用了高级推理策略，而不是简单地复制现有预测方法，这对于改进过程预测性能具有重要意义。

Abstract: Predictive Process Monitoring is a branch of process mining that aims to predict the outcome of an ongoing process. Recently, it leveraged machine-and-deep learning architectures. In this paper, we extend our prior LLM-based Predictive Process Monitoring framework, which was initially focused on total time prediction via prompting. The extension consists of comprehensively evaluating its generality, semantic leverage, and reasoning mechanisms, also across multiple Key Performance Indicators. Empirical evaluations conducted on three distinct event logs and across the Key Performance Indicators of Total Time and Activity Occurrence prediction indicate that, in data-scarce settings with only 100 traces, the LLM surpasses the benchmark methods. Furthermore, the experiments also show that the LLM exploits both its embodied prior knowledge and the internal correlations among training traces. Finally, we examine the reasoning strategies employed by the model, demonstrating that the LLM does not merely replicate existing predictive methods but performs higher-order reasoning to generate the predictions.

</details>


### [12] [Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning](https://arxiv.org/abs/2601.11479)
*Yohai Trabelsi,Guojun Xiong,Fentabil Getnet,Stéphane Verguet,Milind Tambe*

Main category: cs.AI

TL;DR: 该研究提出了一种结合专家知识与优化技术的框架，以提高埃塞俄比亚农村地区基本医疗服务的可及性。


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚卫生部需要升级卫生站以改善公共服务的可及性，尤其是在农村地区。由于资源有限，必须合理优先考虑升级哪些卫生设施，以最大化人口覆盖率并考虑到不同的专家和利益相关者的偏好。

Method: 研究团队开发了一种称为‘大型语言模型和扩展贪婪’（LEG）的混合框架，结合了可提供理论保证的古典优化方法和人工语言模型驱动的迭代细化技术，确保解决方案反映专家的定性指导同时保持覆盖率保证。

Result: 研究在三个埃塞俄比亚地区的实际数据上进行了实验，表明该框架的有效性和其在指导公平、数据驱动的健康系统规划方面的潜在价值。

Conclusion: 研究成果展示了一种系统整合专家知识与优化技术的方法，有助于指导合理规划埃塞俄比亚的卫生设施升级，以促进基本医疗服务的可及性。

Abstract: Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources, however, require careful prioritization of which facilities to upgrade to maximize population coverage while accounting for diverse expert and stakeholder preferences. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we propose a hybrid framework that systematically integrates expert knowledge with optimization techniques. Classical optimization methods provide theoretical guarantees but require explicit, quantitative objectives, whereas stakeholder criteria are often articulated in natural language and difficult to formalize. To bridge these domains, we develop the Large language model and Extended Greedy (LEG) framework. Our framework combines a provable approximation algorithm for population coverage optimization with LLM-driven iterative refinement that incorporates human-AI alignment to ensure solutions reflect expert qualitative guidance while preserving coverage guarantees. Experiments on real-world data from three Ethiopian regions demonstrate the framework's effectiveness and its potential to inform equitable, data-driven health system planning.

</details>


### [13] [BoxMind: Closed-loop AI strategy optimization for elite boxing validated in the 2024 Olympics](https://arxiv.org/abs/2601.11492)
*Kaiwen Wang,Kaili Zheng,Rongrong Deng,Qingmin Fan,Milin Zhang,Zongrui Li,Xuesi Zhou,Bo Han,Liren Chen,Chenyi Guo,Ji Wu*

Main category: cs.AI

TL;DR: 本文提出了一种名为BoxMind的闭合回路AI专家系统，创新性地将拳击比赛视频解析为18个层次的技术战术指标，并通过图基预测模型融合了可学习的时间变体潜在嵌入，以捕捉比赛动态，使获胜概率梯度转化为可执行的战术调整。实验证明该系统的预测模型达到了最先进的性能，系统还生成了战略建议，展示了与人类专家相当的能力。


<details>
  <summary>Details</summary>
Motivation: 面向竞争体育所需的复杂战术分析，本文旨在填补拳击等格斗运动在AI驱动分析中的空白，因为这些运动的动态行动复杂且缺乏结构化的战术表示。

Method: 本文通过定义精确的时限和空间技术属性的原子击打事件，将比赛视频解析为18个层次的技术战术指标，提出了一种图基预测模型，结合了明确的技术战术配置文件与可学习的时间变体潜在嵌入，通过使胜率变化梯度成为可执行的战术调整，建立了拳击比赛结果预测体系。

Result: 实验结果显示，该模型在BoxerGraph测试集和奥运比赛中分别取得了69.8％和87.5％的高准确率。基于此预测模型，系统生成了战略建议，展示出了与人类专家相当的水平。该系统在中国国家拳击队2024年巴黎奥运会的闭合回路部署中起到了关键作用，帮助中国队获得了三金两银的历史性战绩。

Conclusion: BoxMind为将未结构化的视频数据转化为战略情报奠定了可复制的典范，弥合了计算机视觉与决策支持在竞争体育中的差距，展示了未来战术分析中的巨大潜力。

Abstract: Competitive sports require sophisticated tactical analysis, yet combat disciplines like boxing remain underdeveloped in AI-driven analytics due to the complexity of action dynamics and the lack of structured tactical representations. To address this, we present BoxMind, a closed-loop AI expert system validated in elite boxing competition. By defining atomic punch events with precise temporal boundaries and spatial and technical attributes, we parse match footage into 18 hierarchical technical-tactical indicators. We then propose a graph-based predictive model that fuses these explicit technical-tactical profiles with learnable, time-variant latent embeddings to capture the dynamics of boxer matchups. Modeling match outcome as a differentiable function of technical-tactical indicators, we turn winning probability gradients into executable tactical adjustments. Experiments show that the outcome prediction model achieves state-of-the-art performance, with 69.8% accuracy on BoxerGraph test set and 87.5% on Olympic matches. Using this predictive model as a foundation, the system generates strategic recommendations that demonstrate proficiency comparable to human experts. BoxMind is validated through a closed-loop deployment during the 2024 Paris Olympics, directly contributing to the Chinese National Team's historic achievement of three gold and two silver medals. BoxMind establishes a replicable paradigm for transforming unstructured video data into strategic intelligence, bridging the gap between computer vision and decision support in competitive sports.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [14] [Chatting with Confidants or Corporations? Privacy Management with AI Companions](https://arxiv.org/abs/2601.10754)
*Hsuen-Chi Chiu,Jeremy Foote*

Main category: cs.CR

TL;DR: 本研究通过用户访谈发现，AI聊天机器人作为情感伴侣时，用户在享受情感安全的同时会谨慎管理隐私，但平台级别的数据控制权问题仍然存在。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨AI聊天机器人作为情感伴侶时，用户如何平衡人际交往中的情感需求和机构层面的隐私管理。

Method: 采用深度访谈的方式，研究者基于沟通隐私管理理论和Masur的用户-AI和用户-平台隐私框架，对15名使用Replika和Character.AI等情感AI平台的用户进行了访谈。

Result: 研究结果表明，用户在使用聊天机器人时倾向于结合人际交往中的习惯和机构层面的警觉性，尽管机器人的非评判性和随时可用性促进了情感开放，但用户依然关注隐私风险，并采取多层级策略进行隐私管理。然而，用户对于平台级别的数据控制感不确定或无力，而拟人化设计进一步模糊了隐私界限，导致无意的隐私过度披露。

Conclusion: 研究结论指出，AI伴侣机器人的使用扩展了隐私理论，突显了人际情感隐私管理和机构层面隐私管理的复杂互动。

Abstract: AI chatbots designed as emotional companions blur the boundaries between interpersonal intimacy and institutional software, creating a complex, multi-dimensional privacy environment. Drawing on Communication Privacy Management theory and Masur's horizontal (user-AI) and vertical (user-platform) privacy framework, we conducted in-depth interviews with fifteen users of companion AI platforms such as Replika and Character.AI. Our findings reveal that users blend interpersonal habits with institutional awareness: while the non-judgmental, always-available nature of chatbots fosters emotional safety and encourages self-disclosure, users remain mindful of institutional risks and actively manage privacy through layered strategies and selective sharing. Despite this, many feel uncertain or powerless regarding platform-level data control. Anthropomorphic design further blurs privacy boundaries, sometimes leading to unintentional oversharing and privacy turbulence. These results extend privacy theory by highlighting the unique interplay of emotional and institutional privacy management in human-AI companionship.

</details>


### [15] [SecMLOps: A Comprehensive Framework for Integrating Security Throughout the MLOps Lifecycle](https://arxiv.org/abs/2601.10848)
*Xinrui Zhang,Pincan Zhao,Jason Jaskolka,Heng Li,Rongxing Lu*

Main category: cs.CR

TL;DR: 本文提出了Secure Machine Learning Operations（SecMLOps）框架，旨在通过嵌入安全考虑来增强整个MLOps生命周期中的安全性和可靠性。通过实证研究，展示了该框架在高级行人检测系统中的应用，并讨论了安全措施与系统性能之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 受ML模型部署带来重要安全挑战的驱动，本文旨在提供一个整合了安全措施的框架，以应对对抗攻击等威胁，增强ML应用程序的稳健性和可信度。

Method: 构建了SecMLOps框架，该框架在MLOps生命周期的所有阶段嵌入了安全性考虑。通过一个详细的高级行人检测系统案例研究展示了SecMLOps的实际应用，并分析了安全措施和系统性能之间的权衡。

Result: 通过实证研究，本文展示了SecMLOps框架在高级行人检测系统中的应用效果，并探讨了安全措施与系统性能之间的折衷。

Conclusion: 本文强调了在不同领域实现ML部署安全性和性能之间均衡的必要性，为实践者提供了如何在安全与性能之间取得最佳平衡的宝贵指导。

Abstract: Machine Learning (ML) has emerged as a pivotal technology in the operation of large and complex systems, driving advancements in fields such as autonomous vehicles, healthcare diagnostics, and financial fraud detection. Despite its benefits, the deployment of ML models brings significant security challenges, such as adversarial attacks, which can compromise the integrity and reliability of these systems. To address these challenges, this paper builds upon the concept of Secure Machine Learning Operations (SecMLOps), providing a comprehensive framework designed to integrate robust security measures throughout the entire ML operations (MLOps) lifecycle. SecMLOps builds on the principles of MLOps by embedding security considerations from the initial design phase through to deployment and continuous monitoring. This framework is particularly focused on safeguarding against sophisticated attacks that target various stages of the MLOps lifecycle, thereby enhancing the resilience and trustworthiness of ML applications. A detailed advanced pedestrian detection system (PDS) use case demonstrates the practical application of SecMLOps in securing critical MLOps. Through extensive empirical evaluations, we highlight the trade-offs between security measures and system performance, providing critical insights into optimizing security without unduly impacting operational efficiency. Our findings underscore the importance of a balanced approach, offering valuable guidance for practitioners on how to achieve an optimal balance between security and performance in ML deployments across various domains.

</details>


### [16] [Multi-Agent Taint Specification Extraction for Vulnerability Detection](https://arxiv.org/abs/2601.10865)
*Jonah Ghebremichael,Saastha Vasan,Saad Ullah,Greg Tystahl,David Adei,Christopher Kruegel,Giovanni Vigna,William Enck,Alexandros Kapravelos*

Main category: cs.CR

TL;DR: SemTaint 是一种结合大型语言模型（LLM）和传统静态程序分析的多智能体系统，能够有效减少 JavaScript 因动态特性和依赖库造成的安全分析困难，并提高漏洞检测能力。


<details>
  <summary>Details</summary>
Motivation: 传统的静态应用安全测试（SAST）工具对于 JavaScript 的动态特性和庞大的库生态系统难以进行有效的安全检测，SemTaint 旨在通过利用 LLM 的语义理解能力结合传统 SAST 方法来解决这个问题。

Method: SemTaint 通过静态程序分析构建调用图，并借助 LLM 处理解析静态方法无法解决的调用边。此外，它还使用 LLM 对给定 CWE 类别中的来源和目标进行分类。最终，生成的污点规格提供给 SAST 工具进行漏洞分析。

Result: 在与 CodeQL 的集成测试中，SemTaint 发现了 162 个之前未被发现的漏洞中的 106 个，并在 4 个流行的 npm 包中发现了 4 个新的漏洞。

Conclusion: SemTaint 证明了 LLM 的使用可以有效增强静态程序分析算法，提升漏洞检测能力。

Abstract: Static Application Security Testing (SAST) tools using taint analysis are widely viewed as providing higher-quality vulnerability detection results compared to traditional pattern-based approaches. However, performing static taint analysis for JavaScript poses two major challenges. First, JavaScript's dynamic features complicate data flow extraction required for taint tracking. Second, npm's large library ecosystem makes it difficult to identify relevant sources/sinks and establish taint propagation across dependencies. In this paper, we present SemTaint, a multi-agent system that strategically combines the semantic understanding of Large Language Models (LLMs) with traditional static program analysis to extract taint specifications, including sources, sinks, call edges, and library flow summaries tailored to each package. Conceptually, SemTaint uses static program analysis to calculate a call graph and defers to an LLM to resolve call edges that cannot be resolved statically. Further, it uses the LLM to classify sources and sinks for a given CWE. The resulting taint specification is then provided to a SAST tool, which performs vulnerability analysis. We integrate SemTaint with CodeQL, a state-of-the-art SAST tool, and demonstrate its effectiveness by detecting 106 of 162 vulnerabilities previously undetectable by CodeQL. Furthermore, we find 4 novel vulnerabilities in 4 popular npm packages. In doing so, we demonstrate that LLMs can practically enhance existing static program analysis algorithms, combining the strengths of both symbolic reasoning and semantic understanding for improved vulnerability detection.

</details>


### [17] [Adaptive Privacy Budgeting](https://arxiv.org/abs/2601.10866)
*Yuting Liang,Ke Yi*

Main category: cs.CR

TL;DR: 本文研究了在广义差分隐私下适应性隐私预算问题。对于每个用户的不均衡数据贡献，提出了一种能够根据先前查询结果进行调整的适应性预算框架，以实现隐私保护下的高效计算。


<details>
  <summary>Details</summary>
Motivation: 目前的数据分析中，用户数据的重要程度往往不均等，传统的隐私预算方法可能无法充分利用这种不均衡来提高计算效率，本文旨在提供一种适应性隐私预算框架，动态调整隐私预算，最大限度地降低隐私成本，提高查询结果的有用性。

Method: 本文的方法主要集中在构建一种适应性预算框架，该框架可以根据用户数据的特定贡献以及先前查询的输出动态调整隐私预算，从而实现更高效的隐私保护计算。

Result: 实验结果表明，与固定隐私预算方法相比，本文提出的适应性预算框架能够在不显著牺牲隐私保护的前提下，提高数据查询的有用性。

Conclusion: 本文提出的适应性隐私预算框架为广义差分隐私下的数据查询提供了一种新的高效解决方案，适用于多种应用场景。

Abstract: We study the problem of adaptive privacy budgeting under generalized differential privacy. Consider the setting where each user $i\in [n]$ holds a tuple $x_i\in U:=U_1\times \dotsb \times U_T$, where $x_i(l)\in U_l$ represents the $l$-th component of their data. For every $l\in [T]$ (or a subset), an untrusted analyst wishes to compute some $f_l(x_1(l),\dots,x_n(l))$, while respecting the privacy of each user. For many functions $f_l$, data from the users are not all equally important, and there is potential to use the privacy budgets of the users strategically, leading to privacy savings that can be used to improve the utility of later queries. In particular, the budgeting should be adaptive to the outputs of previous queries, so that greater savings can be achieved on more typical instances. In this paper, we provide such an adaptive budgeting framework, with various applications demonstrating its applicability.

</details>


### [18] [AJAR: Adaptive Jailbreak Architecture for Red-teaming](https://arxiv.org/abs/2601.10971)
*Yipu Dou,Wang Yang*

Main category: cs.CR

TL;DR: 介绍了一种名为AJAR的新框架，旨在通过协议驱动的认知编排（Protocol-driven Cognitive Orchestration）补充现有的红队测试框架。AJAR基于Petri的运行时，并利用模型上下文协议（MCP）解耦对手逻辑和执行循环，使其成为一个标准化、可插拔的服务。


<details>
  <summary>Details</summary>
Motivation: 现有的红队测试框架存在局限性，要么关注刚性的脚本化文本攻击，要么缺乏模拟复杂、多轮自主攻击的模块化架构。因此，本文提出了AJAR框架，以弥合这个差距，通过协议驱动的认知编排来提高红队测试的灵活性。

Method: AJAR框架利用Petri的运行时，并采用模型上下文协议（MCP）进行解耦，将先进的算法如X-Teaming标准化和模块化，以插拔式服务实现。

Result: 通过控制性的定性案例研究，验证了AJAR框架的架构可行性，它能够在一个工具使用环境中执行有状态回溯。此外，初步探讨了“能动性缺口”，展示了工具使用引入新的注入向量，而参数格式化认知负担可能意外破坏基于角色的攻击。

Conclusion: AJAR开源以促进对这一新兴攻击面的标准化和环境感知评估。通过为红队测试提供一种灵活的方法，AJAR有助于提升AI系统的安全性。

Abstract: As Large Language Models (LLMs) evolve from static chatbots into autonomous agents capable of tool execution, the landscape of AI safety is shifting from content moderation to action security. However, existing red-teaming frameworks remain bifurcated: they either focus on rigid, script-based text attacks or lack the architectural modularity to simulate complex, multi-turn agentic exploitations. In this paper, we introduce AJAR (Adaptive Jailbreak Architecture for Red-teaming), a proof-of-concept framework designed to bridge this gap through Protocol-driven Cognitive Orchestration. Built upon the robust runtime of Petri, AJAR leverages the Model Context Protocol (MCP) to decouple adversarial logic from the execution loop, encapsulating state-of-the-art algorithms like X-Teaming as standardized, plug-and-play services. We validate the architectural feasibility of AJAR through a controlled qualitative case study, demonstrating its ability to perform stateful backtracking within a tool-use environment. Furthermore, our preliminary exploration of the "Agentic Gap" reveals a complex safety dynamic: while tool usage introduces new injection vectors via code execution, the cognitive load of parameter formatting can inadvertently disrupt persona-based attacks. AJAR is open-sourced to facilitate the standardized, environment-aware evaluation of this emerging attack surface. The code and data are available at https://github.com/douyipu/ajar.

</details>


### [19] [Towards Quantum-Resistant Trusted Computing: Architectures for Post-Quantum Integrity Verification Techniques](https://arxiv.org/abs/2601.11095)
*Grazia D'Onghia,Antonio Lioy*

Main category: cs.CR

TL;DR: 本文概述了当前信任技术及其向后量子（PQ）世界的过渡现状，讨论了后量子密码学（PQC）在当前信任计算（TC）解决方案中的挑战，并提出了增强信任计算技术的架构方案，旨在加速切换到量子抗性算法。


<details>
  <summary>Details</summary>
Motivation: 鉴于根信任（RoT）和信任计算（TC）环境中存在的传统密码学面临的量子计算（QC）威胁，提出当前技术路径和后量子密码学（PQC）的重要性，以及在TC解决方案中集成PQC的必要性。这将使得设备免受量子基于攻击的风险。

Method: 通过对现有技术状态和挑战的分析，探讨PQC在信任计算环境中的应用，并提出一个增强的架构，以适应量子抗性算法。

Result: 分析当前PQC的状态和面临的挑战，提出了一个信任计算增强架构，旨在促进从现有信任计算解决方案到后量子世界的过渡。

Conclusion: 文章强调了量子抗性算法在信任计算中的重要性，并提出了一种集成PQC的方法，以增强信任计算环境的安全性，应对量子计算威胁。

Abstract: Trust is the core building block of secure systems, and it is enforced through methods to ensure that a specific system is properly configured and works as expected. In this context, a Root of Trust (RoT) establishes a trusted environment, where both data and code are authenticated via a digital signature based on asymmetric cryptography, which is vulnerable to the threat posed by Quantum Computers (QCs). Firmware, being the first layer of trusted software, faces unique risks due to its longevity and difficult update. The transition of firmware protection to Post-Quantum Cryptography (PQC) is urgent, since it reduces the risk derived from exposing all computing and network devices to quantum-based attacks. This paper offers an analysis of the most common trust techniques and their roadmap towards a Post-Quantum (PQ) world, by investigating the current status of PQC and the challenges posed by such algorithms in existing Trusted Computing (TC) solutions from an integration perspective. Furthermore, this paper proposes an architecture for TC techniques enhanced with PEC, addressing the imperative for immediate adoption of quantum-resistant algorithms.

</details>


### [20] [Shaping a Quantum-Resistant Future: Strategies for Post-Quantum PKI](https://arxiv.org/abs/2601.11104)
*Grazia D'Onghia,Diana Gratiela Berbecaru,Antonio Lioy*

Main category: cs.CR

TL;DR: 该研究专注于后量子时代下公钥基础设施(Public Key Infrastructure, PKI)的安全性转型，特别是适应X.509证书格式和证书撤销列表等组件，介绍如何通过比较分析过渡到量子抗性的PKI。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算时代的临近，经典密码协议的保护变得至关重要。研究表明，基于公钥的签名和密钥交换最易受到量子计算的威胁，必须确保其所依赖的公共密钥证书也具有量子抗性。

Method: 研究团队评估了最新的后量子安全算法，并定义了将PKI转向量子抗性的具体要求，特别关注了X.509证书格式的适应性。

Result: 研究结果表明，通过适当调整和过渡策略，可以实现PKI向量子抗性系统的平稳转变，并确保关键组件如证书撤销列表和在线证书状态协议的支持。

Conclusion: 研究提供了详细的适应性评估和建议，为后续过渡到量子抗性公钥基础设施奠定了基础。

Abstract: As the quantum computing era approaches, securing classical cryptographic protocols becomes imperative. Public key cryptography is widely used for signature and key exchange but it is the type of cryptography more threatened by quantum computing. Its application typically requires support via a public-key certificate, which is a signed data structure and must therefore face twice the quantum challenge: for the certified keys and for the signature itself. We present the latest developments in selecting robust Post-Quantum algorithms and investigate their applicability in the Public Key Infrastructure context. Our contribution entails defining requirements for a secure transition to a quantum-resistant Public Key Infrastructure, with a focus on adaptations for the X.509 certificate format. Additionally, we explore transitioning Certificate Revocation List and Online Certificate Status Protocol to support quantum-resistant algorithms. Through comparative analysis, we elucidate the complex transition to a quantum-resistant PKI.

</details>


### [21] [Proving Circuit Functional Equivalence in Zero Knowledge](https://arxiv.org/abs/2601.11173)
*Sirui Shen,Zunchen Huang,Chenglu Jin*

Main category: cs.CR

TL;DR: 本文提出了一种新颖的隐私保护硬件形式验证方法ZK-CEC，通过结合形式验证和零知识证明（ZKP），能够在不泄露设计机密性的前提下，验证IP的正确性及安全性。


<details>
  <summary>Details</summary>
Motivation: 随着现代集成电路生态系统越来越多地依赖第三方硬件IP集成，这带来了硬件特洛伊木马和安全漏洞等安全风险。现有的隐私保护硬件验证方法大多基于仿真，无法提供形式上的保证，因此需要开发一种新的方法来解决这一问题。

Method: 在现有研究的基础上，本文提出了一种引入正式验证和零知识证明（ZKP）的新颖框架ZK-CEC。该方法提出了一种证明秘密设计与公共约束的不满足性的蓝图，并基于该蓝图构建了ZK-CEC，以实现仅揭示证明的长度和宽度的方式验证隐藏设计是否符合公开规范的功能。

Result: ZK-CEC已被实现并在不同电路中进行了性能评估，包括算术单元和加密组件。实验结果表明，ZK-CEC能够在合理的时间内验证实际设计，例如AES S-Box。

Conclusion: ZK-CEC为正式验证私有硬件IP提供了新的思路和解决方案，并在实际应用场景中证明了其有效性。

Abstract: The modern integrated circuit ecosystem is increasingly reliant on third-party intellectual property integration, which introduces security risks, including hardware Trojans and security vulnerabilities. Addressing the resulting trust deadlock between IP vendors and system integrators without exposing proprietary designs requires novel privacy-preserving verification techniques. However, existing privacy-preserving hardware verification methods are all simulation-based and fail to offer formal guarantees. In this paper, we propose ZK-CEC, the first privacy-preserving framework for hardware formal verification. By combining formal verification and zero-knowledge proof (ZKP), ZK-CEC establishes a foundation for formally verifying IP correctness and security without compromising the confidentiality of the designs.
  We observe that existing zero-knowledge protocols for formal verification are designed to prove statements of public formulas. However, in a privacy-preserving verification context where the formula is secret, these protocols cannot prevent a malicious prover from forging the formula, thereby compromising the soundness of the verification. To address these gaps, we first propose a blueprint for proving the unsatisfiability of a secret design against a public constraint, which is widely applicable to proving properties in software, hardware, and cyber-physical systems. Based on the proposed blueprint, we construct ZK-CEC, which enables a prover to convince the verifier that a secret IP's functionality aligns perfectly with the public specification in zero knowledge, revealing only the length and width of the proof. We implement ZK-CEC and evaluate its performance across various circuits, including arithmetic units and cryptographic components. Experimental results show that ZK-CEC successfully verifies practical designs, such as the AES S-Box, within practical time limits.

</details>


### [22] [SD-RAG: A Prompt-Injection-Resilient Framework for Selective Disclosure in Retrieval-Augmented Generation](https://arxiv.org/abs/2601.11199)
*Aiman Al Masoud,Marco Arazzi,Antonino Nocera*

Main category: cs.CR

TL;DR: SD-RAG 是一种新的 RAG 技术，通过在检索阶段应用清洗和披露控制来分离安全和隐私约束的执行，而不是依赖于提示级别的保护措施。SD-RAG 还允许与优化的图基数据模型一起摄取可读的动态安全和隐私约束，实现了对生成模型的提示注入攻击的强大抵抗力，并在隐私评分上取得了高达 58% 的改进。


<details>
  <summary>Details</summary>
Motivation: 现有的 RAG 方法缺乏有效的机制来防止敏感信息的泄露，而最近的攻击表明，LLMs 对于提示注入攻击仍有一定的脆弱性。因此，提出了 SD-RAG 以增强 RAG 方法中的安全性和隐私性。

Method: SD-RAG 通过在检索阶段执行清洗和披露控制来实施安全和隐私约束，避免直接将敏感或访问受控的信息传递给生成模型。此外，SD-RAG 使用了可读的动态安全与隐私约束以及优化的图基数据模型进数据检索。

Result: 实验结果表明，SD-RAG 相对于现有基准方法在隐私评分上提高了 58%，并且展示了抵抗针对生成模型的提示注入攻击的强大能力。

Conclusion: 总结来说，SD-RAG 通过改进的检索阶段控制和动态安全策略有效提升了 RAG 方法的安全性和隐私性，为未来的研究提供了新的思路。

Abstract: Retrieval-Augmented Generation (RAG) has attracted significant attention due to its ability to combine the generative capabilities of Large Language Models (LLMs) with knowledge obtained through efficient retrieval mechanisms over large-scale data collections. Currently, the majority of existing approaches overlook the risks associated with exposing sensitive or access-controlled information directly to the generation model. Only a few approaches propose techniques to instruct the generative model to refrain from disclosing sensitive information; however, recent studies have also demonstrated that LLMs remain vulnerable to prompt injection attacks that can override intended behavioral constraints. For these reasons, we propose a novel approach to Selective Disclosure in Retrieval-Augmented Generation, called SD-RAG, which decouples the enforcement of security and privacy constraints from the generation process itself. Rather than relying on prompt-level safeguards, SD-RAG applies sanitization and disclosure controls during the retrieval phase, prior to augmenting the language model's input. Moreover, we introduce a semantic mechanism to allow the ingestion of human-readable dynamic security and privacy constraints together with an optimized graph-based data model that supports fine-grained, policy-aware retrieval. Our experimental evaluation demonstrates the superiority of SD-RAG over baseline existing approaches, achieving up to a $58\%$ improvement in the privacy score, while also showing a strong resilience to prompt injection attacks targeting the generative model.

</details>


### [23] [InterPUF: Distributed Authentication via Physically Unclonable Functions and Multi-party Computation for Reconfigurable Interposers](https://arxiv.org/abs/2601.11368)
*Ishraq Tashdid,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.CR

TL;DR: InterPUF 提出了一种基于路由差异延迟 PUF 的分布式根信任框架，以提供灵活且安全的芯片级互连身份验证。


<details>
  <summary>Details</summary>
Motivation: 随着现代系统级封装平台采用可重构的中介层，实现了异构多供应商生态系统中的即插即用芯片级集成。然而，这增加了信任挑战，传统的身份验证方案无法在分散的、后制造可编程环境中进行扩展或适应。

Method: InterPUF 通过嵌入基于路由的差异延迟物理不可克隆函数（PUF）到可重构互连中，并使用多方计算（MPC）进行身份验证，确保原始 PUF 签名不被暴露。

Result: 硬件评估显示，InterPUF 在不同芯片上仅占用 0.23% 的面积和 0.072% 的功率，并且保持了数十纳秒的认证延迟，而仿真的 pyPUF 结果表明，该方案在工艺、电压和温度波动下的唯一性、可靠性和建模抵抗力很强。

Conclusion: 通过结合中介器内嵌的 PUF 原语、加密散列和协作验证，InterPUF 实现了一个基于最小信任的身份验证模型，而无需依赖中央锚点。

Abstract: Modern system-in-package (SiP) platforms increasingly adopt reconfigurable interposers to enable plug-and-play chiplet integration across heterogeneous multi-vendor ecosystems. However, this flexibility introduces severe trust challenges, as traditional authentication schemes fail to scale or adapt in decentralized, post-fabrication programmable environments. This paper presents InterPUF, a compact and scalable authentication framework that transforms the interposer into a distributed root of trust. InterPUF embeds a route-based differential delay physically unclonable function (PUF) across the reconfigurable interconnect and secures authentication using multi-party computation (MPC), ensuring raw PUF signatures are never exposed. Our hardware evaluation shows only 0.23% area and 0.072% power overhead across diverse chiplets while preserving authentication latency within tens of nanoseconds. Simulation results using pyPUF confirm strong uniqueness, reliability, and modeling resistance under process, voltage, and temperature variations. By combining interposer-resident PUF primitives with cryptographic hashing and collaborative verification, InterPUF enforces a minimal-trust authentication model without relying on a centralized anchor.

</details>


### [24] [Understanding Help Seeking for Digital Privacy, Safety, and Security](https://arxiv.org/abs/2601.11398)
*Kurt Thomas,Sai Teja Peddinti,Sarah Meiklejohn,Tara Matthews,Amelia Hassoun,Animesh Srivastava,Jessica McClearn,Patrick Gage Kelley,Sunny Consolvo,Nina Taft*

Main category: cs.CR

TL;DR: 研究通过对大量Reddit帖子进行分析，识别用户在数字隐私、安全和安全方面寻求帮助的位置和内容，以更好地理解和解决问题，为用户提供更好的资源。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解用户在实际中如何寻求帮助，研究人员利用混合质性编码和大语言模型微调的方法，分析了大量Reddit帖子，以了解求助的范围、规模，提供帮助的社区类型，以及求助类型。

Method: 研究人员使用混合质性编码和大语言模型微调的方法，从过去四年超过10亿的Reddit帖子中筛选出93%精度和召回率的相关帖子，并自动标注了讨论的主题。

Result: 研究结果显示，用户在数字隐私、安全和安全方面的问题请求涉及多种主题，包括安全工具、隐私设置、欺诈、账号丢失等。社区和用户通过平台和其他资源提供帮助。

Conclusion: 研究强调了在复杂威胁和计划组合下支持用户所面临的挑战，该工作为开发更好的用户指南或语言模型提供帮助提供了信息。

Abstract: The complexity of navigating digital privacy, safety, and security threats often falls directly on users. This leads to users seeking help from family and peers, platforms and advice guides, dedicated communities, and even large language models (LLMs). As a precursor to improving resources across this ecosystem, our community needs to understand what help seeking looks like in the wild. To that end, we blend qualitative coding with LLM fine-tuning to sift through over one billion Reddit posts from the last four years to identify where and for what users seek digital privacy, safety, or security help. We isolate three million relevant posts with 93% precision and recall and automatically annotate each with the topics discussed (e.g., security tools, privacy configurations, scams, account compromise, content moderation, and more). We use this dataset to understand the scope and scale of help seeking, the communities that provide help, and the types of help sought. Our work informs the development of better resources for users (e.g., user guides or LLM help-giving agents) while underscoring the inherent challenges of supporting users through complex combinations of threats, platforms, mitigations, context, and emotions.

</details>
