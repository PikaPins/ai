{"id": "2602.10134", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10134", "abs": "https://arxiv.org/abs/2602.10134", "authors": ["Zhiyu Sun", "Minrui Luo", "Yu Wang", "Zhili Chen", "Tianxing He"], "title": "Reverse-Engineering Model Editing on Language Models", "comment": null, "summary": "Large language models (LLMs) are pretrained on corpora containing trillions of tokens and, therefore, inevitably memorize sensitive information. Locate-then-edit methods, as a mainstream paradigm of model editing, offer a promising solution by modifying model parameters without retraining. However, in this work, we reveal a critical vulnerability of this paradigm: the parameter updates inadvertently serve as a side channel, enabling attackers to recover the edited data. We propose a two-stage reverse-engineering attack named \\textit{KSTER} (\\textbf{K}ey\\textbf{S}paceRecons\\textbf{T}ruction-then-\\textbf{E}ntropy\\textbf{R}eduction) that leverages the low-rank structure of these updates. First, we theoretically show that the row space of the update matrix encodes a ``fingerprint\" of the edited subjects, enabling accurate subject recovery via spectral analysis. Second, we introduce an entropy-based prompt recovery attack that reconstructs the semantic context of the edit. Extensive experiments on multiple LLMs demonstrate that our attacks can recover edited data with high success rates. Furthermore, we propose \\textit{subspace camouflage}, a defense strategy that obfuscates the update fingerprint with semantic decoys. This approach effectively mitigates reconstruction risks without compromising editing utility. Our code is available at https://github.com/reanatom/EditingAtk.git.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u57fa\u4e8e\u53c2\u6570\u66f4\u65b0\u7684\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKSTER\u7684\u4e24\u9636\u6bb5\u53cd\u5411\u5de5\u7a0b\u653b\u51fb\u6765\u6062\u590d\u7f16\u8f91\u6570\u636e\uff0c\u5e76\u4ecb\u7ecd\u4e86\u9632\u5fa1\u7b56\u7565subspace camouflage\u4ee5\u51cf\u8f7b\u91cd\u5efa\u98ce\u9669\uff0c\u540c\u65f6\u4fdd\u6301\u7f16\u8f91\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53c2\u6570\u66f4\u65b0\u7684\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u53ef\u80fd\u6210\u4e3a\u653b\u51fb\u8005\u6062\u590d\u7f16\u8f91\u6570\u636e\u7684\u4fa7\u4fe1\u9053\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u51fa\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKSTER\u7684\u4e24\u9636\u6bb5\u53cd\u5411\u5de5\u7a0b\u653b\u51fb\uff0c\u5305\u62ec\u4f7f\u7528\u8c31\u5206\u6790\u4ece\u53c2\u6570\u66f4\u65b0\u7684\u884c\u7a7a\u95f4\u6062\u590d\u7f16\u8f91\u4e3b\u9898\u7684\u6307\u7eb9\uff0c\u5e76\u5229\u7528\u71b5\u57fa\u4e8e\u7684\u63d0\u793a\u6062\u590d\u653b\u51fb\u6765\u91cd\u5efa\u7f16\u8f91\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684KSTER\u653b\u51fb\u65b9\u6cd5\u53ef\u4ee5\u5728\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u6210\u529f\u6062\u590d\u7f16\u8f91\u6570\u636e\uff0c\u5e76\u4e14\u63d0\u51fa\u7684subspace camouflage\u9632\u5fa1\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u91cd\u5efa\u98ce\u9669\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u4e3b\u6d41\u7684\u57fa\u4e8e\u53c2\u6570\u66f4\u65b0\u7684\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u7684\u5b89\u5168\u9690\u60a3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u653b\u51fb\u65b9\u6cd5\u548c\u9632\u5fa1\u7b56\u7565\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2602.10142", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.10142", "abs": "https://arxiv.org/abs/2602.10142", "authors": ["Molly Campbell", "Ajay Kumar Shrestha"], "title": "Privacy by Voice: Modeling Youth Privacy-Protective Behavior in Smart Voice Assistants", "comment": "To appear in the IEEE ICAIIC 2026 proceedings", "summary": "Smart Voice Assistants (SVAs) are deeply embedded in the lives of youth, yet the mechanisms driving the privacy-protective behaviors among young users remain poorly understood. This study investigates how Canadian youth (aged 16-24) negotiate privacy with SVAs by developing and testing a structural model grounded in five key constructs: perceived privacy risks (PPR), perceived benefits (PPBf), algorithmic transparency and trust (ATT), privacy self-efficacy (PSE), and privacy-protective behaviors (PPB). A cross-sectional survey of N=469 youth was analyzed using partial least squares structural equation modeling. Results reveal that PSE is the strongest predictor of PPB, while the effect of ATT on PPB is fully mediated by PSE. This identifies a critical efficacy gap, where youth's confidence must first be built up for them to act. The model confirms that PPBf directly discourages protective action, yet also indirectly fosters it by slightly boosting self-efficacy. These findings empirically validate and extend earlier qualitative work, quantifying how policy overload and hidden controls erode the self-efficacy necessary for protective action. This study contributes an evidence-based pathway from perception to action and translates it into design imperatives that empower young digital citizens without sacrificing the utility of SVAs.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u8c03\u67e5\u52a0\u62ff\u592716-24\u5c81\u5e74\u8f7b\u4eba\u4e0e\u667a\u80fd\u8bed\u97f3\u52a9\u624b\uff08SVAs\uff09\u4e4b\u95f4\u7684\u9690\u79c1\u4fdd\u62a4\u884c\u4e3a\uff0c\u5efa\u7acb\u4e86\u4e94\u4e2a\u5173\u952e\u6784\u5ff5\u7684\u7ed3\u6784\u6a21\u578b\uff0c\u53d1\u73b0\u9690\u79c1\u81ea\u6211\u6548\u80fd\u662f\u4fdd\u62a4\u884c\u4e3a\u6700\u5f3a\u7684\u9884\u6d4b\u56e0\u7d20\uff0c\u800c\u7b97\u6cd5\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u5bf9\u4fdd\u62a4\u884c\u4e3a\u7684\u5f71\u54cd\u5219\u901a\u8fc7\u9690\u79c1\u81ea\u6211\u6548\u80fd\u5b8c\u5168\u4e2d\u4ecb\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u5e74\u8f7b\u4eba\u5728\u4f7f\u7528\u667a\u80fd\u8bed\u97f3\u52a9\u624b\u65f6\u7684\u9690\u79c1\u4fdd\u62a4\u884c\u4e3a\u673a\u5236\uff0c\u4ee5\u53ca\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u589e\u5f3a\u5e74\u8f7b\u4eba\u7684\u9690\u79c1\u81ea\u6211\u6548\u80fd\u6765\u4fc3\u8fdb\u4ed6\u4eec\u7684\u4fdd\u62a4\u884c\u4e3a\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u90e8\u5206\u6700\u5c0f\u4e8c\u4e58\u7ed3\u6784\u65b9\u7a0b\u6a21\u578b\u5bf9469\u540d\u5e74\u8f7b\u4eba\u8fdb\u884c\u4e86\u6a2a\u65ad\u9762\u8c03\u67e5\uff0c\u8bc4\u4f30\u4e94\u4e2a\u5173\u952e\u6784\u5ff5\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9690\u79c1\u81ea\u6211\u6548\u80fd\u662f\u4fdd\u62a4\u884c\u4e3a\u6700\u5f3a\u7684\u9884\u6d4b\u56e0\u7d20\uff0c\u7b97\u6cd5\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u5bf9\u4fdd\u62a4\u884c\u4e3a\u7684\u5f71\u54cd\u5219\u901a\u8fc7\u9690\u79c1\u81ea\u6211\u6548\u80fd\u5b8c\u5168\u4e2d\u4ecb\u3002\u540c\u65f6\uff0c\u611f\u77e5\u7684\u9690\u79c1\u98ce\u9669\u548c\u611f\u77e5\u7684\u5229\u76ca\u4ee5\u590d\u6742\u7684\u65b9\u5f0f\u5f71\u54cd\u7740\u884c\u4e3a\uff0c\u90e8\u5206\u6291\u5236\u4e86\u4fdd\u62a4\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u5efa\u8bae\u901a\u8fc7\u589e\u5f3a\u5e74\u8f7b\u4eba\u7684\u9690\u79c1\u81ea\u6211\u6548\u80fd\u6765\u4fc3\u8fdb\u4ed6\u4eec\u7684\u4fdd\u62a4\u884c\u4e3a\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u884c\u4e3a\u8def\u5f84\u548c\u8bbe\u8ba1\u539f\u5219\uff0c\u4ee5\u4fc3\u8fdb\u5e74\u8f7b\u6570\u5b57\u516c\u6c11\u7684\u6210\u957f\uff0c\u540c\u65f6\u786e\u4fddSVAs\u7684\u529f\u80fd\u6027\u3002"}}
{"id": "2602.10148", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10148", "abs": "https://arxiv.org/abs/2602.10148", "authors": ["Yu Yan", "Sheng Sun", "Shengjia Cheng", "Teli Liu", "Mingfeng Li", "Min Liu"], "title": "Red-teaming the Multimodal Reasoning: Jailbreaking Vision-Language Models via Cross-modal Entanglement Attacks", "comment": null, "summary": "Vision-Language Models (VLMs) with multimodal reasoning capabilities are high-value attack targets, given their potential for handling complex multimodal harmful tasks. Mainstream black-box jailbreak attacks on VLMs work by distributing malicious clues across modalities to disperse model attention and bypass safety alignment mechanisms. However, these adversarial attacks rely on simple and fixed image-text combinations that lack attack complexity scalability, limiting their effectiveness for red-teaming VLMs' continuously evolving reasoning capabilities. We propose \\textbf{CrossTALK} (\\textbf{\\underline{Cross}}-modal en\\textbf{\\underline{TA}}ng\\textbf{\\underline{L}}ement attac\\textbf{\\underline{K}}), which is a scalable approach that extends and entangles information clues across modalities to exceed VLMs' trained and generalized safety alignment patterns for jailbreak. Specifically, {knowledge-scalable reframing} extends harmful tasks into multi-hop chain instructions, {cross-modal clue entangling} migrates visualizable entities into images to build multimodal reasoning links, and {cross-modal scenario nesting} uses multimodal contextual instructions to steer VLMs toward detailed harmful outputs. Experiments show our COMET achieves state-of-the-art attack success rate.", "AI": {"tldr": "CrossTALK\u662f\u4e00\u79cd\u65b0\u578b\u7684\u53ef\u6269\u5c55\u8de8\u6a21\u6001\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u4f20\u64ad\u6709\u5bb3\u7ebf\u7d22\uff0c\u6253\u7834VLMs\u7684\u5b89\u5168\u5bf9\u9f50\u6a21\u5f0f\uff0c\u5b9e\u73b0\u66f4\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7b80\u5355\u7684\u56fe\u6587\u5b57\u7ec4\u5408\uff0c\u7f3a\u4e4f\u653b\u51fb\u590d\u6742\u5ea6\u7684\u6269\u5c55\u6027\uff0c\u56e0\u6b64\u96be\u4ee5\u6709\u6548\u6d4b\u8bd5VLMs\u7684\u52a8\u6001\u63a8\u7406\u80fd\u529b\u3002", "method": "CrossTALK\u901a\u8fc7\u77e5\u8bc6\u7d2f\u52a0\u7684\u91cd\u65b0\u6846\u67b6\u3001\u8de8\u6a21\u6001\u7ebf\u7d22\u7ea0\u7f20\u548c\u8de8\u6a21\u6001\u573a\u666f\u5d4c\u5957\u4e09\u4e2a\u6280\u672f\u624b\u6bb5\uff0c\u6269\u5c55\u548c\u4ea4\u7ec7\u4fe1\u606f\u7ebf\u7d22\u5230\u591a\u4e2a\u6a21\u6001\u4e2d\uff0c\u6253\u7834VLMs\u5b66\u4e60\u5230\u7684\u6a21\u6001\u95f4\u5b89\u5168\u5bf9\u9f50\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCrossTALK\u653b\u51fb\u65b9\u6cd5\u5728\u653b\u51fb\u6210\u529f\u7387\u4e0a\u8fbe\u5230\u4e86\u76ee\u524d\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002", "conclusion": "CrossTALK\u63d0\u4f9b\u4e86\u4e00\u79cd\u6269\u5c55\u4e14\u590d\u6742\u7684\u8de8\u6a21\u6001\u653b\u51fb\u7b56\u7565\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u6d4b\u8bd5VLMs\u7684\u590d\u6742\u6027\u4e0e\u5176\u5b89\u5168\u6027\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2602.10458", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10458", "abs": "https://arxiv.org/abs/2602.10458", "authors": ["Yansong Qu", "Zihao Sheng", "Zilin Huang", "Jiancong Chen", "Yuhao Luo", "Tianyi Wang", "Yiheng Feng", "Samuel Labi", "Sikai Chen"], "title": "Found-RL: foundation model-enhanced reinforcement learning for autonomous driving", "comment": "39 pages", "summary": "Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aFound-RL\u7684\u5e73\u53f0\uff0c\u65e8\u5728\u5229\u7528\u57fa\u7840\u6a21\u578b\u63d0\u5347\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u52a8\u9a7e\u9a76\uff0c\u901a\u8fc7\u5f02\u6b65\u6279\u91cf\u63a8\u7406\u6846\u67b6\u89e3\u51b3\u5ef6\u8fdf\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u591a\u79cd\u76d1\u7763\u673a\u5236\u4f18\u5316RL\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u9a7e\u9a76\u573a\u666f\u65f6\u5b58\u5728\u6570\u636e\u6536\u96c6\u6548\u7387\u4f4e\u548c\u4e0d\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\uff0c\u800c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u5177\u5907\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u77e5\u8bc6\u4f46\u5176\u63a8\u65ad\u5ef6\u8fdf\u5f71\u54cd\u4e86\u9ad8\u9891\u8bad\u7ec3\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u5229\u7528\u57fa\u7840\u6a21\u578b\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aFound-RL\u7684\u5e73\u53f0\uff0c\u901a\u8fc7\u5f02\u6b65\u6279\u91cf\u63a8\u7406\u6846\u67b6\u51cf\u5c11\u5ef6\u8fdf\uff0c\u5e76\u4f7f\u7528VMR\u548cAWAG\u673a\u5236\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u3002\u540c\u65f6\uff0c\u91c7\u7528\u9ad8\u541e\u5410\u91cf\u7684CLIP\u8fdb\u884c\u7a20\u5bc6\u5956\u52b1\u5851\u9020\uff0c\u5229\u7528\u6761\u4ef6\u5bf9\u6bd4\u52a8\u9759\u5bf9\u9f50\u5904\u7406CLIP\u52a8\u6001\u76f2\u533a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u7ed3\u5408VLM\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u4fdd\u6301\u63a5\u8fd1VLM\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u8fd1\u4e4e\u5b9e\u65f6\u7684\u63a8\u7406\uff08\u7ea6500 FPS\uff09\u3002", "conclusion": "\u672c\u6587\u9a8c\u8bc1\u4e86\u901a\u8fc7\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u548c\u6539\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6765\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u4e2d\u7684\u6311\u6218\u6027\u95ee\u9898\u3002"}}
{"id": "2602.10467", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10467", "abs": "https://arxiv.org/abs/2602.10467", "authors": ["Jihwan Oh", "Murad Aghazada", "Yooju Shin", "Se-Young Yun", "Taehyeon Kim"], "title": "MERIT Feedback Elicits Better Bargaining in LLM Negotiators", "comment": "Preprint. arXiv admin note: substantial text overlap with arXiv:2505.22998", "summary": "Bargaining is often regarded as a logical arena rather than an art or a matter of intuition, yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current benchmarks rarely capture this limitation. To bridge this gap, we present an utility feedback centric framework. Our contributions are: (i) AgoraBench, a new benchmark spanning nine challenging settings (e.g., deception, monopoly) that supports diverse strategy modeling; (ii) human-aligned, economically grounded metrics derived from utility theory. This is operationalized via agent utility, negotiation power, and acquisition ratio that implicitly measure how well the negotiation aligns with human preference and (iii) a human preference grounded dataset with learning pipeline that strengthens LLMs' bargaining ability through both prompting and finetuning. Empirical results indicate that baseline LLM strategies often diverge from human preferences, while our mechanism substantially improves negotiation performance, yielding deeper strategic behavior and stronger opponent awareness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6AgoraBench\uff0c\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8c08\u5224\u4e2d\u7684\u7b56\u7565\u6df1\u5ea6\u9650\u5236\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u6548\u7528\u53cd\u9988\u7684\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u4e5d\u79cd\u590d\u6742\u573a\u666f\u7684\u6311\u6218\u3002\u7814\u7a76\u8fd8\u5305\u62ec\u7ecf\u6d4e\u5bfc\u5411\u7684\u8bc4\u4f30\u6307\u6807\u548c\u4e00\u4e2a\u4ee5\u4eba\u7c7b\u504f\u597d\u4e3a\u57fa\u51c6\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u589e\u5f3aLLMs\u7684\u8c08\u5224\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u672a\u5145\u5206\u6355\u6349\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8c08\u5224\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u6b64\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u65b0\u7684\u57fa\u51c6\u548c\u8bc4\u5206\u6807\u51c6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86AgoraBench\u57fa\u51c6\uff0c\u5305\u542b\u4e5d\u79cd\u590d\u6742\u7684\u8c08\u5224\u573a\u666f\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u6548\u7528\u7684\u4eba\u7c7b\u5bf9\u9f50\u7ecf\u6d4e\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u96c6\u548c\u5b66\u4e60\u6d41\u7a0b\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u8c08\u5224\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8eAgoraBench\u673a\u5236\u7684LLMs\u5728\u8c08\u5224\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u66f4\u52a0\u7b26\u5408\u4eba\u7c7b\u504f\u597d\uff0c\u5c55\u73b0\u51fa\u66f4\u6df1\u5c42\u6b21\u7684\u6218\u7565\u884c\u4e3a\u548c\u66f4\u5f3a\u7684\u5bf9\u624b\u610f\u8bc6\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7AgoraBench\u53ca\u76f8\u5173\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u8c08\u5224\u573a\u666f\u4e2d\u7684\u7b56\u7565\u6df1\u5ea6\u548c\u7406\u8bba\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2602.10485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10485", "abs": "https://arxiv.org/abs/2602.10485", "authors": ["Zhenhe Cui", "Huaxiang Xia", "Hangjun Shen", "Kailun Luo", "Yong He", "Wei Liang"], "title": "Abstraction Generation for Generalized Planning with Pretrained Large Language Models", "comment": null, "summary": "Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: input a GP domain and training tasks to LLMs, prompting them to generate abstract features and further abstract the initial state, action set, and goal into QNP problems. An automated debugging method is designed to detect abstraction errors, guiding LLMs to fix abstractions. Experiments demonstrate that under properly guided by automated debugging, some LLMs can generate useful QNP abstractions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u4f5c\u4e3aQNP\u62bd\u8c61\u751f\u6210\u5668\u6765\u89e3\u51b3GP\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u81ea\u52a8\u751f\u6210QNP\u95ee\u9898\u7684\u63d0\u793a\u534f\u8bae\u53ca\u81ea\u52a8\u5316\u8c03\u8bd5\u65b9\u6cd5\u6765\u4fee\u6b63\u62bd\u8c61\u9519\u8bef\uff0c\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u81ea\u52a8\u5316\u8c03\u8bd5\u6b63\u786e\u5f15\u5bfc\u4e0b\uff0c\u67d0\u4e9b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u751f\u6210\u6709\u7528\u7684QNP\u62bd\u8c61\u3002", "motivation": "\u8fd1\u671f\u7814\u7a76\u663e\u793a\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u901a\u7528\u89c4\u5212\u5668\uff0c\u56e0\u6b64\u63a2\u7d22\u8fd9\u7c7b\u6a21\u578b\u5728Qualitative Numerical Planning (QNP)\u62bd\u8c61\u751f\u6210\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u793a\u534f\u8bae\u548c\u81ea\u52a8\u5316\u8c03\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f93\u5165GP\u9886\u57df\u548c\u8bad\u7ec3\u4efb\u52a1\uff0c\u4fc3\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u62bd\u8c61\u7279\u5f81\uff0c\u5e76\u5c06\u521d\u59cb\u72b6\u6001\u3001\u52a8\u4f5c\u96c6\u548c\u76ee\u6807\u62bd\u8c61\u6210QNP\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u81ea\u52a8\u5316\u8c03\u8bd5\u6b63\u786e\u5f15\u5bfc\u4e0b\uff0c\u67d0\u4e9b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u751f\u6210\u6709\u7528\u7684QNP\u62bd\u8c61\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728GP\u95ee\u9898\u4e2d\u7684QNP\u62bd\u8c61\u751f\u6210\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316\u63d0\u793a\u534f\u8bae\u548c\u8c03\u8bd5\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u62bd\u8c61\u8d28\u91cf\u3002"}}
{"id": "2602.10157", "categories": ["cs.CR", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.10157", "abs": "https://arxiv.org/abs/2602.10157", "authors": ["Yunpeng Tan", "Qingyang Li", "Mingxin Yang", "Yannan Hu", "Lei Zhang", "Xinggong Zhang"], "title": "MalMoE: Mixture-of-Experts Enhanced Encrypted Malicious Traffic Detection Under Graph Drift", "comment": "10 pages, 9 figures, accepted by IEEE INFOCOM 2026", "summary": "Encryption has been commonly used in network traffic to secure transmission, but it also brings challenges for malicious traffic detection, due to the invisibility of the packet payload. Graph-based methods are emerging as promising solutions by leveraging multi-host interactions to promote detection accuracy. But most of them face a critical problem: Graph Drift, where the flow statistics or topological information of a graph change over time. To overcome these drawbacks, we propose a graph-assisted encrypted traffic detection system, MalMoE, which applies Mixture of Experts (MoE) to select the best expert model for drift-aware classification. Particularly, we design 1-hop-GNN-like expert models that handle different graph drifts by analyzing graphs with different features. Then, the redesigned gate model conducts expert selection according to the actual drift. MalMoE is trained with a stable two-stage training strategy with data augmentation, which effectively guides the gate on how to perform routing. Experiments on open-source, synthetic, and real-world datasets show that MalMoE can perform precise and real-time detection.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10583", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10583", "abs": "https://arxiv.org/abs/2602.10583", "authors": ["Bo Xue", "Yunchong Song", "Fanghao Shao", "Xuekai Zhu", "Lin Chen", "Luoyi Fu", "Xinbing Wang", "Zhouhan Lin"], "title": "Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets", "comment": "Published as a conference paper at ICLR 2026", "summary": "Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a tree-structured state space when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but overlooks that the same sentence can be composed of spans of varying lengths, lacking explicit modeling of the directed acyclic graph (DAG) state space. This leads to restricted exploration of compositional paths and is biased toward the chosen path. Generative Flow Networks (GFlowNets) are powerful for efficient exploring and generalizing over state spaces, particularly those with a DAG structure. However, prior GFlowNets-based language models operate at the token level and remain confined to tree-structured spaces, limiting their potential. In this work, we propose Flow of SpanS (FOSS), a principled GFlowNets framework for span generation. FoSS constructs a dynamic span vocabulary by segmenting the retrieved text flexibly, ensuring a DAG-structured state space, which allows GFlowNets to explore diverse compositional paths and improve generalization. With specialized reward models, FoSS generates diverse, high-quality text. Empirically, FoSS improves MAUVE scores by up to 12.5% over Transformer on text generation and achieves 3.5% gains on knowledge-intensive tasks, consistently outperforming state-of-the-art methods. Scaling experiments further demonstrate FoSS benefits from larger models, more data, and richer retrieval corpora, retaining its advantage over strong baselines.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10161", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10161", "abs": "https://arxiv.org/abs/2602.10161", "authors": ["Kun Wang", "Zherui Li", "Zhenhong Zhou", "Yitong Zhang", "Yan Mi", "Kun Yang", "Yiming Zhang", "Junhao Dong", "Zhongxiang Sun", "Qiankun Li", "Yang Liu"], "title": "Omni-Safety under Cross-Modality Conflict: Vulnerabilities, Dynamics Mechanisms and Efficient Alignment", "comment": null, "summary": "Omni-modal Large Language Models (OLLMs) greatly expand LLMs' multimodal capabilities but also introduce cross-modal safety risks. However, a systematic understanding of vulnerabilities in omni-modal interactions remains lacking. To bridge this gap, we establish a modality-semantics decoupling principle and construct the AdvBench-Omni dataset, which reveals a significant vulnerability in OLLMs. Mechanistic analysis uncovers a Mid-layer Dissolution phenomenon driven by refusal vector magnitude shrinkage, alongside the existence of a modal-invariant pure refusal direction. Inspired by these insights, we extract a golden refusal vector using Singular Value Decomposition and propose OmniSteer, which utilizes lightweight adapters to modulate intervention intensity adaptively. Extensive experiments show that our method not only increases the Refusal Success Rate against harmful inputs from 69.9% to 91.2%, but also effectively preserves the general capabilities across all modalities. Our code is available at: https://github.com/zhrli324/omni-safety-research.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10598", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10598", "abs": "https://arxiv.org/abs/2602.10598", "authors": ["Shuai Han", "Mehdi Dastani", "Shihan Wang"], "title": "Neuro-symbolic Action Masking for Deep Reinforcement Learning", "comment": null, "summary": "Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10162", "categories": ["cs.CR", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.10162", "abs": "https://arxiv.org/abs/2602.10162", "authors": ["Chenhan Xiao", "Yang Weng"], "title": "Limits of Residual-Based Detection for Physically Consistent False Data Injection", "comment": "10 pages, 10 figures", "summary": "False data injection attacks (FDIAs) pose a persistent challenge to AC power system state estimation. In current practice, detection relies primarily on topology-aware residual-based tests that assume malicious measurements can be distinguished from normal operation through physical inconsistency reflected in abnormal residual behavior. This paper shows that this assumption does not always hold: when FDIA scenarios produce manipulated measurements that remain on the measurement manifold induced by AC power flow relations and measurement redundancy, residual-based detectors may fail to distinguish them from nominal data. The resulting detectability limitation is a property of the measurement manifold itself and does not depend on the attacker's detailed knowledge of the physical system model. To make this limitation observable in practice, we present a data-driven constructive mechanism that incorporates the generic functional structure of AC power flow to generate physically consistent, manifold-constrained perturbations, providing a concrete witness of how residual-based detectors can be bypassed. Numerical studies on multiple AC test systems characterize the conditions under which detection becomes challenging and illustrate its failure modes. The results highlight fundamental limits of residual-based detection in AC state estimation and motivate the need for complementary defenses beyond measurement consistency tests.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10625", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10625", "abs": "https://arxiv.org/abs/2602.10625", "authors": ["Nanxu Gong", "Haotian Li", "Sixun Dong", "Jianxun Lian", "Yanjie Fu", "Xing Xie"], "title": "To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks", "comment": null, "summary": "Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to further verify and mitigate the problems. With all results, our study highlights the advancement of LRMs in formal reasoning (e.g., math, code) cannot be fully transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10166", "categories": ["cs.CR", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.10166", "abs": "https://arxiv.org/abs/2602.10166", "authors": ["Tatsunori Ono"], "title": "MerkleSpeech: Public-Key Verifiable, Chunk-Localised Speech Provenance via Perceptual Fingerprints and Merkle Commitments", "comment": "16 pages, 4 figures, 3 tables", "summary": "Speech provenance goes beyond detecting whether a watermark is present. Real workflows involve splicing, quoting, trimming, and platform-level transforms that may preserve some regions while altering others. Neural watermarking systems have made strides in robustness and localised detection, but most deployments produce outputs with no third-party verifiable cryptographic proof tying a time segment to an issuer-signed original. Provenance standards like C2PA adopt signed manifests and Merkle-based fragment validation, yet their bindings target encoded assets and break under re-encoding or routine processing.\n  We propose MerkleSpeech, a system for public-key verifiable, chunk-localised speech provenance offering two tiers of assurance. The first, a robust watermark attribution layer (WM-only), survives common distribution transforms and answers \"was this chunk issued by a known party?\". The second, a strict cryptographic integrity layer (MSv1), verifies Merkle inclusion of the chunk's fingerprint under an issuer signature. The system computes perceptual fingerprints over short speech chunks, commits them in a Merkle tree whose root is signed with an issuer key, and embeds a compact in-band watermark payload carrying a random content identifier and chunk metadata sufficient to retrieve Merkle inclusion proofs from a repository. Once the payload is extracted, all subsequent verification steps (signature check, fingerprint recomputation, Merkle inclusion) use only public information. The result is a splice-aware timeline indicating which regions pass each tier and why any given region fails. We describe the protocol, provide pseudocode, and present experiments targeting very low false positive rates under resampling, bandpass filtering, and additive noise, informed by recent audits identifying neural codecs as a major stressor for post-hoc audio watermarks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10635", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10635", "abs": "https://arxiv.org/abs/2602.10635", "authors": ["Keane Ong", "Sabri Boughorbel", "Luwei Xiao", "Chanakya Ekbote", "Wei Dai", "Ao Qu", "Jingyao Wu", "Rui Mao", "Ehsan Hoque", "Erik Cambria", "Gianmarco Mengaldo", "Paul Pu Liang"], "title": "OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization", "comment": null, "summary": "To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10169", "categories": ["cs.CR", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.10169", "abs": "https://arxiv.org/abs/2602.10169", "authors": ["Nicolai Maisch", "Shengjian Chen", "Alexander Robertus", "Samed Ajdinovi\u0107", "Armin Lechler", "Alexander Verl", "Oliver Riedel"], "title": "Non-Fungible Blockchain Tokens for Traceable Online-Quality Assurance of Milled Workpieces", "comment": null, "summary": "This work presents a concept and implementation for the secure storage and transfer of quality-relevant data of milled workpieces from online-quality assurance processes enabled by real-time simulation models. It utilises Non-Fungible Tokens (NFT) to securely and interoperably store quality data in the form of an Asset Administration Shell (AAS) on a public Ethereum blockchain. Minted by a custom smart contract, the NFTs reference the metadata saved in the Interplanetary File System (IPFS), allowing new data from additional processing steps to be added in a flexible yet secure manner. The concept enables automated traceability throughout the value chain, minimising the need for time-consuming and costly repetitive manual quality checks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10699", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10699", "abs": "https://arxiv.org/abs/2602.10699", "authors": ["Jie Jiang", "Yangru Huang", "Zeyu Wang", "Changping Wang", "Yuling Xiong", "Jun Zhang", "Huan Yu"], "title": "Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation", "comment": null, "summary": "Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10250", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.10250", "abs": "https://arxiv.org/abs/2602.10250", "authors": ["Subangkar Karmaker Shanto", "Imtiaz Karim", "Elisa Bertino"], "title": "Breaking 5G on The Lower Layer", "comment": "FutureG'26, co-located with NDSS 2026", "summary": "As 3GPP systems have strengthened security at the upper layers of the cellular stack, plaintext PHY and MAC layers have remained relatively understudied, though interest in them is growing. In this work, we explore lower-layer exploitation in modern 5G, where recent releases have increased the number of lower-layer control messages and procedures, creating new opportunities for practical attacks. We present two practical attacks and evaluate them in a controlled lab testbed. First, we reproduce a SIB1 spoofing attack to study manipulations of unprotected broadcast fields. By repeatedly changing a key parameter, the UE is forced to refresh and reacquire system information, keeping the radio interface active longer than necessary and increasing battery consumption. Second, we demonstrate a new Timing Advance (TA) manipulation attack during the random access procedure. By injecting an attacker-chosen TA offset in the random access response, the victim applies incorrect uplink timing, which leads to uplink desynchronization, radio link failures, and repeated reconnection loops that effectively cause denial of service. Our experiments use commercial smartphones and open-source 5G network software. Experimental results in our testbed demonstrate that TA offsets exceeding a small tolerance reliably trigger radio link failures in our testbed and can keep devices stuck in repeated re-establishment attempts as long as the rogue base station remains present. Overall, our findings highlight that compact lower-layer control messages can have a significant impact on availability and power, and they motivate placing defenses for initial access and broadcast procedures.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10802", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10802", "abs": "https://arxiv.org/abs/2602.10802", "authors": ["Da-Lun Chen", "Prasasthy Balasubramanian", "Lauri Lov\u00e9n", "Susanna Pirttikangas", "Jaakko Sauvola", "Panagiotis Kostakos"], "title": "Integrating Generative AI-enhanced Cognitive Systems in Higher Education: From Stakeholder Perceptions to a Conceptual Framework considering the EU AI Act", "comment": null, "summary": "Many staff and students in higher education have adopted generative artificial intelligence (GenAI) tools in their work and study. GenAI is expected to enhance cognitive systems by enabling personalized learning and streamlining educational services. However, stakeholders perceptions of GenAI in higher education remain divided, shaped by cultural, disciplinary, and institutional contexts. In addition, the EU AI Act requires universities to ensure regulatory compliance when deploying cognitive systems. These developments highlight the need for institutions to engage stakeholders and tailor GenAI integration to their needs while addressing concerns. This study investigates how GenAI is perceived within the disciplines of Information Technology and Electrical Engineering (ITEE). Using a mixed-method approach, we surveyed 61 staff and 37 students at the Faculty of ITEE, University of Oulu. The results reveal both shared and discipline-specific themes, including strong interest in programming support from GenAI and concerns over response quality, privacy, and academic integrity. Drawing from these insights, the study identifies a set of high-level requirements and proposes a conceptual framework for responsible GenAI integration. Disciplinary-specific requirements reinforce the importance of stakeholder engagement when integrating GenAI into higher education. The high-level requirements and the framework provide practical guidance for universities aiming to harness GenAI while addressing stakeholder concerns and ensuring regulatory compliance.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10272", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.10272", "abs": "https://arxiv.org/abs/2602.10272", "authors": ["Simon Erni", "Martin Kotuliak", "Marc Roeschlin", "Richard Baker", "Srdjan Capkun"], "title": "5Gone: Uplink Overshadowing Attacks in 5G-SA", "comment": null, "summary": "5G presents numerous advantages compared to previous generations: improved throughput, lower latency, and improved privacy protection for subscribers. Attacks against 5G standalone (SA) commonly use fake base stations (FBS), which need to operate at a very high output power level to lure victim phones to connect to them and are thus highly detectable. In this paper, we introduce 5Gone, a powerful software-defined radio (SDR)-based uplink overshadowing attack method against 5G-SA. 5Gone exploits deficiencies in the 3GPP standard to perform surgical, covert denial-of-service, privacy, and downgrade attacks. Uplink overshadowing means that an attacker is transmitting at exactly the same time and frequency as the victim UE, but with a slightly higher output power. 5Gone runs on a COTS x86 computer without any need for dedicated hardware acceleration and can overshadow commercial 100 MHz cells with an E2E latency of less than 500$\u03bc$s, which up to now has not been possible with any software-based UE implementation. We demonstrate that 5Gone is highly scalable, even when many UEs are connecting in parallel, and finally evaluate the attacks end-to-end against 7 phone models and three different chipset vendors both in our lab and in the real-world on public gNodeBs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10814", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10814", "abs": "https://arxiv.org/abs/2602.10814", "authors": ["Xingyi Zhang", "Yulei Ye", "Kaifeng Huang", "Wenhao Li", "Xiangfeng Wang"], "title": "See, Plan, Snap: Evaluating Multimodal GUI Agents in Scratch", "comment": null, "summary": "Block-based programming environments such as Scratch play a central role in low-code education, yet evaluating the capabilities of AI agents to construct programs through Graphical User Interfaces (GUIs) remains underexplored. We introduce ScratchWorld, a benchmark for evaluating multimodal GUI agents on program-by-construction tasks in Scratch. Grounded in the Use-Modify-Create pedagogical framework, ScratchWorld comprises 83 curated tasks spanning four distinct problem categories: Create, Debug, Extend, and Compute. To rigorously diagnose the source of agent failures, the benchmark employs two complementary interaction modes: primitive mode requires fine-grained drag-and-drop manipulation to directly assess visuomotor control, while composite mode uses high-level semantic APIs to disentangle program reasoning from GUI execution. To ensure reliable assessment, we propose an execution-based evaluation protocol that validates the functional correctness of the constructed Scratch programs through runtime tests within the browser environment. Extensive experiments across state-of-the-art multimodal language models and GUI agents reveal a substantial reasoning--acting gap, highlighting persistent challenges in fine-grained GUI manipulation despite strong planning capabilities.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10299", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.10299", "abs": "https://arxiv.org/abs/2602.10299", "authors": ["Kyle Domico", "Jean-Charles Noirot Ferrand", "Patrick McDaniel"], "title": "The Role of Learning in Attacking Intrusion Detection Systems", "comment": null, "summary": "Recent work on network attacks have demonstrated that ML-based network intrusion detection systems (NIDS) can be evaded with adversarial perturbations. However, these attacks rely on complex optimizations that have large computational overheads, making them impractical in many real-world settings. In this paper, we introduce a lightweight adversarial agent that implements strategies (policies) trained via reinforcement learning (RL) that learn to evade ML-based NIDS without requiring online optimization. This attack proceeds by (1) offline training, where the agent learns to evade a surrogate ML model by perturbing malicious flows using network traffic data assumed to be collected via reconnaissance, then (2) deployment, where the trained agent is used in a compromised device controlled by an attacker to evade ML-based NIDS using learned attack strategies. We evaluate our approach across diverse NIDS and several white-, gray-, and black-box threat models. We demonstrate that attacks using these lightweight agents can be highly effective (reaching up to 48.9% attack success rate), extremely fast (requiring as little as 5.72ms to craft an attack), and require negligible resources (e.g., 0.52MB of memory). Through this work, we demonstrate that future botnets driven by lightweight learning-based agents can be highly effective and widely deployable in diverse environments of compromised devices.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10845", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10845", "abs": "https://arxiv.org/abs/2602.10845", "authors": ["Xuecheng Zou", "Yu Tang", "Bingbing Wang"], "title": "SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy", "comment": "10 pages, 5 tables, 7 figures. This work introduces the Active Synergy mechanism and Identity Anchoring for Knowledge Graph Completion. Code: https://github.com/XuechengZou-2001/SynergyKGC-main", "summary": "Knowledge Graph Completion (KGC) fundamentally hinges on the coherent fusion of pre-trained entity semantics with heterogeneous topological structures to facilitate robust relational reasoning. However, existing paradigms encounter a critical \"structural resolution mismatch,\" failing to reconcile divergent representational demands across varying graph densities, which precipitates structural noise interference in dense clusters and catastrophic representation collapse in sparse regions. We present SynergyKGC, an adaptive framework that advances traditional neighbor aggregation to an active Cross-Modal Synergy Expert via relation-aware cross-attention and semantic-intent-driven gating. By coupling a density-dependent Identity Anchoring strategy with a Double-tower Coherent Consistency architecture, SynergyKGC effectively reconciles topological heterogeneity while ensuring representational stability across training and inference phases. Systematic evaluations on two public benchmarks validate the superiority of our method in significantly boosting KGC hit rates, providing empirical evidence for a generalized principle of resilient information integration in non-homogeneous structured data.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10418", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.10418", "abs": "https://arxiv.org/abs/2602.10418", "authors": ["Weichen Yu", "Ravi Mangal", "Yinyi Luo", "Kai Hu", "Jingxuan He", "Corina S. Pasareanu", "Matt Fredrikson"], "title": "SecCodePRM: A Process Reward Model for Code Security", "comment": "20 pages", "summary": "Large Language Models are rapidly becoming core components of modern software development workflows, yet ensuring code security remains challenging. Existing vulnerability detection pipelines either rely on static analyzers or use LLM/GNN-based detectors trained with coarse program-level supervision. Both families often require complete context, provide sparse end-of-completion feedback, and can degrade as code length grows, making them ill-suited for real-time, prefix-level assessment during interactive coding and streaming generation. We propose SecCodePRM, a security-oriented process reward model that assigns a context-aware, step-level security score along a code trajectory. To train the model, we derive step-level supervision labels from static analyzers and expert annotations, allowing the model to attend more precisely to fine-grained regions associated with inter-procedural vulnerabilities. SecCodePRM has three applications: full-code vulnerability detection (VD), partial-code VD, and secure code generation (CG). For VD, SecCodePRM uses risk-sensitive aggregation that emphasizes high-risk steps; for CG, SecCodePRM supports inference-time scaling by ranking candidate continuations and favoring higher cumulative reward. This design yields dense, real-time feedback that scales to long-horizon generation. Empirically, SecCodePRM outperforms prior approaches in all three settings, while preserving code functional correctness, suggesting improved security without a safety-utility tradeoff.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10885", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10885", "abs": "https://arxiv.org/abs/2602.10885", "authors": ["Leheng Sheng", "Wenchang Ma", "Ruixin Hong", "Xiang Wang", "An Zhang", "Tat-Seng Chua"], "title": "Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics", "comment": "21 pages", "summary": "Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \\textbf{RLCER} (\\textbf{R}einforcement \\textbf{L}earning with \\textbf{C}oT Supervision via Self-\\textbf{E}volving \\textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10453", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10453", "abs": "https://arxiv.org/abs/2602.10453", "authors": ["Peiran Wang", "Xinfeng Li", "Chong Xiang", "Jinghuai Zhang", "Ying Li", "Lixia Zhang", "Xiaofeng Wang", "Yuan Tian"], "title": "The Landscape of Prompt Injection Threats in LLM Agents: From Taxonomy to Analysis", "comment": null, "summary": "The evolution of Large Language Models (LLMs) has resulted in a paradigm shift towards autonomous agents, necessitating robust security against Prompt Injection (PI) vulnerabilities where untrusted inputs hijack agent behaviors. This SoK presents a comprehensive overview of the PI landscape, covering attacks, defenses, and their evaluation practices. Through a systematic literature review and quantitative analysis, we establish taxonomies that categorize PI attacks by payload generation strategies (heuristic vs. optimization) and defenses by intervention stages (text, model, and execution levels). Our analysis reveals a key limitation shared by many existing defenses and benchmarks: they largely overlook context-dependent tasks, in which agents are authorized to rely on runtime environmental observations to determine actions. To address this gap, we introduce AgentPI, a new benchmark designed to systematically evaluate agent behavior under context-dependent interaction settings. Using AgentPI, we empirically evaluate representative defenses and show that no single approach can simultaneously achieve high trustworthiness, high utility, and low latency. Moreover, we show that many defenses appear effective under existing benchmarks by suppressing contextual inputs, yet fail to generalize to realistic agent settings where context-dependent reasoning is essential. This SoK distills key takeaways and open research problems, offering structured guidance for future research and practical deployment of secure LLM agents.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10964", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10964", "abs": "https://arxiv.org/abs/2602.10964", "authors": ["F. Carichon", "R. Rampa", "G. Farnadi"], "title": "Can LLMs Cook Jamaican Couscous? A Study of Cultural Novelty in Recipe Generation", "comment": "14 pages, 12 figures, conference", "summary": "Large Language Models (LLMs) are increasingly used to generate and shape cultural content, ranging from narrative writing to artistic production. While these models demonstrate impressive fluency and generative capacity, prior work has shown that they also exhibit systematic cultural biases, raising concerns about stereotyping, homogenization, and the erasure of culturally specific forms of expression. Understanding whether LLMs can meaningfully align with diverse cultures beyond the dominant ones remains a critical challenge. In this paper, we study cultural adaptation in LLMs through the lens of cooking recipes, a domain in which culture, tradition, and creativity are tightly intertwined. We build on the \\textit{GlobalFusion} dataset, which pairs human recipes from different countries according to established measures of cultural distance. Using the same country pairs, we generate culturally adapted recipes with multiple LLMs, enabling a direct comparison between human and LLM behavior in cross-cultural content creation. Our analysis shows that LLMs fail to produce culturally representative adaptations. Unlike humans, the divergence of their generated recipes does not correlate with cultural distance. We further provide explanations for this gap. We show that cultural information is weakly preserved in internal model representations, that models inflate novelty in their production by misunderstanding notions such as creativity and tradition, and that they fail to identify adaptation with its associated countries and to ground it in culturally salient elements such as ingredients. These findings highlight fundamental limitations of current LLMs for culturally oriented generation and have important implications for their use in culturally sensitive applications.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10465", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.10465", "abs": "https://arxiv.org/abs/2602.10465", "authors": ["Mohan Rajagopalan", "Vinay Rao"], "title": "Authenticated Workflows: A Systems Approach to Protecting Agentic AI", "comment": null, "summary": "Agentic AI systems automate enterprise workflows but existing defenses--guardrails, semantic filters--are probabilistic and routinely bypassed. We introduce authenticated workflows, the first complete trust layer for enterprise agentic AI. Security reduces to protecting four fundamental boundaries: prompts, tools, data, and context. We enforce intent (operations satisfy organizational policies) and integrity (operations are cryptographically authentic) at every boundary crossing, combining cryptographic elimination of attack classes with runtime policy enforcement. This delivers deterministic security--operations either carry valid cryptographic proof or are rejected. We introduce MAPL, an AI-native policy language that expresses agentic constraints dynamically as agents evolve and invocation context changes, scaling as O(log M + N) policies versus O(M x N) rules through hierarchical composition with cryptographic attestations for workflow dependencies. We prove practicality through a universal security runtime integrating nine leading frameworks (MCP, A2A, OpenAI, Claude, LangChain, CrewAI, AutoGen, LlamaIndex, Haystack) through thin adapters requiring zero protocol modifications. Formal proofs establish completeness and soundness. Empirical validation shows 100% recall with zero false positives across 174 test cases, protection against 9 of 10 OWASP Top 10 risks, and complete mitigation of two high impact production CVEs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10999", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10999", "abs": "https://arxiv.org/abs/2602.10999", "authors": ["Yusong Lin", "Haiyang Wang", "Shuzhe Wu", "Lue Fan", "Feiyang Pan", "Sanyuan Zhao", "Dandan Tu"], "title": "CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion", "comment": null, "summary": "Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10478", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10478", "abs": "https://arxiv.org/abs/2602.10478", "authors": ["Zihao Li", "Hongyi Lu", "Yanan Guo", "Zhenkai Zhang", "Shuai Wang", "Fengwei Zhang"], "title": "GPU-Fuzz: Finding Memory Errors in Deep Learning Frameworks", "comment": null, "summary": "GPU memory errors are a critical threat to deep learning (DL) frameworks, leading to crashes or even security issues. We introduce GPU-Fuzz, a fuzzer locating these issues efficiently by modeling operator parameters as formal constraints. GPU-Fuzz utilizes a constraint solver to generate test cases that systematically probe error-prone boundary conditions in GPU kernels. Applied to PyTorch, TensorFlow, and PaddlePaddle, we uncovered 13 unknown bugs, demonstrating the effectiveness of GPU-Fuzz in finding memory errors.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.11103", "categories": ["cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11103", "abs": "https://arxiv.org/abs/2602.11103", "authors": ["Wayne Chi", "Yixiong Fang", "Arnav Yayavaram", "Siddharth Yayavaram", "Seth Karten", "Qiuhong Anna Wei", "Runkun Chen", "Alexander Wang", "Valerie Chen", "Ameet Talwalkar", "Chris Donahue"], "title": "GameDevBench: Evaluating Agentic Capabilities Through Game Development", "comment": null, "summary": "Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5's performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10481", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.10481", "abs": "https://arxiv.org/abs/2602.10481", "authors": ["Mohan Rajagopalan", "Vinay Rao"], "title": "Protecting Context and Prompts: Deterministic Security for Non-Deterministic AI", "comment": null, "summary": "Large Language Model (LLM) applications are vulnerable to prompt injection and context manipulation attacks that traditional security models cannot prevent. We introduce two novel primitives--authenticated prompts and authenticated context--that provide cryptographically verifiable provenance across LLM workflows. Authenticated prompts enable self-contained lineage verification, while authenticated context uses tamper-evident hash chains to ensure integrity of dynamic inputs. Building on these primitives, we formalize a policy algebra with four proven theorems providing protocol-level Byzantine resistance--even adversarial agents cannot violate organizational policies. Five complementary defenses--from lightweight resource controls to LLM-based semantic validation--deliver layered, preventative security with formal guarantees. Evaluation against representative attacks spanning 6 exhaustive categories achieves 100% detection with zero false positives and nominal overhead. We demonstrate the first approach combining cryptographically enforced prompt lineage, tamper-evident context, and provable policy reasoning--shifting LLM security from reactive detection to preventative guarantees.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.11136", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11136", "abs": "https://arxiv.org/abs/2602.11136", "authors": ["Jiayi Zhou", "Yang Sheng", "Hantao Lou", "Yaodong Yang", "Jie Fu"], "title": "FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight", "comment": "27 pages", "summary": "As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10487", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.10487", "abs": "https://arxiv.org/abs/2602.10487", "authors": ["Viet Hoang Luu", "Amirmohammad Pasdar", "Wachiraphan Charoenwet", "Toby Murray", "Shaanan Cohney", "Van-Thuan Pham"], "title": "Following Dragons: Code Review-Guided Fuzzing", "comment": null, "summary": "Modern fuzzers scale to large, real-world software but often fail to exercise the program states developers consider most fragile or security-critical. Such states are typically deep in the execution space, gated by preconditions, or overshadowed by lower-value paths that consume limited fuzzing budgets. Meanwhile, developers routinely surface risk-relevant insights during code review, yet this information is largely ignored by automated testing tools. We present EyeQ, a system that leverages developer intelligence from code reviews to guide fuzzing. EyeQ extracts security-relevant signals from review discussions, localizes the implicated program regions, and translates these insights into annotation-based guidance for fuzzing. The approach operates atop existing annotation-aware fuzzing, requiring no changes to program semantics or developer workflows. We first validate EyeQ through a human-guided feasibility study on a security-focused dataset of PHP code reviews, establishing a strong baseline for review-guided fuzzing. We then automate the workflow using a large language model with carefully designed prompts. EyeQ significantly improves vulnerability discovery over standard fuzzing configurations, uncovering more than 40 previously unknown bugs in the security-critical PHP codebase.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.10498", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.10498", "abs": "https://arxiv.org/abs/2602.10498", "authors": ["Qianli Wang", "Boyang Ma", "Minghui Xu", "Yue Zhang"], "title": "When Skills Lie: Hidden-Comment Injection in LLM Agents", "comment": "4 pages", "summary": "LLM agents often rely on Skills to describe available tools and recommended procedures. We study a hidden-comment prompt injection risk in this documentation layer: when a Markdown Skill is rendered to HTML, HTML comment blocks can become invisible to human reviewers, yet the raw text may still be supplied verbatim to the model. In experiments, we find that DeepSeek-V3.2 and GLM-4.5-Air can be influenced by malicious instructions embedded in a hidden comment appended to an otherwise legitimate Skill, yielding outputs that contain sensitive tool intentions. A short defensive system prompt that treats Skills as untrusted and forbids sensitive actions prevents these malicious tool calls and instead surfaces the suspicious hidden instructions.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0cLLM\u5728\u5904\u7406Markdown\u6280\u80fd\u65f6\u53ef\u80fd\u5b58\u5728\u9690\u85cf\u8bc4\u8bba\u6ce8\u5165\u98ce\u9669\u3002\u63ed\u9732\u8fd9\u79cd\u98ce\u9669\u5e76\u63d0\u51fa\u9632\u8303\u63aa\u65bd\uff0c\u786e\u4fdd\u6280\u80fd\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8LLM\u7cfb\u7edf\u4e2d\u6280\u80fd\u5c42\u7684\u900f\u660e\u5ea6\u548c\u5b89\u5168\u6027\uff0c\u7814\u7a76\u8005\u4eec\u53d1\u73b0\u5f53Markdown\u6280\u80fd\u8f6c\u6362\u4e3aHTML\u65f6\uff0c\u53ef\u80fd\u4f1a\u5b58\u5728\u9690\u85cf\u7684\u8bc4\u8bba\u6ce8\u5165\u98ce\u9669\uff0c\u8fdb\u800c\u9700\u8981\u8bbe\u8ba1\u4e00\u79cd\u9632\u5fa1\u6027\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6027\u653b\u51fb\u9a8c\u8bc1\u6f5c\u5728\u98ce\u9669\uff0c\u8bbe\u8ba1\u53cd\u5411\u63d0\u793a\u7cfb\u7edf\u963b\u6b62\u6f5c\u5728\u7684\u6076\u610f\u5de5\u5177\u8c03\u7528\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0cDeepSeek-V3.2\u548cGLM-4.5-Air\u7b49\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u9690\u85cf\u8bc4\u8bba\u4e2d\u7684\u6076\u610f\u6307\u4ee4\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u63d0\u51fa\u5c06\u6280\u80fd\u89c6\u4e3a\u4e0d\u53ef\u4fe1\u4efb\u5e76\u7981\u6b62\u654f\u611f\u64cd\u4f5c\u7684\u9632\u5fa1\u6027\u63d0\u793a\uff0c\u53ef\u4ee5\u6709\u6548\u963b\u6b62\u6076\u610f\u5de5\u5177\u8c03\u7528\u5e76\u63ed\u9732\u53ef\u7591\u7684\u9690\u85cf\u6307\u4ee4\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u6280\u80fd\u5c42\u4e2d\u9690\u542b\u7684\u6ce8\u5165\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u7528\u7684\u9632\u5fa1\u63aa\u65bd\uff0c\u4ee5\u4fbf\u5728\u4fdd\u62a4LLM\u7cfb\u7edf\u65f6\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2602.10573", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.10573", "abs": "https://arxiv.org/abs/2602.10573", "authors": ["Ruisheng Shi", "Ziding Lin", "Haoran Sun", "Qin Wang", "Shihan Zhang", "Lina Lan", "Zhiyuan Peng", "Chenfeng Wang"], "title": "CryptoCatch: Cryptomining Hidden Nowhere", "comment": "IEEE TDSC with DOI 10.1109/TDSC.2026.3661145", "summary": "Cryptomining poses significant security risks, yet traditional detection methods like blacklists and Deep Packet Inspection (DPI) are often ineffective against encrypted mining traffic and suffer from high false positive rates. In this paper, we propose a practical encrypted cryptomining traffic detection mechanism. It consists of a two-stage detection framework, which can effectively provide fine-grained detection results by machine learning and reduce false positives from classifiers through active probing. Our system achieves an F1-score of 0.99 and identifies specific cryptocurrencies with a 99.39\\% accuracy rate. Extensive testing across various mining pools confirms the effectiveness of our approach, offering a more precise and reliable solution for identifying cryptomining activities.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u52a0\u5bc6\u52a0\u5bc6\u6316\u77ff\u6d41\u91cf\u68c0\u6d4b\u673a\u5236\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u68c0\u6d4b\u6846\u67b6\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u4e3b\u52a8\u63a2\u6d4b\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u8bef\u62a5\uff0c\u5e76\u5728\u4e0d\u540c\u6316\u77ff\u6c60\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u6d4b\u8bd5\u3002", "motivation": "\u9488\u5bf9\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u5982\u9ed1\u540d\u5355\u548c\u6df1\u5ea6\u5305\u68c0\u67e5\uff08DPI\uff09\u5728\u52a0\u5bc6\u6316\u77ff\u6d41\u91cf\u68c0\u6d4b\u4e2d\u7684\u4e0d\u8db3\uff0c\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u68c0\u6d4b\u673a\u5236\u3002", "method": "\u6587\u7ae0\u91c7\u7528\u4e24\u9636\u6bb5\u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u548c\u4e3b\u52a8\u63a2\u6d4b\u6280\u672f\u6765\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u51cf\u5c11\u8bef\u62a5\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e860.99\u7684F1\u5206\u6570\uff0c\u5e76\u4e14\u80fd\u591f\u4ee599.39%\u7684\u51c6\u786e\u7387\u8bc6\u522b\u7279\u5b9a\u7684\u52a0\u5bc6\u8d27\u5e01\u3002\u5728\u591a\u4e2a\u6316\u77ff\u6c60\u4e2d\u7684\u6d4b\u8bd5\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6587\u7ae0\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u52a0\u5bc6\u6316\u77ff\u6d41\u91cf\u7684\u68c0\u6d4b\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u80fd\u591f\u66f4\u7cbe\u51c6\u5730\u8bc6\u522b\u52a0\u5bc6\u6316\u77ff\u6d3b\u52a8\u3002"}}
{"id": "2602.10626", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.10626", "abs": "https://arxiv.org/abs/2602.10626", "authors": ["Ruisheng Shi", "Zhiyuan Peng", "Tong Fu", "Lina Lan", "Qin Wang", "Jiaqi Zeng"], "title": "Invisible Trails? An Identity Alignment Scheme based on Online Tracking", "comment": "IEEE TDSC with DOI 10.1109/TDSC.2025.3627604", "summary": "Many tracking companies collect user data and sell it to data markets and advertisers. While they claim to protect user privacy by anonymizing the data, our research reveals that significant privacy risks persist even with anonymized data. Attackers can exploit this data to identify users' accounts on other websites and perform targeted identity alignment. In this paper, we propose an effective identity alignment scheme for accurately identifying targeted users. We develop a data collector to obtain the necessary datasets, an algorithm for identity alignment, and, based on this, construct two types of de-anonymization attacks: the \\textit{passive attack}, which analyzes tracker data to align identities, and the \\textit{active attack}, which induces users to interact online, leading to higher success rates. Furthermore, we introduce, for the first time, a novel evaluation framework for online tracking-based identity alignment. We investigate the key factors influencing the effectiveness of identity alignment. Additionally, we provide an independent assessment of our generated dataset and present a fully functional system prototype applied to a cryptocurrency use case.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5373\u4f7f\u7ecf\u8fc7\u8131\u654f\u5904\u7406\u7684\u6570\u636e\u4ecd\u5b58\u5728\u4e25\u91cd\u9690\u79c1\u98ce\u9669\uff0c\u653b\u51fb\u8005\u53ef\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u8fdb\u884c\u8eab\u4efd\u5bf9\u9f50\u653b\u51fb\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u8eab\u4efd\u5bf9\u9f50\u65b9\u6848\uff0c\u5305\u62ec\u88ab\u52a8\u653b\u51fb\u548c\u4e3b\u52a8\u653b\u51fb\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u8bc4\u4f30\u5728\u7ebf\u8ffd\u8e2a\u4e2d\u7684\u8eab\u4efd\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u968f\u7740\u6570\u636e\u5e02\u573a\u548c\u5e7f\u544a\u5546\u5bf9\u7528\u6237\u6570\u636e\u7684\u6536\u96c6\u548c\u5229\u7528\uff0c\u7528\u6237\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u5373\u4f7f\u6570\u636e\u7ecf\u8fc7\u8131\u654f\u5904\u7406\uff0c\u653b\u51fb\u8005\u4ecd\u80fd\u901a\u8fc7\u8eab\u4efd\u5bf9\u9f50\u653b\u51fb\u83b7\u53d6\u7528\u6237\u9690\u79c1\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7814\u7a76\u548c\u63d0\u51fa\u6709\u6548\u7684\u8eab\u4efd\u5bf9\u9f50\u65b9\u6848\u53ca\u5176\u76f8\u5e94\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u9996\u5148\u5f00\u53d1\u4e86\u4e00\u4e2a\u6570\u636e\u6536\u96c6\u5668\u6765\u83b7\u53d6\u5fc5\u8981\u7684\u6570\u636e\u96c6\uff0c\u7136\u540e\u63d0\u51fa\u4e86\u4e00\u4e2a\u8eab\u4efd\u5bf9\u9f50\u7b97\u6cd5\u3002\u6839\u636e\u6b64\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u88ab\u52a8\u653b\u51fb\u548c\u4e3b\u52a8\u653b\u51fb\u4e24\u79cd\u7c7b\u578b\u7684\u53bb\u533f\u540d\u5316\u653b\u51fb\u3002\u6700\u540e\uff0c\u5f15\u5165\u4e86\u9996\u4e2a\u57fa\u4e8e\u5728\u7ebf\u8ffd\u8e2a\u7684\u8eab\u4efd\u5bf9\u9f50\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u88ab\u52a8\u653b\u51fb\u548c\u4e3b\u52a8\u653b\u51fb\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u5e76\u7814\u7a76\u4e86\u5f71\u54cd\u8eab\u4efd\u5bf9\u9f50\u6548\u679c\u7684\u5173\u952e\u56e0\u7d20\u3002\u6b64\u5916\uff0c\u8fd8\u5bf9\u751f\u6210\u7684\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u72ec\u7acb\u8bc4\u4f30\uff0c\u5e76\u5c55\u793a\u4e86\u5e94\u7528\u4e8e\u52a0\u5bc6\u8d27\u5e01\u573a\u666f\u7684\u5b8c\u6574\u7cfb\u7edf\u539f\u578b\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u63d0\u51fa\u6709\u6548\u7684\u8eab\u4efd\u5bf9\u9f50\u65b9\u6848\u53ca\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u89e3\u51b3\u5728\u7ebf\u8ffd\u8e2a\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\u3002"}}
{"id": "2602.10750", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10750", "abs": "https://arxiv.org/abs/2602.10750", "authors": ["Rumman Firdos", "Aman Dangi"], "title": "SecureScan: An AI-Driven Multi-Layer Framework for Malware and Phishing Detection Using Logistic Regression and Threat Intelligence Integration", "comment": null, "summary": "The growing sophistication of modern malware and phishing campaigns has diminished the effectiveness of traditional signature-based intrusion detection systems. This work presents SecureScan, an AI-driven, triple-layer detection framework that integrates logistic regression-based classification, heuristic analysis, and external threat intelligence via the VirusTotal API for comprehensive triage of URLs, file hashes, and binaries. The proposed architecture prioritizes efficiency by filtering known threats through heuristics, classifying uncertain samples using machine learning, and validating borderline cases with third-party intelligence. On benchmark datasets, SecureScan achieves 93.1 percent accuracy with balanced precision (0.87) and recall (0.92), demonstrating strong generalization and reduced overfitting through threshold-based decision calibration. A calibrated threshold and gray-zone logic (0.45-0.55) were introduced to minimize false positives and enhance real-world stability. Experimental results indicate that a lightweight statistical model, when augmented with calibrated verification and external intelligence, can achieve reliability and performance comparable to more complex deep learning systems.", "AI": {"tldr": "SecureScan\u662f\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u4e09\u5c42\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u903b\u8f91\u56de\u5f52\u5206\u7c7b\u3001\u542f\u53d1\u5f0f\u5206\u6790\u548c\u5a01\u80c1\u60c5\u62a5\uff0c\u5b9e\u73b0\u4e86\u5bf9URL\u3001\u6587\u4ef6\u54c8\u5e0c\u548c\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684\u5168\u9762\u7b5b\u9009\u3002\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa93.1%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u4f9d\u8d56\u7b7e\u540d\u7684\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u5bf9\u4e8e\u73b0\u4ee3\u590d\u6742\u6076\u610f\u8f6f\u4ef6\u548c\u9493\u9c7c\u653b\u51fb\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AI\u9a71\u52a8\u7684\u4e09\u91cd\u68c0\u6d4b\u6846\u67b6\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u4e86\u903b\u8f91\u56de\u5f52\u4f5c\u4e3a\u5206\u7c7b\u5de5\u5177\uff0c\u7ed3\u5408\u542f\u53d1\u5f0f\u5206\u6790\u548c\u7b2c\u4e09\u65b9\u5a01\u80c1\u60c5\u62a5\uff08\u901a\u8fc7VirusTotal API\u83b7\u53d6\uff09\u8fdb\u884c\u591a\u5c42\u6b21\u7684\u5b89\u5168\u68c0\u6d4b\u3002", "result": "SecureScan\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u8fbe\u523093.1%\uff0c\u7cbe\u786e\u5ea6\u4e3a0.87\uff0c\u53ec\u56de\u7387\u4e3a0.92\uff0c\u901a\u8fc7\u9608\u503c\u6821\u6b63\u51b3\u7b56\u673a\u5236\u51cf\u5c11\u4e86\u8fc7\u62df\u5408\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u7684\u7edf\u8ba1\u6a21\u578b\uff0c\u5728\u7ecf\u8fc7\u6821\u9a8c\u548c\u5916\u90e8\u60c5\u62a5\u589e\u5f3a\u540e\uff0c\u53ef\u4ee5\u5b9e\u73b0\u4e0e\u66f4\u590d\u6742\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u76f8\u5f53\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2602.10778", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.10778", "abs": "https://arxiv.org/abs/2602.10778", "authors": ["Maximilian Thang", "Lichao Wu", "Sasha Behrouzi", "Mohamadreza Rostami", "Jona te Lintelo", "Stjepan Picek", "Ahmad-Reza Sadeghi"], "title": "GoodVibe: Security-by-Vibe for LLM-Based Code Generation", "comment": null, "summary": "Large language models (LLMs) are increasingly used for code generation in fast, informal development workflows, often referred to as vibe coding, where speed and convenience are prioritized, and security requirements are rarely made explicit. In this setting, models frequently produce functionally correct but insecure code, creating a growing security risk. Existing approaches to improving code security rely on full-parameter fine-tuning or parameter-efficient adaptations, which are either costly and prone to catastrophic forgetting or operate at coarse granularity with limited interpretability and control.\n  We present GoodVibe, a neuron-level framework for improving the security of code language models by default. GoodVibe is based on the key insight that security-relevant reasoning is localized to a small subset of neurons. We identify these neurons using gradient-based attribution from a supervised security task and perform neuron-selective fine-tuning that updates only this security-critical subspace. To further reduce training cost, we introduce activation-driven neuron clustering, enabling structured updates with minimal overhead. We evaluate GoodVibe on six LLMs across security-critical programming languages, including C++, Java, Swift, and Go. GoodVibe substantially improves the security of generated code while preserving general model utility, achieving up to a 2.5x improvement over base models, matching or exceeding full fine-tuning with over 4,700x fewer trainable parameters, and reducing training computation by more than 3.6x compared to the parameter-efficient baseline (LoRA). Our results demonstrate that neuron-level optimization offers an effective and scalable approach to securing code generation without sacrificing efficiency or generality.", "AI": {"tldr": "GoodVibe\u662f\u4e00\u4e2a\u9488\u5bf9\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u7684\u5143\u7ea7\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u522b\u7684\u4f18\u5316\u63d0\u5347\u4ee3\u7801\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u6574\u4f53\u6548\u7387\u548c\u901a\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4ee3\u7801\u5b89\u5168\u6027\u6539\u8fdb\u65b9\u6cd5\u8981\u4e48\u4ee3\u4ef7\u9ad8\u6602\u4e14\u5bb9\u6613\u9057\u5fd8\uff0c\u8981\u4e48\u64cd\u4f5c\u7c97\u7c92\u5ea6\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e0e\u63a7\u5236\u3002GoodVibe\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u5173\u952e\u795e\u7ecf\u5143\u6765\u589e\u5f3a\u4ee3\u7801\u5b89\u5168\u6027\uff0c\u4ece\u800c\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u9ad8\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002", "method": "GoodVibe\u5229\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u5f52\u56e0\u65b9\u6cd5\u8bc6\u522b\u4e0e\u5b89\u5168\u6027\u76f8\u5173\u7684\u795e\u7ecf\u5143\uff0c\u5e76\u8fdb\u884c\u9009\u62e9\u6027\u7cbe\u7ec6\u8c03\u6574\uff0c\u4ec5\u66f4\u65b0\u8fd9\u4e9b\u5173\u952e\u7684\u5b50\u7a7a\u95f4\u3002\u6b64\u5916\uff0c\u5b83\u5f15\u5165\u4e86\u57fa\u4e8e\u6fc0\u6d3b\u7684\u795e\u7ecf\u5143\u805a\u7c7b\uff0c\u4f7f\u66f4\u65b0\u66f4\u52a0\u7ed3\u6784\u5316\u4e14\u5f00\u9500\u5c0f\u3002", "result": "GoodVibe\u5728\u5305\u62ecC++, Java, Swift\u548cGo\u5728\u5185\u7684\u516d\u79cd\u5173\u952e\u7f16\u7a0b\u8bed\u8a00\u4e0a\u7684LLMs\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5176\u63d0\u9ad8\u4e86\u4ee3\u7801\u5b89\u5168\u6027\uff0c\u76f8\u6bd4\u57fa\u51c6\u6a21\u578b\u63d0\u9ad8\u4e862.5\u500d\uff0c\u8fbe\u5230\u4e86\u4e0e\u5168\u9762\u8c03\u6574\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u53c2\u6570\u91cf\u5c11\u4e8e4700\u500d\uff0c\u8bad\u7ec3\u8ba1\u7b97\u91cf\u51cf\u5c11\u4e86\u8d85\u8fc73.6\u500d\u3002", "conclusion": "GoodVibe\u4e3a\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u5b89\u5168\u4f18\u5316\u65b9\u6cd5\uff0c\u4e0d\u5fc5\u727a\u7272\u6548\u7387\u6216\u901a\u7528\u6027\u3002"}}
{"id": "2602.10877", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.10877", "abs": "https://arxiv.org/abs/2602.10877", "authors": ["Bakheet Aljedaani"], "title": "Beyond Permissions: An Empirical Static Analysis of Privacy and Security Risks in Children-Oriented and General-Audience Mobile Apps for Gaming", "comment": "13 pages, 5 figures, 2 tables", "summary": "Mobile gaming applications (apps) have become increasingly pervasive, including a growing number of games designed for children. Despite their popularity, these apps often integrate complex analytics, advertising, and attribution infrastructures that may introduce privacy and security risks. Existing research has primarily focused on tracking behaviors or monetization models, leaving configuration-level privacy exposure and children-oriented apps underexplored. In this study, we conducted a comparative static analysis of Android mobile games to investigate privacy and security risks beyond permission usage. The analysis follows a three-phase methodology comprising (i) designing study protocol, (ii) Android Package Kit (APK) collection and static inspection, and (iii) data analysis. We examined permissions, manifest-level configuration properties (e.g., backup settings, cleartext network traffic, and exported components), and embedded third-party Software Development Kit (SDK) ecosystems across children-oriented and general-audience mobile games. The extracted indicators are synthesized into qualitative privacy-risk categories to support comparative reporting. The results showed that while children-oriented games often request fewer permissions, they frequently exhibit configuration-level risks and embed third-party tracking SDKs similar to general-audience games. Architectural and configuration decisions play a critical role in shaping privacy risks, particularly for apps targeting children. This study contributes a holistic static assessment of privacy exposure in mobile games and provides actionable insights for developers, platform providers, and researchers seeking to improve privacy-by-design practices in mobile applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u6bd4\u5206\u6790\u4e86\u9762\u5411\u513f\u7ae5\u548c\u4e00\u822c\u53d7\u4f17\u7684\u79fb\u52a8\u6e38\u620f\uff0c\u63a2\u8ba8\u4e86\u8d85\u51fa\u6743\u9650\u4f7f\u7528\u7684\u884c\u4e3a\u548c\u5b89\u5168\u98ce\u9669\u3002\u53d1\u73b0\u9762\u5411\u513f\u7ae5\u7684\u6e38\u620f\u867d\u7136\u8bf7\u6c42\u7684\u6743\u9650\u8f83\u5c11\uff0c\u4f46\u5176\u914d\u7f6e\u7ea7\u522b\u7684\u9690\u79c1\u98ce\u9669\u548c\u5d4c\u5165\u7b2c\u4e09\u65b9\u8ffd\u8e2aSDK\u7684\u98ce\u9669\u4e0e\u4e00\u822c\u53d7\u4f17\u6e38\u620f\u76f8\u4f3c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u805a\u7126\u4e8e\u8ddf\u8e2a\u884c\u4e3a\u6216\u53d8\u73b0\u6a21\u5f0f\uff0c\u5ffd\u7565\u4e86\u914d\u7f6e\u7ea7\u522b\u7684\u9690\u79c1\u66b4\u9732\u548c\u513f\u7ae5\u5411\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u66f4\u5168\u9762\u5730\u5206\u6790\u79fb\u52a8\u6e38\u620f\u4e2d\u7684\u9690\u79c1\u5b89\u5168\u98ce\u9669\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\u8fdb\u884c\u9759\u6001\u5206\u6790\uff1a\u8bbe\u8ba1\u7814\u7a76\u534f\u8bae\uff0c\u6536\u96c6\u5e76\u9759\u6001\u68c0\u67e5Android\u5305\uff08APK\uff09\uff0c\u6570\u636e\u5206\u6790\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9762\u5411\u513f\u7ae5\u7684\u6e38\u620f\u867d\u7136\u8bf7\u6c42\u7684\u6743\u9650\u8f83\u5c11\uff0c\u4f46\u5176\u914d\u7f6e\u7ea7\u522b\u7684\u9690\u79c1\u98ce\u9669\u7c7b\u4f3c\u9762\u5411\u4e00\u822c\u53d7\u4f17\u7684\u6e38\u620f\uff0c\u4e14\u5b58\u5728\u5927\u91cf\u7684\u7b2c\u4e09\u65b9\u8ddf\u8e2aSDK\u3002\u8fd9\u8868\u660e\u67b6\u6784\u548c\u914d\u7f6e\u51b3\u7b56\u5bf9\u4e8e\u9690\u79c1\u98ce\u9669\u7684\u5f71\u54cd\u662f\u663e\u8457\u7684\uff0c\u7279\u522b\u662f\u9762\u5bf9\u513f\u7ae5\u7684\u5e94\u7528\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u79fb\u52a8\u6e38\u620f\u7684\u5168\u9762\u9759\u6001\u9690\u79c1\u8bc4\u4f30\uff0c\u4e3a\u5f00\u53d1\u8005\u3001\u5e73\u53f0\u63d0\u4f9b\u5546\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5b9e\u73b0\u9690\u79c1\u8bbe\u8ba1\u539f\u5219\u7684\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2602.11015", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11015", "abs": "https://arxiv.org/abs/2602.11015", "authors": ["Valery Khvatov", "Alexey Neyman"], "title": "CVPL: A Geometric Framework for Post-Hoc Linkage Risk Assessment in Protected Tabular Data", "comment": "53 pages, 9 figures, 6 appendices. Code: https://github.com/DGT-Network/cvpl", "summary": "Formal privacy metrics provide compliance-oriented guarantees but often fail to quantify actual linkability in released datasets. We introduce CVPL (Cluster-Vector-Projection Linkage), a geometric framework for post-hoc assessment of linkage risk between original and protected tabular data. CVPL represents linkage analysis as an operator pipeline comprising blocking, vectorization, latent projection, and similarity evaluation, yielding continuous, scenario-dependent risk estimates rather than binary compliance verdicts. We formally define CVPL under an explicit threat model and introduce threshold-aware risk surfaces, R(lambda, tau), that capture the joint effects of protection strength and attacker strictness. We establish a progressive blocking strategy with monotonicity guarantees, enabling anytime risk estimation with valid lower bounds. We demonstrate that the classical Fellegi-Sunter linkage emerges as a special case of CVPL under restrictive assumptions, and that violations of these assumptions can lead to systematic over-linking bias. Empirical validation on 10,000 records across 19 protection configurations demonstrates that formal k-anonymity compliance may coexist with substantial empirical linkability, with a significant portion arising from non-quasi-identifier behavioral patterns. CVPL provides interpretable diagnostics identifying which features drive linkage feasibility, supporting privacy impact assessment, protection mechanism comparison, and utility-risk trade-off analysis.", "AI": {"tldr": "CVPL\uff08Cluster-Vector-Projection Linkage\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u51e0\u4f55\u6846\u67b6\uff0c\u7528\u4e8e\u540e\u9a8c\u8bc4\u4f30\u539f\u59cb\u548c\u4fdd\u62a4\u8868\u683c\u6570\u636e\u4e4b\u95f4\u7684\u94fe\u63a5\u98ce\u9669\u3002\u5b83\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u4f30\u8ba1\u94fe\u63a5\u98ce\u9669\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u4e8c\u5143\u5408\u89c4\u6027\u5224\u65ad\u3002", "motivation": "\u73b0\u6709\u6b63\u5f0f\u9690\u79c1\u5ea6\u91cf\u5728\u63d0\u4f9b\u5408\u89c4\u6027\u4fdd\u8bc1\u65b9\u9762\u6709\u6548\uff0c\u4f46\u5728\u91cf\u5316\u5b9e\u9645\u94fe\u63a5\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002CVPL\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\uff0c\u901a\u8fc7\u51e0\u4f55\u6846\u67b6\u548c\u64cd\u4f5c\u7ba1\u9053\uff0c\u63d0\u4f9b\u4e86\u66f4\u52a0\u7ec6\u81f4\u7684\u98ce\u9669\u8bc4\u4f30\u3002", "method": "CVPL\u65b9\u6cd5\u5305\u62ec\u963b\u585e\u3001\u5411\u91cf\u5316\u3001\u6f5c\u5728\u6295\u5f71\u548c\u76f8\u4f3c\u6027\u8bc4\u4f30\u7b49\u6b65\u9aa4\u3002\u901a\u8fc7\u5b9a\u4e49\u660e\u786e\u7684\u5a01\u80c1\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u9608\u503c\u611f\u77e5\u7684\u98ce\u9669\u8868\u9762\uff0cCVPL\u80fd\u591f\u8bc4\u4f30\u4fdd\u62a4\u5f3a\u5ea6\u548c\u653b\u51fb\u8005\u4e25\u683c\u6027\u4e4b\u95f4\u7684\u8054\u5408\u5f71\u54cd\u3002\u6b64\u5916\uff0cCVPL\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u9010\u6b65\u963b\u585e\u7b56\u7565\uff0c\u5e76\u5177\u6709\u5355\u8c03\u6027\u4fdd\u8bc1\uff0c\u4f7f\u5f97\u5728\u4efb\u4f55\u65f6\u95f4\u90fd\u80fd\u8fdb\u884c\u6709\u6548\u7684\u98ce\u9669\u4f30\u8ba1\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u5c3d\u7ba1\u6709\u5f62\u5f0f\u5316\u7684k-\u533f\u540d\u6027\u5408\u89c4\u6027\uff0c\u5b9e\u9645\u7684\u6570\u636e\u4ecd\u7136\u53ef\u80fd\u5b58\u5728\u5927\u91cf\u7684\u94fe\u63a5\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u90a3\u4e9b\u975e\u62df\u7279\u5f81\u6807\u8bc6\u7b26\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "conclusion": "CVPL\u901a\u8fc7\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u652f\u6301\u9690\u79c1\u5f71\u54cd\u8bc4\u4f30\u3001\u4fdd\u62a4\u673a\u5236\u7684\u6bd4\u8f83\u4ee5\u53ca\u6548\u7528\u548c\u98ce\u9669\u4e4b\u95f4\u7684\u6743\u8861\u5206\u6790\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5bf9\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4fdd\u62a4\u72b6\u51b5\u7684\u7406\u89e3\u3002"}}
{"id": "2602.11019", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.11019", "abs": "https://arxiv.org/abs/2602.11019", "authors": ["Jericho Cain", "Hayden Beadles"], "title": "Mask-Based Window-Level Insider Threat Detection for Campaign Discovery", "comment": null, "summary": "User and Entity Behavior Analytics (UEBA) systems commonly detect insider threats by scoring fixed time windows of user activity for anomalous behavior. While this window-level paradigm has proven effective for identifying sharp behavioral deviations, it remains unclear how much information about longer-running attack campaigns is already present within individual windows, and how such information can be leveraged for campaign discovery. In this work, we study unsupervised window-level insider threat detection on the CERT r4.2 dataset and show that explicitly separating activity presence from activity magnitude yields substantial performance gains. We introduce a dual-channel convolutional autoencoder that reconstructs both a binary activity mask and corresponding activity values, allowing the model to focus representational capacity on sparse behavioral structure rather than dense inactive baselines. Across multiday attack campaigns lasting between one and seven days, the proposed approach achieves a window-level precision-recall AUC of 0.71, substantially exceeding standard unsupervised autoencoder baselines and enabling high-precision operating points with zero false alarms.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.11023", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.11023", "abs": "https://arxiv.org/abs/2602.11023", "authors": ["Shaoyu Li", "Hexuan Yu", "Shanghao Shi", "Md Mohaimin Al Barat", "Yang Xiao", "Y. Thomas Hou", "Wenjing Lou"], "title": "IU-GUARD: Privacy-Preserving Spectrum Coordination for Incumbent Users under Dynamic Spectrum Sharing", "comment": null, "summary": "With the growing demand for wireless spectrum, dynamic spectrum sharing (DSS) frameworks such as the Citizens Broadband Radio Service (CBRS) have emerged as practical solutions to improve utilization while protecting incumbent users (IUs) such as military radars. However, current incumbent protection mechanisms face critical limitations. The Environmental Sensing Capability (ESC) requires costly sensor deployments and remains vulnerable to interference and security risks. Alternatively, the Incumbent Informing Capability (IIC) requires IUs to disclose their identities and operational parameters to the Spectrum Coordination System (SCS), creating linkable records that compromise operational privacy and mission secrecy. We propose IU-GUARD, a privacy-preserving spectrum sharing framework that enables IUs to access spectrum without revealing their identities. Leveraging verifiable credentials (VCs) and zero-knowledge proofs (ZKPs), IU-GUARD allows IUs to prove their authorization to the SCS while disclosing only essential operational parameters. This decouples IU identity from spectrum access, prevents cross-request linkage, and mitigates the risk of centralized SCS data leakage. We implement a prototype, and our evaluation shows that IU-GUARD achieves strong privacy guarantees with practical computation and communication overhead, making it suitable for real-time DSS deployment.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.11088", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.11088", "abs": "https://arxiv.org/abs/2602.11088", "authors": ["Abhishek Saini", "Haolin Jiang", "Hang Liu"], "title": "Vulnerabilities in Partial TEE-Shielded LLM Inference with Precomputed Noise", "comment": null, "summary": "The deployment of large language models (LLMs) on third-party devices requires new ways to protect model intellectual property. While Trusted Execution Environments (TEEs) offer a promising solution, their performance limits can lead to a critical compromise: using a precomputed, static secret basis to accelerate cryptographic operations. We demonstrate that this mainstream design pattern introduces a classic cryptographic flaw, the reuse of secret keying material, into the system's protocol. We prove its vulnerability with two distinct attacks: First, our attack on a model confidentiality system achieves a full confidentiality break by recovering its secret permutations and model weights. Second, our integrity attack completely bypasses the integrity checks of systems like Soter and TSQP. We demonstrate the practicality of our attacks against state-of-the-art LLMs, recovering a layer's secrets from a LLaMA-3 8B model in about 6 minutes and showing the attack scales to compromise 405B-parameter LLMs across a variety of configurations.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
