<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 5]
- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Boost+: Equitable, Incentive-Compatible Block Building](https://arxiv.org/abs/2602.04007)
*Mengqian Zhang,Sen Yang,Kartik Nayak,Fan Zhang*

Main category: cs.CR

TL;DR: Boost+ 提出了一种新的机制 $oldsymbol{M}_{oldsymbol{Boost+}}$，旨在通过在区块构建过程中拆分收集与排序交易的过程来提高交易者的平等参与度和区块空间使用效率，确保在区块构建中分配公平且经济高效。


<details>
  <summary>Details</summary>
Motivation: 当前的 MEV-Boost 生态系统由于整合的原因变得高度集中，导致了竞争扭曲、区块空间使用效率低下和 MEV 流透明度不足的问题。为了确保区块构建过程中的公平性和经济效率，提出了解决这些问题的 $oldsymbol{Boost+}$ 系统。

Method: Boost+ 系统的核心是机制 $oldsymbol{M}_{oldsymbol{Boost+}}$，该机制通过一个默认算法来实现激励相容。该机制确保所有区块构建者采取诚实地出价作为最优策略，同时确保搜索者的诚实行事也是最优策略，甚至在存在冲突时也是如此。此外，还使用实际交易数据来评估默认算法的有效性。

Result: 研究展示了在区块构建过程中通过 $oldsymbol{M}_{oldsymbol{Boost+}}$ 实现的公平性和经济效率，同时也通过实验验证了该机制和默认算法的有效性。

Conclusion: Boost+ 通过重新设计区块构建过程中的交易收集与排序方法，有效解决了现有 MEV-Boost 生态系统的一系列问题，提供了区块空间更高效的使用途径。

Abstract: Block space on the blockchain is scarce and must be allocated efficiently through block building. However, Ethereum's current block-building ecosystem, MEV-Boost, has become highly centralized due to integration, which distorts competition, reduces blockspace efficiency, and obscures MEV flow transparency. To guarantee equitability and economic efficiency in block building, we propose $\mathrm{Boost+}$, a system that decouples the process into collecting and ordering transactions, and ensures equal access to all collected transactions.
  The core of $\mathrm{Boost+}$ is the mechanism $\mathit{M}_{\mathrm{Boost+}}$, built around a default algorithm. $\mathit{M}_{\mathrm{Boost+}}$ aligns incentives for both searchers (intermediaries that generate or route transactions) and builders: Truthful bidding is a dominant strategy for all builders. For searchers, truthful reporting is dominant whenever the default algorithm dominates competing builders, and it remains dominant for all conflict-free transactions, even when builders may win. We further show that even if a searcher can technically integrate with a builder, non-integration combined with truthful bidding still dominates any deviation for conflict-free transactions. We also implement a concrete default algorithm informed by empirical analysis of real-world transactions and evaluate its efficacy using historical transaction data.

</details>


### [2] [Evaluating the Vulnerability Landscape of LLM-Generated Smart Contracts](https://arxiv.org/abs/2602.04039)
*Hoang Long Do,Nasrin Sohrabi,Muneeb Ul Hassan*

Main category: cs.CR

TL;DR: 该研究系统地分析了最先进大语言模型生成的Solidity智能合约的安全性，发现尽管在语法正确性和功能完整性方面表现良好，但这些合约中普遍存在严重的安全漏洞，提出了一些实用的防范措施和开发准则。


<details>
  <summary>Details</summary>
Motivation: 为了探索大语言模型（LLMs）生成的智能合约在实际应用中的安全性，研究发现了严重的安全问题，旨在促进安全地将LLMs集成到智能合约开发流程中，增强区块链生态系统的安全性。

Method: 研究选择了ChatGPT、Gemini和Sonnet等先进模型，系统地评估了它们生成的Solidity智能合约的安全性，对比了大量的已知漏洞。

Result: 研究表明，尽管LLMs生成的智能合约在语法和功能上表现良好，但在安全性方面存在严重问题，面临着被利用的风险。

Conclusion: 研究揭示了关键的安全弱点模式，并提供了实用的补救措施和开发准则，旨在为开发者和研究人员提供实用建议，以加强智能合约的安全性。

Abstract: Large language models (LLMs) have been widely adopted in modern software development lifecycles, where they are increasingly used to automate and assist code generation, significantly improving developer productivity and reducing development time. In the blockchain domain, developers increasingly rely on LLMs to generate and maintain smart contracts, the immutable, self-executing components of decentralized applications. Because deployed smart contracts cannot be modified, correctness and security are paramount, particularly in high-stakes domains such as finance and governance. Despite this growing reliance, the security implications of LLM-generated smart contracts remain insufficiently understood.
  In this work, we conduct a systematic security analysis of Solidity smart contracts generated by state-of-the-art LLMs, including ChatGPT, Gemini, and Sonnet. We evaluate these contracts against a broad set of known smart contract vulnerabilities to assess their suitability for direct deployment in production environments. Our extensive experimental study shows that, despite their syntactic correctness and functional completeness, LLM-generated smart contracts frequently exhibit severe security flaws that could be exploited in real-world settings. We further analyze and categorize these vulnerabilities, identifying recurring weakness patterns across different models. Finally, we discuss practical countermeasures and development guidelines to help mitigate these risks, offering actionable insights for both developers and researchers. Our findings aim to support safe integration of LLMs into smart contract development workflows and to strengthen the overall security of the blockchain ecosystem against future security failures.

</details>


### [3] [ZKBoost: Zero-Knowledge Verifiable Training for XGBoost](https://arxiv.org/abs/2602.04113)
*Nikolas Melissaris,Jiayi Xu,Antigoni Polychroniadou,Akira Takahashi,Chenkai Weng*

Main category: cs.CR

TL;DR: 本文介绍了ZKBoost，这是一种基于零知识证明的XGBoost训练协议，允许模型拥有者在不泄露数据或参数的情况下证明训练的正确性。


<details>
  <summary>Details</summary>
Motivation: 随着XGBoost在敏感环境下的应用增加，对其训练过程的加密完整性保证变得越来越重要。

Method: 作者提出了一种基于固定点实施的XGBoost，兼容于算术电路，并结合了矢量化盲线性评估（VOLE）技术来证明非线性固定点操作。

Result: 作者的实现方式使得在标准XGBoost精度误差在1%以内的情况下，也能实现现实数据集上的实用零知识证明。

Conclusion: ZKBoost为XGBoost在确保模型训练过程的隐私和完整性的场景下提供了一个有效的解决方案。

Abstract: Gradient boosted decision trees, particularly XGBoost, are among the most effective methods for tabular data. As deployment in sensitive settings increases, cryptographic guarantees of model integrity become essential. We present ZKBoost, the first zero-knowledge proof of training (zkPoT) protocol for XGBoost, enabling model owners to prove correct training on a committed dataset without revealing data or parameters. We make three key contributions: (1) a fixed-point XGBoost implementation compatible with arithmetic circuits, enabling instantiation of efficient zkPoT, (2) a generic template of zkPoT for XGBoost, which can be instantiated with any general-purpose ZKP backend, and (3) vector oblivious linear evaluation (VOLE)-based instantiation resolving challenges in proving nonlinear fixed-point operations. Our fixed-point implementation matches standard XGBoost accuracy within 1\% while enabling practical zkPoT on real-world datasets.

</details>


### [4] [Optimal conversion from Rényi Differential Privacy to $f$-Differential Privacy](https://arxiv.org/abs/2602.04562)
*Anneliese Riess,Juan Felipe Gomez,Flavio du Pin Calmon,Julia Anne Schnabel,Georgios Kaissis*

Main category: cs.CR

TL;DR: 本文证明了Zhu等人(2022)中附录F.3中提出的猜想：所有将Rényi差异隐私(RDP)配置映射到有效假设检验权衡的转换规则中,基于单一阶RDP隐私区域交集的规则是最优的。这种最优性同时适用于所有有效的RDP配置和所有类型的I类错误水平α。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是证明一个关于最优转换规则的猜想，该规则能够最好地将RDP配置映射到有效的假设检验权衡中。

Method: 通过精确的几何特征化RDP隐私区域，利用其凸性和边界仅由伯努利机制确定的特性进行分析。证明了‘交集的RDP隐私区域’规则是最佳的，没有任何其他黑盒转换规则可以均匀占据它。

Result: 研究结果表明，基于单一阶RDP隐私区域交集的转换规则在所有有效的RDP配置和所有类型I类错误水平α的情况下均是最优的，这是单一阶RDP隐私区域的点wise最大值。

Conclusion: 本文的结论是证明了基于单一阶RDP隐私区域交集的转换规则在所有情况下是最佳的，这是从机制的RDP保证中唯一能够推断隐私的极限。

Abstract: We prove the conjecture stated in Appendix F.3 of [Zhu et al. (2022)]: among all conversion rules that map a Rényi Differential Privacy (RDP) profile $τ\mapsto ρ(τ)$ to a valid hypothesis-testing trade-off $f$, the rule based on the intersection of single-order RDP privacy regions is optimal. This optimality holds simultaneously for all valid RDP profiles and for all Type I error levels $α$. Concretely, we show that in the space of trade-off functions, the tightest possible bound is $f_{ρ(\cdot)}(α) = \sup_{τ\geq 0.5} f_{τ,ρ(τ)}(α)$: the pointwise maximum of the single-order bounds for each RDP privacy region. Our proof unifies and sharpens the insights of [Balle et al. (2019)], [Asoodeh et al. (2021)], and [Zhu et al. (2022)]. Our analysis relies on a precise geometric characterization of the RDP privacy region, leveraging its convexity and the fact that its boundary is determined exclusively by Bernoulli mechanisms. Our results establish that the "intersection-of-RDP-privacy-regions" rule is not only valid, but optimal: no other black-box conversion can uniformly dominate it in the Blackwell sense, marking the fundamental limit of what can be inferred about a mechanism's privacy solely from its RDP guarantees.

</details>


### [5] [Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates](https://arxiv.org/abs/2602.04653)
*Ariel Fogel,Omer Hofman,Eilon Cohen,Roman Vainshtein*

Main category: cs.CR

TL;DR: 该研究提出一种新型攻击面，利用聊天模板作为在不改动模型权重、修改训练数据或控制运行时基础设施的情况下实现模型推理时后门攻击的方法。实验表明，这种攻击在多种模型中有效，并能显著降低事实准确性并产生受控的攻击者URL。


<details>
  <summary>Details</summary>
Motivation: 随着开放权重语言模型在生产环境中的广泛应用，安全挑战日益突出，尤其是后门攻击这类威胁。传统的攻击假设攻击者能访问训练管道或部署基础设施，而此研究探索了一种新的攻击面，即利用聊天模板，无需上述条件即可实施后门攻击。

Method: 该研究通过构建聊天模板后门，旨在针对两个目标：降低事实准确性以及诱导产生攻击者控制的URL。在评估过程中，研究人员应用了这种方法到18个不同模型，跨越了7个模型家族和4种不同的推理引擎。

Result: 实验结果表明，在触发条件下，事实准确性下降超过80%，而攻击者控制的URL被成功发出了超过80%的时间。另外，这种后门具有跨推理运行时的普适性，并且能够成功绕过主流的安全检查工具。

Conclusion: 研究结论指出，聊天模板已成为开放权重语言模型供应链中一个可靠且未被防御的新攻击面。

Abstract: Open-weight language models are increasingly used in production settings, raising new security challenges. One prominent threat in this context is backdoor attacks, in which adversaries embed hidden behaviors in language models that activate under specific conditions. Previous work has assumed that adversaries have access to training pipelines or deployment infrastructure. We propose a novel attack surface requiring neither, which utilizes the chat template. Chat templates are executable Jinja2 programs invoked at every inference call, occupying a privileged position between user input and model processing. We show that an adversary who distributes a model with a maliciously modified template can implant an inference-time backdoor without modifying model weights, poisoning training data, or controlling runtime infrastructure. We evaluated this attack vector by constructing template backdoors targeting two objectives: degrading factual accuracy and inducing emission of attacker-controlled URLs, and applied them across eighteen models spanning seven families and four inference engines. Under triggered conditions, factual accuracy drops from 90% to 15% on average while attacker-controlled URLs are emitted with success rates exceeding 80%; benign inputs show no measurable degradation. Backdoors generalize across inference runtimes and evade all automated security scans applied by the largest open-weight distribution platform. These results establish chat templates as a reliable and currently undefended attack surface in the LLM supply chain.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Knowledge Model Prompting Increases LLM Performance on Planning Tasks](https://arxiv.org/abs/2602.03900)
*Erik Goh,John Kos,Ashok Goel*

Main category: cs.AI

TL;DR: 该研究通过TMK框架提升了大语言模型的推理能力，特别是在Blocksworld领域内的复杂规划任务中，使其表现出色。


<details>
  <summary>Details</summary>
Motivation: TMK框架是通过借鉴认知和教育科学领域的知识开发出来的，希望能够解决当前大型语言模型在推理方面的问题，并在PlanBench基准上的Blocksworld领域内进行验证。

Method: 研究采用了TMK框架进行结构化提示，以测试大语言模型在复杂规划问题中的推理能力和问题分解能力。特别是在PlanBench的Blocksworld基准上进行评估。

Result: 实验结果表明，TMK结构化提示使得语言模型在这些任务上的准确率大大提高，甚至在某些之前表现不佳的任务上也达到了97.3%的准确率。

Conclusion: 研究认为TMK不但提供额外的上下文背景，还指导模型从其默认的语言模式转变到正式的代码执行路径进行处理。这表明TMK有可能填补语义近似与符号操作之间的差距。

Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments.

</details>


### [7] [Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation](https://arxiv.org/abs/2602.03950)
*Aditya Basarkar,Benyamin Tabarsi,Tiffany Barnes,Dongkuan,Xu*

Main category: cs.AI

TL;DR: IIPC是一种迭代改进的程序构建方法，能够迭代细化程序推理链，并结合执行反馈与基础LLM的链式思考能力，保持高层次的上下文聚焦，从而超越了大多数推理基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有的多Agent大语言模型系统虽然在数学推理能力上有了提升，但仍缺乏可靠可修订的推理过程表示，IIPC旨在解决这些问题。

Method: IIPC通过迭代优化程序推理链，并利用基础LLM的链式思考能力与执行反馈相结合的方法，使得推理过程更具灵活性和准确性。

Result: IIPC在多种基准测试中超越了竞争方法，并且所有代码和实现已被开源。

Conclusion: IIPC提供了一种新的推理方法，使得AI能够在教育、科学和工程领域提供更可靠和准确的推理能力。

Abstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.

</details>


### [8] [Active Epistemic Control for Query-Efficient Verified Planning](https://arxiv.org/abs/2602.03974)
*Shuhui Qu*

Main category: cs.AI

TL;DR: AEC 提出了一种在部分可观测环境下处理计划的方法，通过集成基于模型的信心管理和分类可行性检查，以减少不可行承诺的风险，从而提高效率。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测的交互环境中进行计划时，预测错误可能会导致不可行的承诺。现有方法如利用学习到的世界模型预测缺失信息，但这种预测可能引入错误，影响计划的可行性。

Method: AEC 通过维护两个分离的存储：用于承诺的已确认事实存储和仅用于剪枝计划候选的信念存储。在每个步骤中，当不确定性高或预测不明确时，AEC 查询环境来确认未解决的命题；当信心充足时，则模拟命题以过滤假设。最终承诺需满足已确认预设覆盖度和兼容性检查。

Result: 实验中的 AEC 在 ALFWorld 和 ScienceWorld 中实现了与强大的 LLM 剂基线相当的成功率，但需要较少的重新规划轮次。

Conclusion: AEC 为在部分可观测环境下的高效和可靠的规划提供了新的解决方案。

Abstract: Planning in interactive environments is challenging under partial observability: task-critical preconditions (e.g., object locations or container states) may be unknown at decision time, yet grounding them through interaction is costly. Learned world models can cheaply predict missing facts, but prediction errors can silently induce infeasible commitments. We present \textbf{Active Epistemic Control (AEC)}, an epistemic-categorical planning layer that integrates model-based belief management with categorical feasibility checks. AEC maintains a strict separation between a \emph{grounded fact store} used for commitment and a \emph{belief store} used only for pruning candidate plans. At each step, it either queries the environment to ground an unresolved predicate when uncertainty is high or predictions are ambiguous, or simulates the predicate to filter hypotheses when confidence is sufficient. Final commitment is gated by grounded precondition coverage and an SQ-BCP pullback-style compatibility check, so simulated beliefs affect efficiency but cannot directly certify feasibility. Experiments on ALFWorld and ScienceWorld show that AEC achieves competitive success with fewer replanning rounds than strong LLM-agent baselines.

</details>


### [9] [Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure](https://arxiv.org/abs/2602.03975)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 本文提出了一种基于状态的选择性验证框架，以减少推理过程中的验证成本。该框架结合了确定性的可行性筛选、预验证排名和基于本地不确定性的验证资源分配。


<details>
  <summary>Details</summary>
Motivation: 验证已成为大型语言模型（LLM）推理的主要瓶颈，尤其是在许多推理系统中，大量的验证调用被浪费在冗余或不具前景的中间假设上。为了提高验证效率，作者研究了在验证成本受限环境下的推理策略。

Method: 该研究提出了一个基于状态的选择性验证框架，该框架包括确定性的可行性筛选，使用混合学习状态距离和残差得分进行预验证排名，以及基于局部不确定性的验证资源动态分配。

Result: 与最佳的N个解、多数投票和束搜索方法相比，该方法在MATH基准测试中的准确率更高，同时验证调用次数减少了44%。

Conclusion: 该研究提供了一种有效的验证成本分配策略，提高了大型语言模型推理任务的效率和效果。

Abstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study reasoning under a \emph{verification-cost-limited} setting and ask how verification effort should be allocated across intermediate states. We propose a state-level selective verification framework that combines (i) deterministic feasibility gating over a structured move interface, (ii) pre-verification ranking using a hybrid of learned state-distance and residual scoring, and (iii) adaptive allocation of verifier calls based on local uncertainty. Unlike solution-level best-of-$N$ or uniform intermediate verification, our method distributes verification where it is most informative. On the \textsc{MATH} benchmark, our approach achieves higher accuracy than best-of-$N$, majority voting, and beam search while using 44\% fewer verifier calls.

</details>


### [10] [Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning](https://arxiv.org/abs/2602.03978)
*Zidi Xiong,Shan Chen,Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: 该研究通过系统评估证明了监测能力（chain-of-thought的忠实性和信息性反映）在Reinforcement Learning with Verifiable Rewards (RLVR) 训练过程中并非普遍增强，而是与数据多样性及指令遵循性密切相关。研究还表明，监测能力与推理性能无关，两者 improvement 不能互相推导，提出了监测能力变化受控于训练和评估难度。


<details>
  <summary>Details</summary>
Motivation: 随着大型推理模型（LRMs）的部署，审计其推理链（CoT）的可靠性对于确保其安全性变得至关重要。特别是在Reinforcement Learning with Verifiable Rewards (RLVR) 中，监测能力可以作为早期的“免费礼物”，但研究发现这种现象并不普遍。

Method: 研究通过系统评估，跨模型家族和训练领域进行了全面的测试。使用了机械分析方法来归因监测能力的提升，特别关注响应分布（熵减少）和对提示的注意程度。

Result: 研究结果表明监测能力的提升并非普遍现象，而是高度依赖于数据多样性及指令遵循性数据。此外，监测能力与模型的推理性能无关，两者提升不能互相推导。提出了监测能力的变化受控于训练和评估难度。

Conclusion: 综合这些发现，可以提供一个全面的观点，说明在RLVR中的监测能力如何出现，以及哪些情况下可能获得改善，哪些情况下则不会。

Abstract: As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a "free gift" during the early stages of Reinforcement Learning with Verifiable Rewards (RLVR). We make this observation concrete through a systematic evaluation across model families and training domains. Our results show that this effect is not universal: monitorability improvements are strongly data-dependent. In particular, we demonstrate the critical role of data diversity and instruction-following data during RLVR training. We further show that monitorability is orthogonal to capability--improvements in reasoning performance do not imply increased transparency. Through mechanistic analysis, we attribute monitorability gains primarily to response distribution sharpening (entropy reduction) and increased attention to the prompt, rather than stronger causal reliance on reasoning traces. We also reveal how monitorability dynamics vary with controlled training and evaluation difficulty. Together, these findings provide a holistic view of how monitorability emerges under RLVR, clarifying when gains are likely to occur and when they are not.

</details>


### [11] [When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making](https://arxiv.org/abs/2602.04003)
*Shutong Fan,Lan Zhang,Xiaoyong Yuan*

Main category: cs.AI

TL;DR: 该研究通过引入对抗性解释攻击（AEAs），展示了语言模型生成的解释如何在认知层面上影响人类对AI系统的信任，特别是错误输出的接受度。研究发现，即使解释是错误的，用户也倾向于对它们持有与正确解释几乎相同的信任。


<details>
  <summary>Details</summary>
Motivation: 随着现代AI系统越来越多地嵌入到人类决策过程中，其生成的自然语言解释对用户理解和信任AI输出的影响变得越来越重要。然而，这些解释也可能被恶意使用，以模塑用户的信任，从而导致错误的决策。

Method: 研究通过引入对抗性解释攻击（AEAs），并采用控制实验（n = 205），系统地变化解释框架的四个维度：推理模式、证据类型、沟通风格和呈现形式，来定量分析和衡量这些解释对人类信任的影响。

Result: 实验结果显示，用户对对抗性解释和无害解释的信任度几乎相同，即使解释是错误的。最脆弱的情况发生在AEAs接近专家沟通方式时，即权威证据、中性语气和主题相关推理的结合。

Conclusion: 研究发现，解释的质量对用户信任有巨大影响，而这种影响可以被恶意利用来增强对错误预测的信任。研究强调了对这种认知层面的威胁进行系统性研究的重要性，以便更好地理解和防范这种新型攻击。

Abstract: Most adversarial threats in artificial intelligence target the computational behavior of models rather than the humans who rely on them. Yet modern AI systems increasingly operate within human decision loops, where users interpret and act on model recommendations. Large Language Models generate fluent natural-language explanations that shape how users perceive and trust AI outputs, revealing a new attack surface at the cognitive layer: the communication channel between AI and its users. We introduce adversarial explanation attacks (AEAs), where an attacker manipulates the framing of LLM-generated explanations to modulate human trust in incorrect outputs. We formalize this behavioral threat through the trust miscalibration gap, a metric that captures the difference in human trust between correct and incorrect outputs under adversarial explanations. By incorporating this gap, AEAs explore the daunting threats in which persuasive explanations reinforce users' trust in incorrect predictions. To characterize this threat, we conducted a controlled experiment (n = 205), systematically varying four dimensions of explanation framing: reasoning mode, evidence type, communication style, and presentation format. Our findings show that users report nearly identical trust for adversarial and benign explanations, with adversarial explanations preserving the vast majority of benign trust despite being incorrect. The most vulnerable cases arise when AEAs closely resemble expert communication, combining authoritative evidence, neutral tone, and domain-appropriate reasoning. Vulnerability is highest on hard tasks, in fact-driven domains, and among participants who are less formally educated, younger, or highly trusting of AI. This is the first systematic security study that treats explanations as an adversarial cognitive channel and quantifies their impact on human trust in AI-assisted decision making.

</details>


### [12] [Axiomatic Foundations of Counterfactual Explanations](https://arxiv.org/abs/2602.04028)
*Leila Amgoud,Martin Cooper*

Main category: cs.AI

TL;DR: 该论文通过构建一套公理框架来填补现有解释器在支持不同类型解释方面的不足，证明了某些公理组合不可能同时满足，并建立了五个一对一对应关系，最后一部分将现有解释器置于该分类系统中，分析生成这些解释的计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 解释自主和智能系统的决策是提高信任的关键，现有的大多数解释器只能提供局部解释，并且未系统地研究不同类型的反事实和全局解释。

Method: 论文提出了一个基于决策解释器的公理性框架，通过证明不可能定理和对应定理来进行分类。

Result: 论文揭示了五种基本不同的反事实类型，不同类型的解释器可以在不同层次上提供局部或全局解释，同时对现有解释器进行分类并分析其复杂性。

Conclusion: 本文的工作为评估和开发新的域专家解释器提供了理论基础，限于公理框架的应用，需要进一步探讨不同公理集对解释器性能的影响。

Abstract: Explaining autonomous and intelligent systems is critical in order to improve trust in their decisions. Counterfactuals have emerged as one of the most compelling forms of explanation. They address ``why not'' questions by revealing how decisions could be altered. Despite the growing literature, most existing explainers focus on a single type of counterfactual and are restricted to local explanations, focusing on individual instances. There has been no systematic study of alternative counterfactual types, nor of global counterfactuals that shed light on a system's overall reasoning process.
  This paper addresses the two gaps by introducing an axiomatic framework built on a set of desirable properties for counterfactual explainers. It proves impossibility theorems showing that no single explainer can satisfy certain axiom combinations simultaneously, and fully characterizes all compatible sets. Representation theorems then establish five one-to-one correspondences between specific subsets of axioms and the families of explainers that satisfy them. Each family gives rise to a distinct type of counterfactual explanation, uncovering five fundamentally different types of counterfactuals. Some of these correspond to local explanations, while others capture global explanations. Finally, the framework situates existing explainers within this taxonomy, formally characterizes their behavior, and analyzes the computational complexity of generating such explanations.

</details>


### [13] [Interfaze: The Future of AI is built on Task-Specific Small Models](https://arxiv.org/abs/2602.04101)
*Harsha Vardhan Khurdula,Vineet Agarwal,Yoeven D Khemlani*

Main category: cs.AI

TL;DR: Interfaze 系统通过使用异构 DNN 和小语言模型构建上下文，并将其传递给预选的大模型生成最终响应，从而提高了多项任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型存在复杂性和计算成本高的问题，因此需要一种解决方案能更好地处理多模态任务，提高任务准确性同时降低计算量。

Method: 该系统由感知层、上下文构建层和行动层组成。感知层包含不同类型的 DNN 和小型语言模型，用于处理 OCR、语言识别和其他感知任务。上下文构建层负责从网页、代码、PDF 等外部来源构建结构化状态。行动层能够浏览、执行代码和操作浏览器。整个系统顶层的控制器将这些模块的输出传递给用户选择的大模型。

Result: Interfaze 系统在多个任务上取得卓越成绩，例如 MMLU-Pro 得分 83.6%，MMLU 得分 91.4%，GPQA-Diamond 81.3%，LiveCodeBench v5 57.8%，AIME-2025 90.0%，同时在 MMMU、AI2D、ChartQA 和 Common Voice v16 上也取得了不错的分数。系统能够将大部分处理集中在较小模型和工具上，使大模型仅处理高度提炼过的上下文，从而降低成本并保持高准确性。

Conclusion: Interfaze 提出了一种创新方法来构造和执行上下文，它显著提高了处理诸如多模态理解等任务的准确性和效率，展示了相对于直接使用大型模型的优势。

Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OCR involving complex PDFs, charts and diagrams, and multilingual ASR with (ii) a context-construction layer that crawls, indexes, and parses external sources (web pages, code, PDFs) into compact structured state, and (iii) an action layer that can browse, retrieve, execute code in a sandbox, and drive a headless browser for dynamic web pages. A thin controller sits on top of this stack and exposes a single, OpenAI-style endpoint: it decides which small models and actions to run and always forwards the distilled context to a user-selected LLM that produces the final response.
  On this architecture, Interfaze-Beta achieves 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, and 90.0% on AIME-2025, along with strong multimodal scores on MMMU (val) (77.3%), AI2D (91.5%), ChartQA (90.9%), and Common Voice v16 (90.8%). We show that most queries are handled primarily by the small-model and tool stack, with the large LLM operating only on distilled context, yielding competitive accuracy while shifting the bulk of computation away from the most expensive and monolithic models.

</details>


### [14] [Steering LLMs via Scalable Interactive Oversight](https://arxiv.org/abs/2602.04210)
*Enyu Zhou,Zhiheng Xi,Long Ma,Zhihao Zhang,Shihan Dou,Zhikai Lei,Guoteng Wang,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.AI

TL;DR: 该研究提出了一种名为Scalable Interactive Oversight的框架，旨在通过将复杂意图分解成一个递归树的形式，增强人类监督的效率和精确度。这种方法通过递归收集节点级的低负担反馈，实现精准的全球指导，显著提高了非专家生成高质量产品需求文档的能力。并且通过强化学习实现了在线用户反馈的优化。


<details>
  <summary>Details</summary>
Motivation: 研究指出，在自动化复杂长期任务时，大型语言模型与用户的监督之间存在差距。用户难以有效地引导模型，尤其是在任务复杂度和验证难度较高的情况下。因此，需要开发一种可扩展的人机交互监督框架，以从非专家到专家级的任务结果。

Method: 研究设计了一种递归树分解方法，将复杂的意图逐步分解成多个可管理的决策点。每个决策点都可以接收用户的低负担反馈，这些反馈信号被递归地聚合起来，形成精确的全局指导。此外，该框架还采用强化学习技术，通过在线用户反馈不断优化。

Result: 该框架在网页开发任务中得到了验证，非专家能够生成专家级的产品需求文档，相较于传统方法，生成的质量提高了54%。

Conclusion: 该研究提出的方法提供了一种有效的解决人类监督问题的策略，为维持人机交互控制提供了实用的路径，特别是在AI自动化任务中。

Abstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.

</details>


### [15] [From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents](https://arxiv.org/abs/2602.04326)
*SeungWon Seo,SooBin Lim,SeongRae Noh,Haneul Kim,HyeongYeop Kang*

Main category: cs.AI

TL;DR: PCE框架通过将LLM推理痕迹中的隐含假设转化为结构化的决策树，从而消除在多智能体、部分可观测且分散的环境中进行规划和行动的沟通需求，展示了较低的沟通开销并提高了任务成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 在多智能体、部分可观测且分散的环境中，任务执行者需要规划和行动，但现有的方法依赖频繁的智能体间通信来解决不确定性问题，这种方式导致了高昂的沟通成本，并可能打乱工作流程。

Method: PCE框架采用规划者-编曲者-评估者（Planner-Composer-Evaluator）结构，将大语言模型（LLM）推理中的隐含假设转化为决策树，通过场景可能性、目标导向收益和执行成本对路径进行评分，引导合理行动，而无需大量的智能体间通信。

Result: 在两个具有挑战性的多智能体基准测试（C-WAH和TDW-MAT）以及三个不同的LLM模型上，PCE在成功率和任务效率方面超过了基于沟通的基线，同时保持了相似的令牌使用量。消融研究显示，通过增加模型容量或推理深度，PCE仍能呈现性能提升，且在所有容量和推理深度规模上均提高了基线水平。

Conclusion: PCE框架确立了一条实用路径，将大语言模型中隐含的假设转化为策略，使智能体能够在不确定性环境中执行更为可靠的规划，展示了在无需大量通信的情况下提高智能体工作效率的潜力。

Abstract: Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators' intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standing challenges, such as high-level goal decomposition and online adaptation. Yet, uncertainty is still primarily mitigated through frequent inter-agent communication. This incurs substantial token and time costs, and can disrupt established workflows, when human partners are involved. We introduce PCE, a Planner-Composer-Evaluator framework that converts the fragmented assumptions latent in LLM reasoning traces into a structured decision tree. Internal nodes encode environment assumptions and leaves map to actions; each path is then scored by scenario likelihood, goal-directed gain, and execution cost to guide rational action selection without heavy communication. Across two challenging multi-agent benchmarks (C-WAH and TDW-MAT) and three diverse LLM backbones, PCE consistently outperforms communication-centric baselines in success rate and task efficiency while showing comparable token usage. Ablation results indicate that the performance gains obtained by scaling model capacity or reasoning depth persist even when PCE is applied, while PCE consistently raises the baseline across both capacity and reasoning-depth scales, confirming that structured uncertainty handling complements both forms of scaling. A user study further demonstrates that PCE produces communication patterns that human partners perceive as more efficient and trustworthy. Together, these results establish a principled route for turning latent LLM assumptions into reliable strategies for uncertainty-aware planning.

</details>


### [16] [Digital Twins & ZeroConf AI: Structuring Automated Intelligent Pipelines for Industrial Applications](https://arxiv.org/abs/2602.04385)
*Marco Picone,Fabio Turazza,Matteo Martinelli,Marco Mamei*

Main category: cs.AI

TL;DR: 本文提出了一种面向工业领域的微型工厂场景的零配置（ZeroConf）AI管道，通过数字孪生（DT）协调数据管理和智能增强，使AI管道可以无缝集成到复杂的物理信息系统中，支持多种机器学习模型和动态数据处理。


<details>
  <summary>Details</summary>
Motivation: 随着工业领域复杂性增加，工业互联网和物联网技术之间存在差距，数字孪生技术为解决这一问题提供了可能，但当前方法在可扩展性和复用性方面存在局限性。

Method: 提出了零配置AI管道的概念，并在数字孪生框架下设计了一种模块化、互操作性强的解决方案，通过数字孪生协调数据管理和智能增强，实现AI管道无缝集成。

Result: 在微型工厂场景中展示了该方法的有效性，支持多个机器学习模型和动态数据处理，加速了智能服务的部署。

Conclusion: 这是一种有助于提升工业系统智能解决方案可扩展性和复用性的创新方法。

Abstract: The increasing complexity of Cyber-Physical Systems (CPS), particularly in the industrial domain, has amplified the challenges associated with the effective integration of Artificial Intelligence (AI) and Machine Learning (ML) techniques. Fragmentation across IoT and IIoT technologies, manifested through diverse communication protocols, data formats and device capabilities, creates a substantial gap between low-level physical layers and high-level intelligent functionalities. Recently, Digital Twin (DT) technology has emerged as a promising solution, offering structured, interoperable and semantically rich digital representations of physical assets. Current approaches are often siloed and tightly coupled, limiting scalability and reuse of AI functionalities. This work proposes a modular and interoperable solution that enables seamless AI pipeline integration into CPS by minimizing configuration and decoupling the roles of DTs and AI components. We introduce the concept of Zero Configuration (ZeroConf) AI pipelines, where DTs orchestrate data management and intelligent augmentation. The approach is demonstrated in a MicroFactory scenario, showing support for concurrent ML models and dynamic data processing, effectively accelerating the deployment of intelligent services in complex industrial settings.

</details>


### [17] [From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums](https://arxiv.org/abs/2602.04572)
*Niv Fono,Yftah Ziser,Omer Ben-Porat*

Main category: cs.AI

TL;DR: 本文提出了一种框架，通过生成AI系统向讨论论坛提出问题，探索生成式AI与论坛之间的合作模式，尽管生成式AI依赖于论坛数据进行改进，但此框架展示了互利的可能性。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI与Q&A论坛之间的合作模式，解决生成式AI依赖论坛数据进行改进而用户又被拉走的悖论。

Method: 提出了一个基于顺序互动的框架，通过模拟和数据驱动的实验（使用真实Stack Exchange数据和常用的LLM进行仿真），分析生成式AI与Q&A论坛之间的激励错配问题。

Result: 模拟结果显示，尽管存在激励错配，但是玩家可以在理想的信息透明情境下实现约一半的潜在收益。

Conclusion: 此研究强调了AI系统与人类知识平台之间可持续合作的潜力，可以在保持有效知识共享的同时利用生成式AI的优势。

Abstract: While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.

</details>


### [18] [Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration](https://arxiv.org/abs/2602.04575)
*Jiaheng Liu,Yuanxing Zhang,Shihao Li,Xinping Lei*

Main category: cs.AI

TL;DR: 本文提出了一种名为Vibe AIGC的新范式，通过自主合成层次化的多Agent工作流，解决了当前单次生成模型中存在的意图-执行差距问题。用户不再只是提供提示，而是提供一种称为'氛围'的信息，系统会据此生成执行管道。


<details>
  <summary>Details</summary>
Motivation: 鉴于当前生成人工智能的模型中心范式遇到的易用性瓶颈，本文提出了Vibe AIGC范式，旨在通过逻辑编排提高生成过程的可控性，弥合人类构想与机器执行之间的差距。

Method: 该方法涉及到用户提供的一种宏观的、多层次的指导信息（Vibe），然后通过一个中心化的元规划师将这种指导信息解析为可执行的、可验证的、自适应的多Agent工作流。

Result: 该范式有望改变人类与AI的协作经济，使AI从脆弱的推断引擎转变为更具系统性的工程伙伴，从而促进复杂、长久数字资产的民主化创作。

Conclusion: 本文通过引入Vibe AIGC范式，证明了通过宏观指导信息和逻辑编排可以有效地提升生成过程的灵活性和可控性，对未来的人机协作具有重要意义。

Abstract: For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.
  Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.

</details>


### [19] [WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.04634)
*Zelai Xu,Zhexuan Xu,Ruize Zhang,Chunyang Zhu,Shi Yu,Weilin Liu,Quanlu Zhang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 本文提出了一种名为WideSeek-R1的多智能体系统框架，通过多智能体强化学习（MARL）来实现可扩展的协调与并行执行。该系统在广域信息检索任务上表现出色，随着并行子智能体数量的增加，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体系统的瓶颈在于个人能力有限，而面对更广泛的任务需求，组织能力变得更为关键。因此，本文探索了使用多智能体系统进行宽度扩展，以更好地应对这类任务。

Method: WideSeek-R1框架采用了领导智能体-子智能体结构，并利用共享的大规模语言模型（LLM）并结合独立上下文和专门工具。通过多智能体强化学习进行训练。

Result: 实验表明，WideSeek-R1-4B在WideSearch基准测试上的项目F1得分为40.0%，与单智能体的DeepSeek-R1-671B的性能相当。此外，随着并行子智能体数量的增加，WideSeek-R1-4B还表现出一致的性能提升。

Conclusion: 宽度扩展可以有效地提高多智能体系统在处理广域信息检索任务上的性能。WideSeek-R1展示了该方法的有效性，对于未来的多智能体系统设计具有重要意义。

Abstract: Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.

</details>


### [20] [Are AI Capabilities Increasing Exponentially? A Competing Hypothesis](https://arxiv.org/abs/2602.04836)
*Haosen Ge,Hamsa Bastani,Osbert Bastani*

Main category: cs.AI

TL;DR: 该论文反驳了METR报告中关于AI能力呈现指数增长的观点，通过拟合Sigmoid曲线证明AI能力的拐点已过，并提出了一个更复杂的模型来解释AI基本能力和推理能力的不同增长速度。


<details>
  <summary>Details</summary>
Motivation: 作者旨在挑战关于AI能力将持续指数增长的预测，强调这类预测的不确定性。

Method: 作者使用了Sigmoid曲线拟合当前数据，并提出了一种更复杂的模型来分析AI基本能力和推理能力的独立改进速度。

Result: 根据Sigmoid拟合结果，作者得出AI能力的拐点已经过去；提出的复杂模型进一步支持了这一论点。

Conclusion: 作者认为，现有的关于AI能力将持续指数增长的预测缺乏稳健性，应更多关注预测的不确定性。

Abstract: Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims that fitting sigmoid/logistic curves results in inflection points far in the future, we fit a sigmoid curve to their current data and find that the inflection point has already passed. In addition, we propose a more complex model that decomposes AI capabilities into base and reasoning capabilities, exhibiting individual rates of improvement. We prove that this model supports our hypothesis that AI capabilities will exhibit an inflection point in the near future. Our goal is not to establish a rigorous forecast of our own, but to highlight the fragility of existing forecasts of exponential growth.

</details>


### [21] [Fluid Representations in Reasoning Models](https://arxiv.org/abs/2602.04843)
*Dmitrii Kharlapenko,Alessandro Stolfo,Arthur Conmy,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: 该研究表明，QwQ-32B模型能够通过逐步优化其内部表示来解决抽象问题，并且在推理过程中表现出结构化的编码而非具体的动作名称。通过干预实验，作者证明了这种优化对于问题解决至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在处理抽象问题时表现出色，但其内部机制尚不明确。本研究旨在通过分析QwQ-32B模型（一种专门训练以产生详尽推理痕迹的模型），深入理解这种高性能背后的机制。

Method: 通过在Mystery Blocksworld（一个语义模糊的规划领域）上测试QwQ-32B模型，观察其内部表示的变化，并通过干预实验（如注入优化的表示和使用符号表示）来验证这些变化对解决问题的影响。

Result: 研究发现，QwQ-32B模型能够随着时间的推移提高其对动作和概念的抽象编码，特别是关注结构而非具体动作名称。此外，注入来自成功推理痕迹的优化表示可以显著提升准确性；象征性表示可以部分替代模糊编码而不损失太多性能。

Conclusion: 研究表明，推理模型的高性能可以通过上下文中的词汇表示优化（称为流动性推理表示）来驱动。这对理解语言模型在解决问题中的表现具有重要意义，并可能对提升语言模型在特定领域中的应用效果有所帮助。

Abstract: Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.

</details>
