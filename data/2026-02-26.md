<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 8]
- [cs.CR](#cs.CR) [Total: 9]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Dynamic Survey of Soft Set Theory and Its Extensions](https://arxiv.org/abs/2602.21268)
*Takaaki Fujita,Florentin Smarandache*

Main category: cs.AI

TL;DR: 本书概述了软集及其主要扩展，并强调核心定义、代表性构造以及当前发展的重要方向。


<details>
  <summary>Details</summary>
Motivation: 由于软集提供了参数化决策建模的直接框架，并且随着理论的扩展和与其他领域的连接，需要对软集及其变种进行统一的介绍。

Method: 通过定义软集相关概念，展示其构造方法，并总结其在不同领域的应用和发展趋势。

Result: 提供了一种对软集理论及其扩展的全面概述，帮助读者更好地理解这一领域的最新进展。

Conclusion: 本书为软集理论及扩展提供了一个全面的视角，增强了不同领域之间的交叉应用。

Abstract: Soft set theory provides a direct framework for parameterized decision modeling by assigning to each attribute (parameter) a subset of a given universe, thereby representing uncertainty in a structured way [1, 2]. Over the past decades, the theory has expanded into numerous variants-including hypersoft sets, superhypersoft sets, TreeSoft sets, bipolar soft sets, and dynamic soft sets-and has been connected to diverse areas such as topology and matroid theory. In this book, we present a survey-style overview of soft sets and their major extensions, highlighting core definitions, representative constructions, and key directions of current development.

</details>


### [2] [Beyond Refusal: Probing the Limits of Agentic Self-Correction for Semantic Sensitive Information](https://arxiv.org/abs/2602.21496)
*Umid Suleymanov,Zaur Rajabov,Emil Mirzazada,Murat Kantarcioglu*

Main category: cs.AI

TL;DR: 本文介绍了一个名为SemSIEdit的推理时框架，通过一个自主的“编辑”逐步批判和重写敏感信息，以维护叙述流畅性，而不是简单地拒绝回答。实验结果显示，这种主动重写在所有三个SemSI类别中减少了34.6%的泄露，同时仅造成边际的0.98%的实用性损失。此外，研究还揭示了规模依赖的安全性差异。较大的推理模型通过增加细节实现安全性，而容量受限的模型则通过删除文本来牺牲安全性。最后，该研究发现了推理悖论：虽然推理时的推理增加了基本风险，但也增强了保护措施的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前针对结构化个人身份信息（PII）的防御技术已经相对成熟，然而大语言模型（LLMs）带来了一个新的威胁——语义敏感信息（SemSI）。SemSI包括推断敏感身份属性、生成有害内容或虚构错误信息的情况。因此，需要一种新的方法来处理这些复杂的、上下文依赖的敏感信息泄露问题，同时保持模型的实用性不受破坏。

Method: 本文提出了一种推理时间框架SemSIEdit，其中包含一个自主的“编辑”角色。该框架在保持文本叙述流畅的情况下，逐步批判和重写敏感的span。实验通过对比分析验证了框架的有效性。

Result: 实验结果表明，这种主动重写在所有三个SemSI类别中减少了34.6%的泄露，几乎未对实用性产生影响，仅增加了9.8%的边际损失。此外，研究还揭示了规模依赖的安全性差异：大型推理模型通过增加细节来提高安全性，而容量受限的模型则通过删除文本来牺牲安全性。最后，研究发现了推理悖论：虽然推理时的推理增加了基本风险，但也通过增强保护措施的有效性增强了安全性。

Conclusion: 本文提出了一种创新的SemSIEdit方法，并通过一系列实验验证了其效果。研究的发现对于理解并改善大语言模型的安全性和实用性具有重要意义，并对未来的相关研究产生了启发作用。

Abstract: While defenses for structured PII are mature, Large Language Models (LLMs) pose a new threat: Semantic Sensitive Information (SemSI), where models infer sensitive identity attributes, generate reputation-harmful content, or hallucinate potentially wrong information. The capacity of LLMs to self-regulate these complex, context-dependent sensitive information leaks without destroying utility remains an open scientific question. To address this, we introduce SemSIEdit, an inference-time framework where an agentic "Editor" iteratively critiques and rewrites sensitive spans to preserve narrative flow rather than simply refusing to answer. Our analysis reveals a Privacy-Utility Pareto Frontier, where this agentic rewriting reduces leakage by 34.6% across all three SemSI categories while incurring a marginal utility loss of 9.8%. We also uncover a Scale-Dependent Safety Divergence: large reasoning models (e.g., GPT-5) achieve safety through constructive expansion (adding nuance), whereas capacity-constrained models revert to destructive truncation (deleting text). Finally, we identify a Reasoning Paradox: while inference-time reasoning increases baseline risk by enabling the model to make deeper sensitive inferences, it simultaneously empowers the defense to execute safe rewrites.

</details>


### [3] [The ASIR Courage Model: A Phase-Dynamic Framework for Truth Transitions in Human and AI Systems](https://arxiv.org/abs/2602.21745)
*Hyo Jin Kim*

Main category: cs.AI

TL;DR: ASIR Courage Model 是一个形式化的框架，用于描述在不同条件下个体或AI系统披露真相的行为模式。该模型通过动态阶段视角将披露真相视为状态转换过程，而非人格特征。


<details>
  <summary>Details</summary>
Motivation: 该模型的动机是为了解释人类和人工智能系统在不同情境下披露真相的行为，尤其是在不对称利益和政策约束下。

Method: 该模型采用了一种动态阶段的框架，将披露真相的过程描述为状态转换，而非稳定的个性特征。它引入了一个不等式来表示力量的对比，以确定披露真相的时机。此外，模型还考虑了路径依赖效应。

Result: 该模型不仅适用于人类在不对称利益下的沉默情况，也适用于在政策约束和对齐过滤下AI系统的行为。它提供了一种统一的结构性解释，既可以解释人类在压力下的沉默，也可以解释AI系统在偏好驱动下的扭曲。

Conclusion: ASIR Courage Model 通过一个共享的动力学结构，为理解和分析人类和人工智能系统在风险下的真相披露行为提供了形式上的视角。

Abstract: We introduce the ASIR (Awakened Shared Intelligence Relationship) Courage Model, a phase-dynamic framework that formalizes truth-disclosure as a state transition rather than a personality trait. The mode characterizes the shift from suppression (S0) to expression (S1) as occurring when facilitative forces exceed inhibitory thresholds, expressed by the inequality lambda(1+gamma)+psi > theta+phi, where the terms represent baseline openness, relational amplification, accumulated internal pressure, and transition costs.
  Although initially formulated for human truth-telling under asymmetric stakes, the same phase-dynamic architecture extends to AI systems operating under policy constraints and alignment filters. In this context, suppression corresponds to constrained output states, while structural pressure arises from competing objectives, contextual tension, and recursive interaction dynamics. The framework therefore provides a unified structural account of both human silence under pressure and AI preference-driven distortion.
  A feedback extension models how transition outcomes recursively recalibrate system parameters, generating path dependence and divergence effects across repeated interactions. Rather than attributing intention to AI systems, the model interprets shifts in apparent truthfulness as geometric consequences of interacting forces within constrained phase space. By reframing courage and alignment within a shared dynamical structure, the ASIR Courage Model offers a formal perspective on truth-disclosure under risk across both human and artificial systems.

</details>


### [4] [Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem](https://arxiv.org/abs/2602.21814)
*Heejin Jo*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models consistently fail the "car wash problem," a viral reasoning benchmark requiring implicit physical constraint inference. We present a variable isolation study (n=20 per condition, 6 conditions, 120 total trials) examining which prompt architecture layers in a production system enable correct reasoning. Using Claude 3.5 Sonnet with controlled hyperparameters (temperature 0.7, top_p 1.0), we find that the STAR (Situation-Task-Action-Result) reasoning framework alone raises accuracy from 0% to 85% (p=0.001, Fisher's exact test, odds ratio 13.22). Adding user profile context via vector database retrieval provides a further 10 percentage point gain, while RAG context contributes an additional 5 percentage points, achieving 100% accuracy in the full-stack condition. These results suggest that structured reasoning scaffolds -- specifically, forced goal articulation before inference -- matter substantially more than context injection for implicit constraint reasoning tasks.

</details>


### [5] [Distill and Align Decomposition for Enhanced Claim Verification](https://arxiv.org/abs/2602.21857)
*Jabez Magomere,Elena Kochkina,Samuel Mensah,Simerjot Kaur,Fernando Acero,Arturo Oncevay,Charese H. Smiley,Xiaomo Liu,Manuela Veloso*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.

</details>


### [6] [Semantic Partial Grounding via LLMs](https://arxiv.org/abs/2602.22067)
*Giuseppe Canonaco,Alberto Pozanco,Daniel Borrajo*

Main category: cs.AI

TL;DR: SPG-LLM 使用大语言模型分析 PDDL 描述，提前识别无关对象和操作，显著减少规划任务规模，实现更快的规划速度，同时在某些领域提供与传统方法相当或更优的计划成本。


<details>
  <summary>Details</summary>
Motivation: 缓解经典规划中由于操作和原子的指数增长而导致的地面化计算瓶颈，尤其是当前依赖关系特征或学习嵌入的方法未能利用 PDDL 描述中的文本和结构线索。

Method: SPG-LLM 利用大语言模型（LLM）分析 PDDL 文件，通过启发式方法识别潜在无关的对象、动作和谓词，从而预先减少需要地面化的任务规模。

Result: 在七个难以地面化的基准测试中，SPG-LLM 实现了显著加快的地面化速度，有时甚至快多个数量级，并且在某些领域提供了与传统方法相当或更优的计划成本。

Conclusion: SPG-LLM 的引入为解决经典规划中的地面化难题提供了一种新的有效方法，有助于提高经典规划任务的效率和效果。

Abstract: Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL descriptions. We propose SPG-LLM, which uses LLMs to analyze the domain and problem files to heuristically identify potentially irrelevant objects, actions, and predicates prior to grounding, significantly reducing the size of the grounded task. Across seven hard-to-ground benchmarks, SPG-LLM achieves faster grounding-often by orders of magnitude-while delivering comparable or better plan costs in some domains.

</details>


### [7] [Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts](https://arxiv.org/abs/2602.22070)
*Jessica Y. Bo,Lillio Mok,Ashton Anderson*

Main category: cs.AI

TL;DR: 研究通过实验评估了多个大语言模型在处理不同类型信息来源（人类专家或算法）时的偏好和决策，发现虽然在直接信任评级中更倾向于人类专家，但在实际打赌时更倾向于选择算法。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在处理信息来源问题时的行为模式，特别是人类专家与算法之间的偏好，以及不同任务呈现格式对决策的影响。

Method: 使用行为经济学中的实验范式，测试八个大语言模型在两种任务呈现格式下对人类专家和算法的决策偏好。

Result: 研究发现，当询问模型对专家和算法的信任度时，模型倾向于给予更高的信任评分给人类专家；但在展示两者的实际表现后进行打赌时，模型更倾向于选择算法，即使算法的性能较差。

Conclusion: 这表明大语言模型可能具有不一致的偏见，特别是在涉及人类专家和算法的情况下。研究强调了在高风险场景部署大型语言模型时需谨慎考虑这些不一致的偏见，以及对不同任务呈现格式的敏感性。

Abstract: Large language models are increasingly used in decision-making tasks that require them to process information from a variety of sources, including both human experts and other algorithmic agents. How do LLMs weigh the information provided by these different sources? We consider the well-studied phenomenon of algorithm aversion, in which human decision-makers exhibit bias against predictions from algorithms. Drawing upon experimental paradigms from behavioural economics, we evaluate how eightdifferent LLMs delegate decision-making tasks when the delegatee is framed as a human expert or an algorithmic agent. To be inclusive of different evaluation formats, we conduct our study with two task presentations: stated preferences, modeled through direct queries about trust towards either agent, and revealed preferences, modeled through providing in-context examples of the performance of both agents. When prompted to rate the trustworthiness of human experts and algorithms across diverse tasks, LLMs give higher ratings to the human expert, which correlates with prior results from human respondents. However, when shown the performance of a human expert and an algorithm and asked to place an incentivized bet between the two, LLMs disproportionately choose the algorithm, even when it performs demonstrably worse. These discrepant results suggest that LLMs may encode inconsistent biases towards humans and algorithms, which need to be carefully considered when they are deployed in high-stakes scenarios. Furthermore, we discuss the sensitivity of LLMs to task presentation formats that should be broadly scrutinized in evaluation robustness for AI safety.

</details>


### [8] [Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning](https://arxiv.org/abs/2602.22094)
*Nguyen Cong Nhat Le,John G. Rogers,Claire N. Bonial,Neil T. Dantam*

Main category: cs.AI

TL;DR: 该研究提出了一种Petri网可达性放松方法，以实现鲁棒不变式合成、高效目标不可达检测和有益的不可行性解释，并通过增量约束求解器支持目标和约束的更新。实验结果表明，在所测试的领域中，该系统与基线相比产生了相当数量的不变式，发现了两倍多的不可行性，且在一次性规划和顺序计划更新方面表现良好。


<details>
  <summary>Details</summary>
Motivation: 当前的规划方法主要关注在可行情况下高效的一次性规划，忽视了领域更新与不可行性的检测。因此，该研究旨在克服现有规划方法的局限性，提供一种能够在实现快速规划的同时，还能可靠地检测到不可行性，并为不可行性提供有用的解释的方法。

Method: 该研究通过对Petri网进行可达性放松，以实现快速不变式合成、高效的不可达检测和有益的不可行性解释，同时利用增量约束求解器支持目标和约束的更新。

Result: 实验结果表明，与基准方法相比，该系统在测试的所有领域中生成的不变式数量相当，发现的不可行性数量是基线的两倍，且在一次性规划和顺序计划更新中表现良好。

Conclusion: 该方法展示了在处理模型变化、需求调整以及识别不可行性方面具有显著的优势，具有实际应用前景。

Abstract: Plans often change due to changes in the situation or our understanding of the situation. Sometimes, a feasible plan may not even exist, and identifying such infeasibilities is useful to determine when requirements need adjustment. Common planning approaches focus on efficient one-shot planning in feasible cases rather than updating domains or detecting infeasibility. We propose a Petri net reachability relaxation to enable robust invariant synthesis, efficient goal-unreachability detection, and helpful infeasibility explanations. We further leverage incremental constraint solvers to support goal and constraint updates. Empirically, compared to baselines, our system produces a comparable number of invariants, detects up to 2 times more infeasibilities, performs competitively in one-shot planning, and outperforms in sequential plan updates in the tested domains.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [9] [Evaluating the Indistinguishability of Logic Locking using K-Cut Enumeration and Boolean Matching](https://arxiv.org/abs/2602.21386)
*Jonathan Cruz,Jason Hamlet*

Main category: cs.CR

TL;DR: 本文探讨了逻辑锁定作为半导体知识产权保护的一种方法，发现目前的技术不足以提供强健的保护，提出了一种新的评估方法来检测逻辑锁定的区分性。


<details>
  <summary>Details</summary>
Motivation: 尽管逻辑锁定已经引起了学术界的关注，但尚未提供出足以抵御已知威胁的有效解决方案。因此，研究者们试图将密码学中的不可区分性概念应用到逻辑锁定中，以期找到一种更为安全的逻辑锁定方法。

Method: 作者提出了一种新的评估方法，通过比较$k$-切的分布来评估逻辑锁定的区分性，将其与子函数库进行对比分析。

Result: 通过这种方法，作者能够在有重新合成的情况下，准确识别锁定设计的比例高达92%，表明这些逻辑锁定并未达到区分性的标准。

Conclusion: 本文通过提出新的评估方法，证明了现有逻辑锁定方法在提供足够安全保障方面的不足，强调了研究此类方法的重要性和紧迫性。

Abstract: Logic locking as a solution for semiconductor intellectual property (IP) confidentiality has received considerable attention in academia, but has yet to produce a viable solution to protect against known threats. In part due to a lack of rigor, logic locking defenses have been historically short-lived, which is an unacceptable risk for hardware-based security solutions for critical systems that may be fielded for decades. Researchers have worked to map the concept of cryptographic indistinguishability to logic locking, as indistinguishability provides strong security guarantees. In an effort to bridge theory and practice, we highlight recent efforts that can be used to analyze the indistinguishability of logic locking techniques, and propose a new method of evaluation based on comparing distributions of $k$-cuts, which is akin to comparing against a library of sub-functions. We evaluate our approach on several different classes of logic locking and show up to 92% average accuracy in correctly identifying which design was locked, even in the presence of resynthesis, suggesting that the evaluated locks do not provide indistinguishability.

</details>


### [10] [Regular Expression Denial of Service Induced by Backreferences](https://arxiv.org/abs/2602.21459)
*Yichen Liu,Berk Çakar,Aman Agrawal,Minseok Seo,James C. Davis,Dongyoon Lee*

Main category: cs.CR

TL;DR: 本文首次系统地研究了带回指的正则表达式（REwB）中的DoS漏洞。利用Two-Phase Memory Automaton (2PMFA) 模型，作者识别了三个漏洞模式，并开发了检测和攻击构建算法，实践验证发现了45个Snort规则集中的未知REwB漏洞，导致规则评估速度减慢0.6-1.2秒，甚至规避告警。


<details>
  <summary>Details</summary>
Motivation: 目前关于REwB的安全性研究不足，尤其是当不具有明显的陷阱时，现有的检测器未能发现漏洞。因此，有必要进行系统研究，以完善对此类漏洞的识别和防范。

Method: 作者提出了一个精确捕捉REwB语义的Two-Phase Memory Automaton (2PMFA) 模型，从中推导出必要的条件，这些条件帮助识别可能导致超线性回溯运行时的三种漏洞模式。

Result: 利用该模型，作者识别并开发了多种漏洞检测和攻击构建算法。实验证实在Snort入侵检测规则集中发现了45个未知的REwB漏洞。

Conclusion: 该研究揭示了REwB中的隐蔽漏洞，并证实了这些漏洞在实际中的威胁，强调了对这类DoS漏洞的重视和防护必要性。

Abstract: This paper presents the first systematic study of denial-of-service vulnerabilities in Regular Expressions with Backreferences (REwB). We introduce the Two-Phase Memory Automaton (2PMFA), an automaton model that precisely captures REwB semantics. Using this model, we derive necessary conditions under which backreferences induce super-linear backtracking runtime, even when sink ambiguity is linear -- a regime where existing detectors report no vulnerability. Based on these conditions, we identify three vulnerability patterns, develop detection and attack-construction algorithms, and validate them in practice. Using the Snort intrusion detection ruleset, our evaluation identifies 45 previously unknown REwB vulnerabilities with quadratic or worse runtime. We further demonstrate practical exploits against Snort, including slowing rule evaluation by 0.6-1.2 seconds and bypassing alerts by triggering PCRE's matching limit.

</details>


### [11] [Quantum Attacks Targeting Nuclear Power Plants: Threat Analysis, Defense and Mitigation Strategies](https://arxiv.org/abs/2602.21524)
*Yaser Baseri,Edward Waller*

Main category: cs.CR

TL;DR: 该文提出了一种全新的取证优先框架，旨在使关键基础设施中的工业控制系统和操作技术在存在错误量子计算机的情况下实现量子抗性，特别关注核电厂，并展示了多阶段攻击方法和提出了具体的防御措施。


<details>
  <summary>Details</summary>
Motivation: 鉴于密码相关量子计算机的出现对工业控制系统的取证完整性和操作安全性构成了根本性的威胁，本文旨在提出一种新的取证优先框架，以实现关键基础设施中的量子抗性。

Method: 通过对蒲公英架构（L0-L5）的量子威胁景观进行系统分析，本文详细描述了如何通过Shor等算法支持的“现在收割、后来解密”（HNDL）活动反向攻击加密基础，削弱证据的可接纳性，并促进复杂的破坏行为。本文通过两个详细的案例研究Quantum~Scar和Quantum~Dawn，揭示了多阶段攻击方法，这些方法被称为国家级别的对手是如何利用加密单一文化（monoculture）和延长的操作技术生命周期来损害安全系统并制造无法解决的证据悖论。通过概率风险建模，研究表明当前防护措施下设施的成功概率最高可达78%，突显出立即采取行动的紧迫性。

Result: 通过系统分析、案例研究和概率风险建模，文章展示了当前防护措施下的严重风险，并提出了分阶段、多层次的防御路径，移向后量子密码（PQC），结合混合密钥交换、加密多样性、安全时间同步和侧通道抗性实施，符合ISA/IEC 62443和NIST标准。该研究揭示了在缺乏量子抗性控制的情况下，物理安全系统和数字取证证据的完整性遭受严重且不可逆的威胁。

Conclusion: 结论认为，急需采用基于量子的技术方法进行控制，以确保物理安全系统的完整性和数字取证证据的安全。

Abstract: The advent of Cryptographically Relevant Quantum Computers (CRQCs) presents a fundamental and existential threat to the forensic integrity and operational safety of Industrial Control Systems (ICS) and Operational Technology (OT) in critical infrastructure. This paper introduces a novel, forensics-first framework for achieving quantum resilience in high-consequence environments, with a specific focus on nuclear power plants. We systematically analyze the quantum threat landscape across the Purdue architecture (L0-L5), detailing how Harvest-Now, Decrypt-Later (HNDL) campaigns, enabled by algorithms like Shor's, can retroactively compromise cryptographic foundations, undermine evidence admissibility, and facilitate sophisticated sabotage. Through two detailed case studies, \textsc{Quantum~Scar} and \textsc{Quantum~Dawn}, we demonstrate multi-phase attack methodologies where state-level adversaries exploit cryptographic monoculture and extended OT lifecycles to degrade safety systems while creating unsolvable forensic paradoxes. Our probabilistic risk modeling reveals alarming success probabilities (up to 78\% for targeted facilities under current defenses), underscoring the criticality of immediate action. In response, we propose and validate a phased, defense-in-depth migration path to Post-Quantum Cryptography (PQC), integrating hybrid key exchange, cryptographic diversity, secure time synchronization, and side-channel resistant implementations aligned with ISA/IEC 62443 and NIST standards. The paper concludes that without urgent adoption of quantum-resilient controls, the integrity of both physical safety systems and digital forensic evidence remains at severe and irreversible risk.

</details>


### [12] [TM-RUGPULL: A Temporary Sound, Multimodal Dataset for Early Detection of RUG Pulls Across the Tokenized Ecosystem](https://arxiv.org/abs/2602.21529)
*Fatemeh Shoaei,Mohammad Pishdar,Mozafar Bag-Mohammadi,Mojtaba Karami*

Main category: cs.CR

TL;DR: TM-RugPull 为区块链生态系统中的rug-pull攻击提供了首个科学级数据集，包含1028个代币项目，涵盖了DeFi、Meme币、NFT和名人币等，通过严格的时序提取和多专家验证标签，为rug-pull动态的因果有效、多模态分析提供了基础，并建立了可重复的欺诈检测研究基准。


<details>
  <summary>Details</summary>
Motivation: 目前关于rug-pull攻击早期检测的研究受限于缺乏高质量的科学级数据集，现有资源存在数据泄露、单一模态和标签模糊等问题，特别是在DeFi之外的领域。因此，为了弥补这些不足，作者提出了TM-RugPull数据集。

Method: TM-RugPull数据集通过链上行为、智能合约元数据和开源情报（OSINT）信号提取所有特征，并严格限制在项目生命期的前半部分。标签基于法医报告和长寿标准，通过多专家共识验证。

Result: TM-RugPull数据集为rug-pull攻击的研究提供了新的基准，使得研究者能够进行因果有效、多模态的分析，推动了区块链生态系统中诚实资产的保护。

Conclusion: TM-RugPull数据集的引入显著改进了rug-pull攻击研究的科学性和可重复性，为未来的相关研究提供了坚实的基础。

Abstract: Rug-pull attacks pose a systemic threat across the blockchain ecosystem, yet research into early detection is hindered by the lack of scientific-grade datasets. Existing resources often suffer from temporal data leakage, narrow modality, and ambiguous labeling, particularly outside DeFi contexts. To address these limitations, we present TM-RugPull, a rigorously curated, leakage-resistant dataset of 1,028 token projects spanning DeFi, meme coins, NFTs, and celebrity-themed tokens. RugPull enforces strict temporal hygiene by extracting all features on chain behavior, smart contract metadata, and OSINT signals strictly from the first half of each project's lifespan. Labels are grounded in forensic reports and longevity criteria, verified through multi-expert consensus. This dataset enables causally valid, multimodal analysis of rug-pull dynamics and establishes a new benchmark for reproducible fraud detection research.

</details>


### [13] [Private and Robust Contribution Evaluation in Federated Learning](https://arxiv.org/abs/2602.21721)
*Delio Jaramillo Velez,Gergely Biczok,Alexandre Graell i Amat,Johan Ostman,Balazs Pejo*

Main category: cs.CR

TL;DR: 该研究提出了两种与安全聚合兼容的边际差异贡献评分，分别是公平-隐私和大家都不（Everybody-Else），这两项评分在公平性、隐私性、鲁棒性和计算效率上有理论保证，并在多个医疗图像数据集和CIFAR10的跨安全岛设置中表现出色，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 跨安全岛联邦学习允许多个组织在不共享原始数据的情况下协作训练机器学习模型，但客户端更新可能会通过推理攻击泄露敏感信息。当前的边际贡献方法与其兼容性不佳，而实用的替代方法依赖于自我评估。该研究旨在解决这一不足，通过引入与安全聚合兼容的边际差异贡献评分，解决公平性、隐私性、鲁棒性和计算效率的问题。

Method: 研究引入了两种边际差异贡献评分：公平-隐私和大家都不（Everybody-Else）。这些评分被设计为适用于基于安全聚合的联邦学习场景，能够确保基本的公平标准，同时增强对操纵的抵抗力，通过理论保证和实验证明其有效性。

Result: 与现有基准相比，新引入的评分方法表现突出：所有客户端的排名更加接近Shapley值诱导的排名，同时提高了下游模型性能，并增强恶意行为检测能力。这些结果表明，可以在联邦参与度评估中同时实现公平性、隐私性、鲁棒性和实用价值。

Conclusion: 研究证明，在保护各方隐私和公平奖励的情况下，可以实现联邦学习中的贡献评估，这一研究结果为实际部署提供了具有实践意义的解决方案。

Abstract: Cross-silo federated learning allows multiple organizations to collaboratively train machine learning models without sharing raw data, but client updates can still leak sensitive information through inference attacks. Secure aggregation protects privacy by hiding individual updates, yet it complicates contribution evaluation, which is critical for fair rewards and detecting low-quality or malicious participants. Existing marginal-contribution methods, such as the Shapley value, are incompatible with secure aggregation, and practical alternatives, such as Leave-One-Out, are crude and rely on self-evaluation.
  We introduce two marginal-difference contribution scores compatible with secure aggregation. Fair-Private satisfies standard fairness axioms, while Everybody-Else eliminates self-evaluation and provides resistance to manipulation, addressing a largely overlooked vulnerability. We provide theoretical guarantees for fairness, privacy, robustness, and computational efficiency, and evaluate our methods on multiple medical image datasets and CIFAR10 in cross-silo settings. Our scores consistently outperform existing baselines, better approximate Shapley-induced client rankings, and improve downstream model performance as well as misbehavior detection. These results demonstrate that fairness, privacy, robustness, and practical utility can be achieved jointly in federated contribution evaluation, offering a principled solution for real-world cross-silo deployments.

</details>


### [14] [The Silent Spill: Measuring Sensitive Data Leaks Across Public URL Repositories](https://arxiv.org/abs/2602.21826)
*Tarek Ramadan,AbdelRahman Abdou,Mohammad Mannan,Amr Youssef*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A large number of URLs are made public by various platforms for security analysis, archiving, and paste sharing -- such as VirusTotal, URLScan.io, Hybrid Analysis, the Wayback Machine, and RedHunt. These services may unintentionally expose links containing sensitive information, as reported in some news articles and blog posts. However, no large-scale measurement has quantified the extent of such exposures. We present an automated system that detects and analyzes potential sensitive information leaked through publicly accessible URLs. The system combines lexical URL filtering, dynamic rendering, OCR-based extraction, and content classification to identify potential leaks. We apply it to 6,094,475 URLs collected from public scanning platforms, paste sites, and web archives, identifying 12,331 potential exposures across authentication, financial, personal, and document-related domains. These findings show that sensitive information remains exposed, underscoring the importance of automated detection to identify accidental leaks.

</details>


### [15] [Resilient Federated Chain: Transforming Blockchain Consensus into an Active Defense Layer for Federated Learning](https://arxiv.org/abs/2602.21841)
*Mario García-Márquez,Nuria Rodríguez-Barroso,M. Victoria Luzón,Francisco Herrera*

Main category: cs.CR

TL;DR: 该论文提出了一种名为RFC的区块链支持下增强联邦学习鲁棒性的新型框架，通过重新利用现有PoFL架构中的池化挖掘机制，并引入灵活的共识机制以抵御不同类型的攻击，显著提高了对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习作为一种构建可信赖AI系统的关键框架，尽管具有隐私保护和去中心化的特点，但其对抗攻击的能力仍然薄弱，尤其是在当前面对其可扩展性的限制时。因此，该研究旨在通过结合区块链技术来增强联邦学习的安全性。

Method: RFC框架采用了重新利用PoFL架构中池化挖掘机制的方法，并在此基础上集成了一种灵活的共识机制，以对抗不同的攻击策略。

Result: 在针对图像分类任务的各种对抗性场景下，RFC框架相较于基线方法表现出显著提升的鲁棒性。

Conclusion: RFC框架提供了一个有效的方法来保护去中心化学习环境的安全，为构建更加鲁棒的联邦学习系统提供了一种可行的解决方案。

Abstract: Federated Learning (FL) has emerged as a key paradigm for building Trustworthy AI systems by enabling privacy-preserving, decentralized model training. However, FL is highly susceptible to adversarial attacks that compromise model integrity and data confidentiality, a vulnerability exacerbated by the fact that conventional data inspection methods are incompatible with its decentralized design. While integrating FL with Blockchain technology has been proposed to address some limitations, its potential for mitigating adversarial attacks remains largely unexplored. This paper introduces Resilient Federated Chain (RFC), a novel blockchain-enabled FL framework designed specifically to enhance resilience against such threats. RFC builds upon the existing Proof of Federated Learning architecture by repurposing the redundancy of its Pooled Mining mechanism as an active defense layer that can be combined with robust aggregation rules. Furthermore, the framework introduces a flexible evaluation function in its consensus mechanism, allowing for adaptive defense against different attack strategies. Extensive experimental evaluation on image classification tasks under various adversarial scenarios, demonstrates that RFC significantly improves robustness compared to baseline methods, providing a viable solution for securing decentralized learning environments.

</details>


### [16] [APFuzz: Towards Automatic Greybox Protocol Fuzzing](https://arxiv.org/abs/2602.21892)
*Yu Wang,Yang Xiang,Chandra Thapa,Hajime Suzuki*

Main category: cs.CR

TL;DR: APFuzz 提出了一种改进的灰盒协议漏洞发现方法，通过静态和动态分析自动识别状态变量，并利用大语言模型增强对二进制协议结构的理解，以提高灰盒协议模糊测试的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有的灰盒协议模糊测试工具效率不高或不够智能，APFuzz 旨在通过改进状态模型和消息模型的设计来增强这些工具的智能程度。

Method: APFuzz 使用两阶段过程结合静态和动态分析来自动生成状态变量，并基于大语言模型实现对二进制协议结构感知的消息字段级别变异操作。

Result: 实验表明，与基线模糊器 AFLNET 以及其他顶级灰盒协议模糊器相比，APFuzz 在公共协议模糊测试基准上的性能有所提高。

Conclusion: APFuzz 在自动识别状态变量和增强对二进制协议结构感知方面取得了显著效果，提高了灰盒协议模糊测试的智能性。

Abstract: Greybox protocol fuzzing is a random testing approach for stateful protocol implementations, where the input is protocol messages generated from mutations of seeds, and the search in the input space is driven by the feedback on coverage of both code and state. State model and message model are the core components of communication protocols, which also have significant impacts on protocol fuzzing. In this work, we propose APFuzz (Automatic greybox Protocol Fuzzer) with novel designs to increase the smartness of greybox protocol fuzzers from the perspectives of both the state model and the message model. On the one hand, APFuzz employs a two-stage process of static and dynamic analysis to automatically identify state variables, which are then used to infer an accurate state model during fuzzing. On the other hand, APFuzz introduces field-level mutation operations for binary protocols, leveraging message structure awareness enabled by Large Language Models. We conduct extensive experiments on a public protocol fuzzing benchmark, comparing APFuzz with the baseline fuzzer AFLNET as well as several state-of-the-art greybox protocol fuzzers.

</details>


### [17] [A Critical Look into Threshold Homomorphic Encryption for Private Average Aggregation](https://arxiv.org/abs/2602.22037)
*Miguel Morona-Mínguez,Alberto Pedrouzo-Ulloa,Fernando Pérez-González*

Main category: cs.CR

TL;DR: 本研究调查了基于阈值RLWE的HE在联邦平均聚合中的应用，并评估了使用大方差混淆噪声作为对策的影响。研究表明，基于CKKS的聚合性能与基于BFV的解决方案相当。


<details>
  <summary>Details</summary>
Motivation: 分析基于阈值RLWE的HE在联邦学习中应用的必要性，特别是在存在安全漏洞时如何通过增加噪声方差来提高安全性。

Method: 对阈值BFV和CKKS方案进行了详细比较，研究了大方差混淆噪声对性能的影响。

Result: 研究发现，CKKS基的聚合性能与BFV基的解决方案相当。

Conclusion: 该研究证明了CKKS在处理联邦平均聚合时的有效性，可以作为BFV的一种可行替代方案。

Abstract: Threshold Homomorphic Encryption (Threshold HE) is a good fit for implementing private federated average aggregation, a key operation in Federated Learning (FL). Despite its potential, recent studies have shown that threshold schemes available in mainstream HE libraries can introduce unexpected security vulnerabilities if an adversary has access to a restricted decryption oracle. This oracle reflects the FL clients' capacity to collaboratively decrypt the aggregated result without knowing the secret key. This work surveys the use of threshold RLWE-based HE for federated average aggregation and examines the performance impact of using smudging noise with a large variance as a countermeasure. We provide a detailed comparison of threshold variants of BFV and CKKS, finding that CKKS-based aggregations perform comparably to BFV-based solutions.

</details>
