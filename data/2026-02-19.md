<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 4]
- [cs.AI](#cs.AI) [Total: 12]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Weak Zero-Knowledge and One-Way Functions](https://arxiv.org/abs/2602.16156)
*Rohit Chatterjee,Yunqi Li,Prashant Nalini Vasudevan*

Main category: cs.CR

TL;DR: 这篇论文探讨了弱零知识（ZK）协议对最难语言的影响，并在假设 NP 中存在最难语言的情况下，展示了弱 ZK 证明或论证的存在性与 OWFs 之间的关系。论文分为三个结论：1. 如果所有 NP 语言都存在满足 ε_c+ε_s+ε_z < 1 的非零知识证明，则 OWFs 存在；2. 如果所有 NP 语言都存在 k 轮公共硬币零知识证明且满足 ε_c+ε_s+(2k-1)ε_z < 1，则 OWFs 存在；3. 如果对于常数 k，所有 NP 语言都存在 k 轮公共硬币零知识证明且满足 ε_c+ε_s+kε_z < 1，则无穷多次 OWFs 存在。


<details>
  <summary>Details</summary>
Motivation: 研究 ZK 协议对于 NP 完全问题的影响，旨在探索在假设 NP 中存在最难问题的情况下，相关证明或论证的存在性如何与密码学中的基本假设（如 OWFs）联系起来。

Method: 通过形式化的数学证明来论证结论。假设存在具有特定误差率的 NP 语言，进而推导出 OWFs 的存在。使用了复杂度理论和零知识证明的相关概念。

Result: 证明了在特定条件下，NP 语言的某些零知识证明或论证的存在性可推出 OWFs 的存在。在不严格限制误差率的情况下，提供了比以往研究更广泛的结论。

Conclusion: 论文的研究结果加强了零知识证明领域和基础密码学之间的联系，为在包含最难问题的语言中寻找弱零知识协议奠定了基础，并进一步探讨了 OWFs 的存在条件。

Abstract: We study the implications of the existence of weak Zero-Knowledge (ZK) protocols for worst-case hard languages. These are protocols that have completeness, soundness, and zero-knowledge errors (denoted $ε_c$, $ε_s$, and $ε_z$, respectively) that might not be negligible. Under the assumption that there are worst-case hard languages in NP, we show the following:
  1. If all languages in NP have NIZK proofs or arguments satisfying $ ε_c+ε_s+ ε_z < 1 $, then One-Way Functions (OWFs) exist.
  This covers all possible non-trivial values for these error rates. It additionally implies that if all languages in NP have such NIZK proofs and $ε_c$ is negligible, then they also have NIZK proofs where all errors are negligible. Previously, these results were known under the more restrictive condition $ ε_c+\sqrt{ε_s}+ε_z < 1 $ [Chakraborty et al., CRYPTO 2025].
  2. If all languages in NP have $k$-round public-coin ZK proofs or arguments satisfying $ ε_c+ε_s+(2k-1).ε_z < 1 $, then OWFs exist.
  3. If, for some constant $k$, all languages in NP have $k$-round public-coin ZK proofs or arguments satisfying $ ε_c+ε_s+k.ε_z < 1 $, then infinitely-often OWFs exist.

</details>


### [2] [Quantum Oracle Distribution Switching and its Applications to Fully Anonymous Ring Signatures](https://arxiv.org/abs/2602.16268)
*Marvin Beckmann,Christian Majenz*

Main category: cs.CR

TL;DR: 本研究在量子访问随机预言模型（QROM）中为两种环签名构造提供了四个安全归约：一种基于AOS框架，另一种基于环陷门结构。使用了多种技术手段，包括量子随机预言模型中的直行化提取工具、压缩预言、无历史归约以及量子重现工具。


<details>
  <summary>Details</summary>
Motivation: 随着后量子时代加密协议的需求增加，特别是对于可抵赖的认证密钥交换的需求，现有的环签名在ROM中的安全性已不足以保护系统的安全。因此需要在QROM中进行安全归约，确保证据的安全性。

Method: 本研究采用了多种技术手段，在QROM中为环签名构造提供安全归约，包括对AOS框架和环陷门结构的构造进行安全归约，使用了直行化提取工具、压缩预言、无历史归约以及量子重现工具。

Result: 研究提供了AOS框架和环陷门结构两种环签名构造的安全归约，并通过实验证明了其在QROM中的可行性。

Conclusion: 研究表明，基于AOS框架和环陷门结构的环签名在QROM中具有较高的安全性，能够满足后量子时代的需求。

Abstract: Ring signatures are a powerful primitive that allows a member to sign on behalf of a group, without revealing their identity. Recently, ring signatures have received additional attention as an ingredient for post-quantum deniable authenticated key exchange, e.g., for a post-quantum version of the Signal protocol, employed by virtually all end-to-end-encrypted messenger services. While several ring signature constructions from post-quantum assumptions offer suitable security and efficiency for use in deniable key exchange, they are currently proven secure in the random oracle model (ROM) only, which is insufficient for post-quantum security.
  In this work, we provide four security reductions in the quantum-accessible random oracle model (QROM) for two generic ring signature constructions: two for the AOS framework and two for a construction paradigm based on ring trapdoors, whose generic backbone we formalize. The two security proofs for AOS ring signatures differ in their requirements on the underlying sigma protocol and their tightness. The two reductions for the ring-trapdoor-based ring signatures exhibit various differences in requirements and the security they provide. We employ the measure-and-reprogram technique, QROM straightline extraction tools based on the compressed oracle, history-free reductions and QROM reprogramming tools. To make use of Rényi divergence properties in the QROM, we study the behavior of quantum algorithms that interact with an oracle whose distribution is based on one of two different distributions over the set of outputs. We provide tight bounds for the statistical distance, show that the Rényi divergence can not be used to replace the entire oracle and provide a workaround.

</details>


### [3] [Mind the Gap: Evaluating LLMs for High-Level Malicious Package Detection vs. Fine-Grained Indicator Identification](https://arxiv.org/abs/2602.16304)
*Ahmed Ryan,Ibrahim Khalil,Abdullah Al Jahid,Md Erfan,Akond Ashfaque Ur Rahman,Md Rayhanur Rahman*

Main category: cs.CR

TL;DR: 该研究评估了13个大型语言模型在检测恶意软件包及其特定恶意指标方面的性能。发现GPT-4.1在二分类任务中几乎完美，但在检测特定恶意指标时性能下降约41%。研究还发现，通用模型更擅长过滤大部分威胁，而专门的编程模型更适合检测遵循严格、可预测代码结构的攻击。


<details>
  <summary>Details</summary>
Motivation: 鉴于开源仓库中恶意软件包的威胁，探索大型语言模型在检测恶意软件中的应用，以增强软件供应链的安全性。

Method: 使用一个由4,070个包组成的精心策划的数据集（3,700个良性包和370个恶意包），评估13个大型语言模型在二分类和多标签分类任务中的表现。进一步探讨提示策略、温度设置和模型规格对检测准确度的影响。

Result: 研究显示，GPT-4.1在二分类任务中的表现几乎完美，但在检测特定恶意指标时，准确性下降了约41%。通用模型更擅长过滤大多数威胁，而专门的编程模型更适合识别严格、可预测代码结构的攻击。参数量和上下文宽度对检测准确性的影响较小。

Conclusion: 尽管大型语言模型在包层面具有强大的检测能力，但在精细的指标层面仍缺乏所需的语义深度。

Abstract: The prevalence of malicious packages in open-source repositories, such as PyPI, poses a critical threat to the software supply chain. While Large Language Models (LLMs) have emerged as a promising tool for automated security tasks, their effectiveness in detecting malicious packages and indicators remains underexplored. This paper presents a systematic evaluation of 13 LLMs for detecting malicious software packages. Using a curated dataset of 4,070 packages (3,700 benign and 370 malicious), we evaluate model performance across two tasks: binary classification (package detection) and multi-label classification (identification of specific malicious indicators). We further investigate the impact of prompting strategies, temperature settings, and model specifications on detection accuracy. We find a significant "granularity gap" in LLMs' capabilities. While GPT-4.1 achieves near-perfect performance in binary detection (F1 $\approx$ 0.99), performance degrades by approximately 41\% when the task shifts to identifying specific malicious indicators. We observe that general models are best for filtering out the majority of threats, while specialized coder models are better at detecting attacks that follow a strict, predictable code structure. Our correlation analysis indicates that parameter size and context width have negligible explanatory power regarding detection accuracy. We conclude that while LLMs are powerful detectors at the package level, they lack the semantic depth required for precise identification at the granular indicator level.

</details>


### [4] [Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents](https://arxiv.org/abs/2602.16520)
*Doron Shavit*

Main category: cs.CR

TL;DR: RLM-JB 是一个基于递归语言模型（RLMs）的全面的反越狱检测框架，通过分步处理、证据聚合和跨片段信号处理，有效检测潜在的攻击，并保持高度的精度和低误报率。


<details>
  <summary>Details</summary>
Motivation: 面对日益复杂的语言模型攻击，特别是针对代执行工具的攻击，需要一个全面且有效的检测框架来保证系统的安全性。

Method: RLM-JB 使用了递归语言模型来构建检测框架，通过分步处理输入、证据聚合以及跨片段信号处理等方法，提高了检测的可靠性和准确性。

Result: 在针对 AutoDAN 风格的对抗输入中，RLM-JB 能够在三个不同的大规模语言模型后端中实现高检测效力（ASR/召回率92.5-98.0%），并保持高精度（98.99-100%）和低误报率（0.0-2.0%）。

Conclusion: 该研究证明了基于递归语言模型的反越狱检测框架的有效性，且在不同语言模型后端均能保持高性能，为未来的大规模语言模型安全提供了借鉴意义。

Abstract: Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries worker models over covered segments, and aggregates evidence into an auditable decision. RLM-JB treats detection as a procedure rather than a one-shot classification: it normalizes and de-obfuscates suspicious inputs, chunks text to reduce context dilution and guarantee coverage, performs parallel chunk screening, and composes cross-chunk signals to recover split-payload attacks. On AutoDAN-style adversarial inputs, RLM-JB achieves high detection effectiveness across three LLM backends (ASR/Recall 92.5-98.0%) while maintaining very high precision (98.99-100%) and low false positive rates (0.0-2.0%), highlighting a practical sensitivity-specificity trade-off as the screening backend changes.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [Towards Efficient Constraint Handling in Neural Solvers for Routing Problems](https://arxiv.org/abs/2602.16012)
*Jieyi Bi,Zhiguang Cao,Jianan Zhou,Wen Song,Yaoxin Wu,Jie Zhang,Yining Ma,Cathy Wu*

Main category: cs.AI

TL;DR: 论文提出了名为Construct-and-Refine（CaR）的框架，该框架是一个基于显式学习可满足性细化的通用且高效的约束处理方案，特别适用于神经路由求解器在面对复杂约束时的有效性，相较于先前方法，在处理硬约束时具有更高的可行性和解决方案质量，且计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 针对神经求解器在处理复杂约束时表现不佳的问题，以及现有处理约束的方法效率低下或不适用于硬约束的情况，提出该框架旨在提高在复杂约束下的性能。

Method: 框架通过设计一个联合训练框架，指导构造模块生成多样且高质量的解决方案以适用于轻量级改进过程。同时，提出的构造-改进共享表示法可以在更复杂的约束场景中潜在地共享知识。

Result: 实验表明，CaR在典型硬路由约束下获得了比传统和神经状态最先进求解器更好的可行性和解决方案质量，并且具有更高的计算效率。

Conclusion: 该框架为神经路由求解器处理复杂约束提供了一种有效且通用的方法，具有广阔的应用前景。

Abstract: Neural solvers have achieved impressive progress in addressing simple routing problems, particularly excelling in computational efficiency. However, their advantages under complex constraints remain nascent, for which current constraint-handling schemes via feasibility masking or implicit feasibility awareness can be inefficient or inapplicable for hard constraints. In this paper, we present Construct-and-Refine (CaR), the first general and efficient constraint-handling framework for neural routing solvers based on explicit learning-based feasibility refinement. Unlike prior construction-search hybrids that target reducing optimality gaps through heavy improvements yet still struggle with hard constraints, CaR achieves efficient constraint handling by designing a joint training framework that guides the construction module to generate diverse and high-quality solutions well-suited for a lightweight improvement process, e.g., 10 steps versus 5k steps in prior work. Moreover, CaR presents the first use of construction-improvement-shared representation, enabling potential knowledge sharing across paradigms by unifying the encoder, especially in more complex constrained scenarios. We evaluate CaR on typical hard routing constraints to showcase its broader applicability. Results demonstrate that CaR achieves superior feasibility, solution quality, and efficiency compared to both classical and neural state-of-the-art solvers.

</details>


### [6] [Optimization Instability in Autonomous Agentic Workflows for Clinical Symptom Detection](https://arxiv.org/abs/2602.16037)
*Cameron Cagan,Pedram Fard,Jiazi Tian,Jingya Cheng,Shawn N. Murphy,Hossein Estiri*

Main category: cs.AI

TL;DR: 研究发现自动优化的临床症状分类系统在罕见症状分类中表现出优化不稳定现象，即使采用回溯选择机制也无法完全避免失败。然而，通过回溯选择，系统在脑雾和胸部疼痛检测方面的表现优于专家定制的词典。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探究自动优化过程中可能出现的优化不稳定现象，尤其是在低频临床症状分类任务中的表现。研究还探讨了针对此问题的两种扩展机制的有效性。

Method: 研究使用Pythia框架对三种不同临床症状（呼吸困难、胸痛和长期COVID脑雾）进行自动化优化评估，同时提出两种干预策略来监测和纠正优化不稳定现象。

Result: 研究发现，系统在低频症状（如长COVID脑雾）分类中表现出优化不稳定现象，导致系统准确性高但未能检测出任何阳性病例。两种干预策略中，回溯选择（selector agent）在脑雾和胸痛检测方面优于主动干预（guiding agent），并在低频症状分类中表现出更稳定的表现。

Conclusion: 本文揭示了自动优化系统在低频症状分类中的关键失败模式，并证明了回溯选择机制对于提升低频症状分类任务稳定性的优越性。

Abstract: Autonomous agentic workflows that iteratively refine their own behavior hold considerable promise, yet their failure modes remain poorly characterized. We investigate optimization instability, a phenomenon in which continued autonomous improvement paradoxically degrades classifier performance, using Pythia, an open-source framework for automated prompt optimization. Evaluating three clinical symptoms with varying prevalence (shortness of breath at 23%, chest pain at 12%, and Long COVID brain fog at 3%), we observed that validation sensitivity oscillated between 1.0 and 0.0 across iterations, with severity inversely proportional to class prevalence. At 3% prevalence, the system achieved 95% accuracy while detecting zero positive cases, a failure mode obscured by standard evaluation metrics. We evaluated two interventions: a guiding agent that actively redirected optimization, amplifying overfitting rather than correcting it, and a selector agent that retrospectively identified the best-performing iteration successfully prevented catastrophic failure. With selector agent oversight, the system outperformed expert-curated lexicons on brain fog detection by 331% (F1) and chest pain by 7%, despite requiring only a single natural language term as input. These findings characterize a critical failure mode of autonomous AI systems and demonstrate that retrospective selection outperforms active intervention for stabilization in low-prevalence classification tasks.

</details>


### [7] [How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment](https://arxiv.org/abs/2602.16039)
*Hang Li,Kaiqi Yang,Xianxuan Long,Fedor Filippov,Yucheng Chu,Yasemin Copur-Gencturk,Peng He,Cory Miller,Namsoo Shin,Joseph Krajcik,Hui Liu,Jiliang Tang*

Main category: cs.AI

TL;DR: 该研究评估了多种不确定性量化方法在基于大型语言模型的自动评估中的表现，并通过全面分析不确定性行为，探讨了不同不确定性指标的优缺点及其影响因素，为开发更可靠和有效的不确定性感知评分系统提供了实用见解。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在自动评估中的崛起，评估结果的不准确或未校准的不确定性估计可能导致下游干预不稳定，影响学生的学习过程。因此，有必要系统地理解这一挑战，指导未来的研究。

Method: 研究通过全面分析多个评估数据集、不同大型语言模型家族以及解码设置下的不确定性行为，研究不同不确定性度量的优缺点。

Result: 研究揭示了大型语言模型在评分场景中展示出的不确定性模式，并评估了不同不确定性指标的有效性，分析了模型家族、评估任务和解码策略等关键因素对不确定性估计的影响。

Conclusion: 研究为开发更可靠和有效的不确定性感知评分系统提供了行动指南，为未来的研究奠定了基础。

Abstract: The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they also introduce new challenges related to output uncertainty, stemming from the inherently probabilistic nature of LLMs. Output uncertainty is an inescapable challenge in automatic assessment, as assessment results often play a critical role in informing subsequent pedagogical actions, such as providing feedback to students or guiding instructional decisions. Unreliable or poorly calibrated uncertainty estimates can lead to unstable downstream interventions, potentially disrupting students' learning processes and resulting in unintended negative consequences. To systematically understand this challenge and inform future research, we benchmark a broad range of uncertainty quantification methods in the context of LLM-based automatic assessment. Although the effectiveness of these methods has been demonstrated in many tasks across other domains, their applicability and reliability in educational settings, particularly for automatic grading, remain underexplored. Through comprehensive analyses of uncertainty behaviors across multiple assessment datasets, LLM families, and generation control settings, we characterize the uncertainty patterns exhibited by LLMs in grading scenarios. Based on these findings, we evaluate the strengths and limitations of different uncertainty metrics and analyze the influence of key factors, including model families, assessment tasks, and decoding strategies, on uncertainty estimates. Our study provides actionable insights into the characteristics of uncertainty in LLM-based automatic assessment and lays the groundwork for developing more reliable and effective uncertainty-aware grading systems in the future.

</details>


### [8] [Improving Interactive In-Context Learning from Natural Language Feedback](https://arxiv.org/abs/2602.16066)
*Martin Klissarov,Jonathan Cook,Diego Antognini,Hao Sun,Jingling Li,Natasha Jaques,Claudiu Musat,Edward Grefenstette*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的框架，旨在使大规模语言模型具备在上下文互动中动态学习的能力。通过将单轮可验证任务转化为多轮教学互动，模型在困难推理任务中的反馈整合能力显著提升。在数学问题上的互动训练推广到多个领域，展示了增强的上下文适应性和自我改善能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型训练方式过分依赖于静态语料库，忽略了重要的反馈循环机制。文章旨在弥补这一缺陷，通过引入一个可扩展的方法重新定义模型互动学习的能力，从而使得大语言模型能够更好地适应实际应用场景。

Method: 本文的方法首先通过多轮互动将单轮验证任务转化为教学互动，强化信息不对称以促进模型逐步提高。其次，对现有主流模型在困难推理任务上的反馈整合能力进行实验分析，发现它们表现不佳。然后，通过本文的方法进行训练，显著提升了模型在语言反馈中的多轮交互能力。最后，通过定性分析研究了模型改进的原因。

Result: 实验结果表明，较小的模型在多轮表现上几乎达到了更大模型的效果。此外，互动训练在数学问题上的成效能够推广至其他领域，包括编程、谜题和迷宫导航等，表现出强大的分布外泛化能力。

Conclusion: 本文通过将互动学习作为一个可被训练的技能引入，实现了大语言模型在上下文互动中的能力提升，并展示了这一方法的有效性。该框架为模型自我改进提供了一个统一路径，通过预测老师的批评，模型能够自纠错，无需外部教师介入。

Abstract: Adapting one's thought process based on corrective feedback is an essential ability in human learning, particularly in collaborative settings. In contrast, the current large language model training paradigm relies heavily on modeling vast, static corpora. While effective for knowledge acquisition, it overlooks the interactive feedback loops essential for models to adapt dynamically to their context. In this work, we propose a framework that treats this interactive in-context learning ability not as an emergent property, but as a distinct, trainable skill. We introduce a scalable method that transforms single-turn verifiable tasks into multi-turn didactic interactions driven by information asymmetry. We first show that current flagship models struggle to integrate corrective feedback on hard reasoning tasks. We then demonstrate that models trained with our approach dramatically improve the ability to interactively learn from language feedback. More specifically, the multi-turn performance of a smaller model nearly reaches that of a model an order of magnitude larger. We also observe robust out-of-distribution generalization: interactive training on math problems transfers to diverse domains like coding, puzzles and maze navigation. Our qualitative analysis suggests that this improvement is due to an enhanced in-context plasticity. Finally, we show that this paradigm offers a unified path to self-improvement. By training the model to predict the teacher's critiques, effectively modeling the feedback environment, we convert this external signal into an internal capability, allowing the model to self-correct even without a teacher.

</details>


### [9] [GPSBench: Do Large Language Models Understand GPS Coordinates?](https://arxiv.org/abs/2602.16105)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: 该研究引入了一个名为GPSBench的大规模数据集，用于评估大型语言模型在地理空间推理方面的表现，涵盖坐标操作和结合世界知识的推理。研究发现，尽管模型在现实地理推理方面表现较好，但在几何计算和地理知识的层次性方面存在显著差异，且微调会导致计算和知识之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在现实世界应用中的部署增加，如导航、机器人或制图，鲁棒的地理空间推理成为关键能力。然而，语言模型在处理GPS坐标和现实世界地理方面的推理能力研究不足。研究者创建了GPSBench数据集以填补这一空白，旨在更好地评估并促进相关技术的发展。

Method: 研究人员构建了一个包含57,800个样本的GPSBench数据集，涵盖17项任务以及几何坐标操作和结合世界知识的推理。他们评估了14款最先进的语言模型，使用一个标准来衡量这些模型的地理空间推理能力，即考虑模型的内在能力而非工具辅助。

Result: 实验结果显示，模型在现实地理方面的推理表现优于几何计算。地理知识呈现出层次性的衰退，表现为在国家层面的性能优于城市层面的定位。模型对坐标噪声的鲁棒性表明，这些模型真正理解了坐标而非仅依赖记忆。此外，研究人员发现坐标增强能够提升下游地理空间任务的效果，同时微调会带来几何计算能力提升与世界知识下降之间的权衡。

Conclusion: 研究得出结论，尽管大型语言模型在地理方面的推理有潜力，但在几何计算、层次地理知识和鲁棒性方面仍存在问题。未来研究需更深入探索模型的能力，并考虑如何优化以获得更全面的性能提升。

Abstract: Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains underexplored. We introduce GPSBench, a dataset of 57,800 samples across 17 tasks for evaluating geospatial reasoning in LLMs, spanning geometric coordinate operations (e.g., distance and bearing computation) and reasoning that integrates coordinates with world knowledge. Focusing on intrinsic model capabilities rather than tool use, we evaluate 14 state-of-the-art LLMs and find that GPS reasoning remains challenging, with substantial variation across tasks: models are generally more reliable at real-world geographic reasoning than at geometric computations. Geographic knowledge degrades hierarchically, with strong country-level performance but weak city-level localization, while robustness to coordinate noise suggests genuine coordinate understanding rather than memorization. We further show that GPS-coordinate augmentation can improve in downstream geospatial tasks, and that finetuning induces trade-offs between gains in geometric computation and degradation in world knowledge. Our dataset and reproducible code are available at https://github.com/joey234/gpsbench

</details>


### [10] [Revolutionizing Long-Term Memory in AI: New Horizons with High-Capacity and High-Speed Storage](https://arxiv.org/abs/2602.16192)
*Hiroaki Yamanaka,Daisuke Miyashita,Takashi Toi,Asuka Maki,Taiga Ikeda,Jun Deguchi*

Main category: cs.AI

TL;DR: 本文探索了实现人工智能超级智能（ASI）至关重要的‘记忆’设计概念，提出了提取后再存储、存储后再按需提取、从大量概率体验中发现更深层次的洞见以及提高体验收集效率等替代方法。


<details>
  <summary>Details</summary>
Motivation: 为了实现人工智能超级智能（ASI），探索并强调了不同的‘记忆’设计策略，以避免信息损失和促进更高效的体验收集，同时推进相关研究领域。

Method: 在没有提出新型方法的情况下，讨论了四种替代方法，并通过简单的实验验证了它们的有效性。

Result: 简单的实验表明，提出的替代方法在实践中是有效的。

Conclusion: 尽管这些替代方法看似有效，但在调查这些有前途的方向中仍面临重大挑战，提出了一些研究主题来解决这些问题。

Abstract: Driven by our mission of "uplifting the world with memory," this paper explores the design concept of "memory" that is essential for achieving artificial superintelligence (ASI). Rather than proposing novel methods, we focus on several alternative approaches whose potential benefits are widely imaginable, yet have remained largely unexplored. The currently dominant paradigm, which can be termed "extract then store," involves extracting information judged to be useful from experiences and saving only the extracted content. However, this approach inherently risks the loss of information, as some valuable knowledge particularly for different tasks may be discarded in the extraction process. In contrast, we emphasize the "store then on-demand extract" approach, which seeks to retain raw experiences and flexibly apply them to various tasks as needed, thus avoiding such information loss. In addition, we highlight two further approaches: discovering deeper insights from large collections of probabilistic experiences, and improving experience collection efficiency by sharing stored experiences. While these approaches seem intuitively effective, our simple experiments demonstrate that this is indeed the case. Finally, we discuss major challenges that have limited investigation into these promising directions and propose research topics to address them.

</details>


### [11] [Verifiable Semantics for Agent-to-Agent Communication](https://arxiv.org/abs/2602.16424)
*Philipp Schoenegger,Matt Carlson,Chris Schneider,Chris Daly*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multiagent AI systems require consistent communication, but we lack methods to verify that agents share the same understanding of the terms used. Natural language is interpretable but vulnerable to semantic drift, while learned protocols are efficient but opaque. We propose a certification protocol based on the stimulus-meaning model, where agents are tested on shared observable events and terms are certified if empirical disagreement falls below a statistical threshold. In this protocol, agents restricting their reasoning to certified terms ("core-guarded reasoning") achieve provably bounded disagreement. We also outline mechanisms for detecting drift (recertification) and recovering shared vocabulary (renegotiation). In simulations with varying degrees of semantic divergence, core-guarding reduces disagreement by 72-96%. In a validation with fine-tuned language models, disagreement is reduced by 51%. Our framework provides a first step towards verifiable agent-to-agent communication.

</details>


### [12] [Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.16435)
*Arun Vignesh Malarkkan,Wangyang Ying,Yanjie Fu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors, grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ~4x relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and efficiency of automated feature engineering.

</details>


### [13] [Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach](https://arxiv.org/abs/2602.16481)
*Zihao Li,Fabrizio Russo*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.

</details>


### [14] [Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs](https://arxiv.org/abs/2602.16512)
*Felix Fricke,Simon Malberg,Georg Groh*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.

</details>


### [15] [Creating a digital poet](https://arxiv.org/abs/2602.16578)
*Vered Tohar,Tsahi Hayat,Amir Leshem*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image. In a blinded authorship test with 50 humanities students and graduates (three AI poems and three poems by well-known poets each), judgments were at chance: human poems were labeled human 54% of the time and AI poems 52%, with 95% confidence intervals including 50%. After the workshop, a commercial publisher released a poetry collection authored by the model. These results show that workshop-style prompting can support long-horizon creative shaping and renew debates on creativity and authorship.

</details>


### [16] [Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments](https://arxiv.org/abs/2602.16653)
*Yangjie Xu,Lujun Li,Lama Sleem,Niccolo Gentile,Yewei Song,Yiqun Wang,Siming Ji,Wenbo Wu,Radu State*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.

</details>
