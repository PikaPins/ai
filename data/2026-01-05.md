<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 21]
- [cs.AI](#cs.AI) [Total: 19]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing](https://arxiv.org/abs/2601.00042)
*Manish Bhatt,Adrian Wood,Idan Habler,Ammar Al-Kahfah*

Main category: cs.CR

TL;DR: 该研究使用Go-Explore评估了GPT-4o-mini，并发现随机种子变异是主要因素，奖励塑造会负面影响性能，而简单的状态签名更优。多模型有利于攻击类型的多样性，但单个模型在单一攻击类型上能更好优化覆盖。


<details>
  <summary>Details</summary>
Motivation: 由于即使经过安全训练，具备工具使用能力的LLM仍需安全测试，本研究旨在探索评估此类模型的有效方法。

Method: 研究使用Go-Explore方法进行了28次实验，分别测试了六项不同的研究问题，通过随机种子和算法参数的变化，观察其对模型性能的影响。

Result: 研究发现，随机种子变异显著影响了模型的表现，奖励塑造会降低探索力并导致假阳性较高的情况，简单状态签名更为有效。

Conclusion: 基于这些结果，研究指出，即使模型经过安全训练，种子变异和特定领域的知识可能在安全性测试中比算法复杂性更为关键。

Abstract: Production LLM agents with tool-using capabilities require security testing despite their safety training. We adapt Go-Explore to evaluate GPT-4o-mini across 28 experimental runs spanning six research questions. We find that random-seed variance dominates algorithmic parameters, yielding an 8x spread in outcomes; single-seed comparisons are unreliable, while multi-seed averaging materially reduces variance in our setup. Reward shaping consistently harms performance, causing exploration collapse in 94% of runs or producing 18 false positives with zero verified attacks. In our environment, simple state signatures outperform complex ones. For comprehensive security testing, ensembles provide attack-type diversity, whereas single agents optimize coverage within a given attack type. Overall, these results suggest that seed variance and targeted domain knowledge can outweigh algorithmic sophistication when testing safety-trained models.

</details>


### [2] [Overlooked Safety Vulnerability in LLMs: Malicious Intelligent Optimization Algorithm Request and its Jailbreak](https://arxiv.org/abs/2601.00213)
*Haoran Gu,Handing Wang,Yi Mei,Mengjie Zhang,Yaochu Jin*

Main category: cs.CR

TL;DR: 该研究关注大语言模型在自动化算法设计中的安全性漏洞，并提出了一个恶意优化算法基准（MalOptBench）和相应的破解方法（MOBjailbreak），揭示了现存模型在面对此类攻击时的高度易受攻击性。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型的广泛应用引发了对其在算法设计中的潜在风险和安全隐患的关注，该项研究旨在填补现有研究的空白，关注在自动化算法设计中的安全问题。

Method: 该研究通过创建一个包含60个恶意优化算法请求的基准MalOptBench，并提出了一种专门针对该场景的破解方法MOBjailbreak，对13种主流的大语言模型（包括最新的GPT-5和DeepSeek-V3.1）进行了广泛的评估，以揭示模型在面对此类攻击时的脆弱性和攻击的成功率。

Result: 研究发现大部分模型在面对此类攻击时存在较高的易受攻击率，平均攻击成功率达到了83.59%，同时对一些新的插件式防御进行了评估，发现这些防御措施对MOBjailbreak的有效性有限，可能产生过度的安全措施。

Conclusion: 研究强调了在算法设计中加强大语言模型的安全保护措施的紧迫性，并指出需要更强的对齐技术来防止模型被误用。

Abstract: The widespread deployment of large language models (LLMs) has raised growing concerns about their misuse risks and associated safety issues. While prior studies have examined the safety of LLMs in general usage, code generation, and agent-based applications, their vulnerabilities in automated algorithm design remain underexplored. To fill this gap, this study investigates this overlooked safety vulnerability, with a particular focus on intelligent optimization algorithm design, given its prevalent use in complex decision-making scenarios. We introduce MalOptBench, a benchmark consisting of 60 malicious optimization algorithm requests, and propose MOBjailbreak, a jailbreak method tailored for this scenario. Through extensive evaluation of 13 mainstream LLMs including the latest GPT-5 and DeepSeek-V3.1, we reveal that most models remain highly susceptible to such attacks, with an average attack success rate of 83.59% and an average harmfulness score of 4.28 out of 5 on original harmful prompts, and near-complete failure under MOBjailbreak. Furthermore, we assess state-of-the-art plug-and-play defenses that can be applied to closed-source models, and find that they are only marginally effective against MOBjailbreak and prone to exaggerated safety behaviors. These findings highlight the urgent need for stronger alignment techniques to safeguard LLMs against misuse in algorithm design.

</details>


### [3] [Evolution of Android's Permission-based Security Model and Challenges](https://arxiv.org/abs/2601.00252)
*Rajendra Kumar Solanki,Vijay Laxmi,Manoj Singh Gaur*

Main category: cs.CR

TL;DR: 该研究通过全面的文献综述和对比分析，关注2010年至2022年间Android权限模型及其相关研究，系统化地概述了Android API调用与权限映射、权限演变及权限检查的知识，并识别了过去十年中的权限相关问题和研究工作，总结研究中的空白并为早期和有经验的研究人员提供未来方向。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于分析自2008年Android操作系统推出以来，尤其是2010年至2022年间，Android权限模型的演变和相关研究成果。鉴于权限模型尚未完全成熟，通过总结相关研究工作，识别现有问题，可为未来研究提供有价值的参考。

Method: 该研究采用了全面的文献调研方法，重点研究了2010年至2022年间关于Android权限模型的研究。研究通过对比分析的方法，系统化地概述了API调用与权限映射关系，分析了权限模型的演变过程及权限检查机制，并识别了现有研究中的空白，为未来研究提供了方向。

Result: 研究结果显示，尽管Android权限模型有了显著改进，但仍存在一些未解决的问题。研究团队总结了过去十年中关于Android权限模型的研究工作，识别出权限管理方面的潜在问题和相关研究，为未来研究提供了有意义的方向。

Conclusion: 综合文献调研结果和对比分析，该研究为理解和解决Android权限模型相关问题提供了深入见解，并指出了未来的研究方向。研究认为，通过进一步的研究，可以改进Android权限系统的灵活性和安全性，使其更加适应不断变化的应用需求和技术进步。

Abstract: Android Permission Model and Application (app) analysis has consistently remained the focus of the investigation of research groups and stakeholders of the Android ecosystem since it was launched in 2008. Even though the Android smartphone operating system (OS) permission model has evolved significantly from `all-or-none access' to `user-chosen dangerous resource access', specific challenges and issues remain unresolved even after 15 years after the smartphone OS launch. This study addresses the issues and documents the research work in this arena through a comprehensive literature survey and comparative analysis.
  The survey's focal point is the Android permission model and relevant research between 2010-2022. We systematize the knowledge on (i) Android API Calls to permissions mapping, (ii) Android Permissions evolution, and (iii) how permissions are checked. Furthermore, the survey identifies the permission-related issues and relevant research addressed during the last decade. We reference seminal work in these areas. We summarize the identified research gaps and present future directions for early and experienced researchers.

</details>


### [4] [Rectifying Adversarial Examples Using Their Vulnerabilities](https://arxiv.org/abs/2601.00270)
*Fumiya Morimoto,Ryuto Morita,Satoshi Ono*

Main category: cs.CR

TL;DR: 本文提出了一种方法以修复对抗样本（AEs），通过重新攻击AEs使其超出决策边界从而准确预测其原始输入的标签。该方法无需针对特定攻击方法调整参数或先进行培训，展示了对不同攻击方法生成的AEs具有一致性表现，并在多种攻击下的稳定性上优于传统修正方法和输入变换方法。


<details>
  <summary>Details</summary>
Motivation: 现有对抗样本检测方法大多侧重于仅鉴别对抗样本，而不是在攻击前进行正确的类别分类处理。本文研究旨在提出一种方法来修正对抗样本，以恢复它们的正确标签，增强目标应用的安全性。

Method: 该方法基于重新攻击对抗样本，使得它们在决策边界之外，从而实现准确的标签预测。只考虑对抗样本作为输入，避免了对特定攻击类型进行参数调整的必要。研究采用了简单的方法，无需先进行训练或调整参数，因此适用于多种攻击类型。

Result: 实验结果表明，该方法能够一致地修复通过不同攻击方法生成的对抗样本，不仅适用于白盒攻击，还能有效应对黑盒攻击，且在抵御各种攻击方面的稳定性优于传统修正方法和输入变换方法。

Conclusion: 本文提出的方法为对抗样本的修正提供了新的途径，可以提升机器学习模型在安全应用中的稳健性。

Abstract: Deep neural network-based classifiers are prone to errors when processing adversarial examples (AEs). AEs are minimally perturbed input data undetectable to humans posing significant risks to security-dependent applications. Hence, extensive research has been undertaken to develop defense mechanisms that mitigate their threats. Most existing methods primarily focus on discriminating AEs based on the input sample features, emphasizing AE detection without addressing the correct sample categorization before an attack. While some tasks may only require mere rejection on detected AEs, others necessitate identifying the correct original input category such as traffic sign recognition in autonomous driving. The objective of this study is to propose a method for rectifying AEs to estimate the correct labels of their original inputs. Our method is based on re-attacking AEs to move them beyond the decision boundary for accurate label prediction, effectively addressing the issue of rectifying minimally perceptible AEs created using white-box attack methods. However, challenge remains with respect to effectively rectifying AEs produced by black-box attacks at a distance from the boundary, or those misclassified into low-confidence categories by targeted attacks. By adopting a straightforward approach of only considering AEs as inputs, the proposed method can address diverse attacks while avoiding the requirement of parameter adjustments or preliminary training. Results demonstrate that the proposed method exhibits consistent performance in rectifying AEs generated via various attack methods, including targeted and black-box attacks. Moreover, it outperforms conventional rectification and input transformation methods in terms of stability against various attacks.

</details>


### [5] [From Consensus to Chaos: A Vulnerability Assessment of the RAFT Algorithm](https://arxiv.org/abs/2601.00273)
*Tamer Afifi,Abdelfatah Hegazy,Ehab Abousaif*

Main category: cs.CR

TL;DR: 本文对RAFT协议进行了系统的安全性分析，指出了其在消息重播和伪造攻击方面的脆弱性，并提出了一种基于加密、认证消息验证和新鲜度检查的方法来加强RAFT的安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管RAFT因其简单性、可靠性和效率而闻名，但其安全性尚未得到充分的认识，这使得实现了RAFT的系统容易受到不同类型的攻击和威胁，从而导致数据不一致性。因此，本文旨在揭示RAFT协议在安全性方面可能存在的问题，并提出改进建议。

Method: 通过模拟场景来评估这些攻击的可行性和识别RAFT设计中的关键弱点。然后，提出了一种基于加密、消息认证和新鲜度检查的新型方法来加强RAFT的安全性。

Result: 本文的工作包括了暴露RAFT潜在的安全威胁，并提出了一种新的安全增强框架，这一框架提供了增强现有RAFT实现安全性的基础。

Conclusion: 本文的研究结果表明，通过实施基于加密和认证的消息验证，可以显著提高RAFT协议的安全性，提供了一个增强分布式系统可靠性的可行方案。

Abstract: In recent decades, the RAFT distributed consensus algorithm has become a main pillar of the distributed systems ecosystem, ensuring data consistency and fault tolerance across multiple nodes. Although the fact that RAFT is well known for its simplicity, reliability, and efficiency, its security properties are not fully recognized, leaving implementations vulnerable to different kinds of attacks and threats, which can transform the RAFT harmony of consensus into a chaos of data inconsistency. This paper presents a systematic security analysis of the RAFT protocol, with a specific focus on its susceptibility to security threats such as message replay attacks and message forgery attacks. Examined how a malicious actor can exploit the protocol's message-passing mechanism to reintroduce old messages, disrupting the consensus process and leading to data inconsistency. The practical feasibility of these attacks is examined through simulated scenarios, and the key weaknesses in RAFT's design that enable them are identified. To address these vulnerabilities, a novel approach based on cryptography, authenticated message verification, and freshness check is proposed. This proposed solution provides a framework for enhancing the security of the RAFT implementations and guiding the development of more resilient distributed systems.

</details>


### [6] [Making Theft Useless: Adulteration-Based Protection of Proprietary Knowledge Graphs in GraphRAG Systems](https://arxiv.org/abs/2601.00274)
*Weijie Wang,Peizhuo Lv,Yan Wang,Rujie Dai,Guokun Xu,Qiujian Lv,Hangcheng Liu,Weiqing Huang,Wei Dong,Jiaheng Zhang*

Main category: cs.CR

TL;DR: AURA框架通过预先在知识图谱中注入虚假但合理的篡改，使得未经授权的攻击者无法使用窃取的知识图谱，而授权用户则可以通过解密标签高效地过滤掉这些篡改。实验表明，AURA能将非授权系统的性能降至5.3%的准确率，同时保持授权用户查询结果的完全准确性。


<details>
  <summary>Details</summary>
Motivation: 鉴于组织的知识图谱可能被用于内部应用并包含敏感信息，需要一种有效的方法来防止此类知识图谱被未经授权的用户窃取并用于非正当目的。传统的水印技术因需要访问输出才能检测而无效，而强加密则会带来难以接受的延迟成本。

Method: AURA框架通过在知识图谱中加入虚假但可信的篡改元素作为‘疫苗’，使得攻击者在使用此类知识图谱时生成的结果变得不可信。授权用户则持有私钥，可以解密并过滤这些篡改，从而确保查询结果的准确性。

Result: 实验结果表明，AURA能够显著降低非授权系统的性能，准确率降至5.3%。而对授权用户来说，由于私钥的存在，其查询结果的准确性得到保证，同时AURA也能够有效对抗各种净化尝试，保留80.2%的篡改。

Conclusion: AURA框架提供了一种有效且高效的方法来保护知识图谱的安全性，能够在保证授权用户使用的准确性和便利性的同时，有效遏制未经授权的使用行为。

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) has emerged as a key technique for enhancing Large Language Models (LLMs) with proprietary Knowledge Graphs (KGs) in knowledge-intensive applications. As these KGs often represent an organization's highly valuable intellectual property (IP), they face a significant risk of theft for private use. In this scenario, attackers operate in isolated environments. This private-use threat renders passive defenses like watermarking ineffective, as they require output access for detection. Simultaneously, the low-latency demands of GraphRAG make strong encryption which incurs prohibitive overhead impractical. To address these challenges, we propose AURA, a novel framework based on Data Adulteration designed to make any stolen KG unusable to an adversary. Our framework pre-emptively injects plausible but false adulterants into the KG. For an attacker, these adulterants deteriorate the retrieved context and lead to factually incorrect responses. Conversely, for authorized users, a secret key enables the efficient filtering of all adulterants via encrypted metadata tags before they are passed to the LLM, ensuring query results remain completely accurate. Our evaluation demonstrates the effectiveness of this approach: AURA degrades the performance of unauthorized systems to an accuracy of just 5.3%, while maintaining 100% fidelity for authorized users with negligible overhead. Furthermore, AURA proves robust against various sanitization attempts, retaining 80.2% of its adulterants.

</details>


### [7] [PQC standards alternatives -- reliable semantically secure key encapsulation mechanism and digital signature protocols using the rank-deficient matrix power function](https://arxiv.org/abs/2601.00332)
*Juan Pedro Hecht,Hugo Daniel Scolnik*

Main category: cs.CR

TL;DR: 本文介绍了新型协议、密钥封装机制、数字签名方案以及对抗线性攻击的特殊保护措施。目的是为当前标准寻找可靠的替代方案，设计更紧凑、更快且更安全的公钥交换和数字签名，从而保障互联网流量的安全。


<details>
  <summary>Details</summary>
Motivation: 为了对抗未来的量子计算机带来的威胁，确保现有数据的安全，避免在现在收集数据、待量子计算技术成熟后再进行解密的潜在风险。

Method: 提出了新的协议和机制，特别设计用于提高公钥互换和数字签名的安全性，同时确保这些机制具有紧凑性和高效性。

Result: 成功实现了新型的密钥封装机制和数字签名方案，提高了安全性和效率。

Conclusion: 研究为加密协议提供了新的解决方案，能够顺利过渡到后量子密钥交换和数字签名，增强现有系统在未来的安全性。

Abstract: Post-quantum cryptography-PQC- aims to develop public-key primitives that are secure against adversaries using classical and quantum computing technologies. This study introduces novel protocols, a key encapsulation mechanism, a digital signature scheme, and special protection against linear attacks. Our purpose is to create reliable alternatives to current standards, seeking compact, fast, and secure replacements of the key interchange and digital signature in the TLS 1_3 protocol, which safeguards Internet traffic, allowing an easy post-quantum transition to protect current data from the harvest now, decrypt later threat.

</details>


### [8] [Diamond: Design and Implementation of Breach-Resilient Authenticated Encryption Framework For Internet of Things](https://arxiv.org/abs/2601.00353)
*Saif E. Nouma,Gokhan Mumcu,Attila A. Yavuz*

Main category: cs.CR

TL;DR: Diamond 是一种新型的高性能、低能耗的认证加密框架，适用于资源受限的物联网设备，它减少了预处理时间并降低了端到端延迟，同时保持了紧凑的标签聚合和强大的泄露恢复能力。


<details>
  <summary>Details</summary>
Motivation: 资源受限的物联网设备需要在敌对的无线通道下传输敏感数据。现有的轻量级认证加密（AE）标准缺乏前向安全性保证、紧凑的标签聚合以及在线下-在线优化，这些对于现代高性能的物联网管道是必需的。

Method: Diamond 通过轻量级密钥演化机制、在线下-在线优化计算管道以及针对不同物联网平台的性能分级实现，是首个既具备前向安全性和聚合功能的认证加密框架。

Result: 实验结果表明，Diamond 相对于基准模型和美国国家标准与技术研究院（NIST）轻量级 AE 候选方案在认证加密吞吐量和端到端验证延迟上均表现出更优性能，同时保持了紧凑的标签聚合和强大的泄露恢复能力。

Conclusion: Diamond 已经被正式证明安全，并提供了解决方案的正式证明。开放源代码的发布使得该方案可以被复制和无缝整合进物联网平台。

Abstract: Resource-constrained Internet of Things (IoT) devices, from medical implants to small drones, must transmit sensitive telemetry under adversarial wireless channels while operating under stringent computing and energy budgets. Authenticated Encryption (AE) is essential for ensuring confidentiality, integrity, and authenticity. However, existing lightweight AE standards lack forward-security guarantees, compact tag aggregation, and offline-online (OO) optimizations required for modern high-throughput IoT pipelines.
  We introduce Diamond, the first provable secure Forward-secure and Aggregate Authenticated Encryption (FAAE) framework that extends and generalizes prior FAAE constructions through a lightweight key evolution mechanism, an OO-optimized computation pipeline, and a set of performance-tiered instantiations tailored to heterogeneous IoT platforms. Diamond substantially reduces amortized offline preprocessing (up to 47%) and achieves up to an order-ofmagnitude reduction in end-to-end latency for large telemetry batches. Our comprehensive evaluation across 64-bit ARM Cortex-A72, 32-bit ARM Cortex-M4, and 8-bit AVR architectures confirms that Diamond consistently outperforms baseline FAAE variants and NIST lightweight AE candidates across authenticated encryption throughput and end-to-end verification latency while maintaining compact tag aggregation and strong breach resilience. We formally prove the security of Diamond and provide two concrete instantiations optimized for compliance and high efficiency. Our open-source release enables reproducibility and seamless integration into IoT platforms.

</details>


### [9] [Traffic-MoE: A Sparse Foundation Model for Network Traffic Analysis](https://arxiv.org/abs/2601.00357)
*Jiajun Zhou,Changhui Sun,Meng Shen,Shanqing Yu,Qi Xuan*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While pre-trained large models have achieved state-of-the-art performance in network traffic analysis, their prohibitive computational costs hinder deployment in real-time, throughput-sensitive network defense environments. This work bridges the gap between advanced representation learning and practical network protection by introducing Traffic-MoE, a sparse foundation model optimized for high-efficiency real-time inference. By dynamically routing traffic tokens to a small subset of specialized experts, Traffic-MoE effectively decouples model capacity from computational overhead. Extensive evaluations across three security-oriented tasks demonstrate that Traffic-MoE achieves up to a 12.38% improvement in detection performance compared to leading dense competitors. Crucially, it delivers a 91.62% increase in throughput, reduces inference latency by 47.81%, and cuts peak GPU memory consumption by 38.72%. Beyond efficiency, Traffic-MoE exhibits superior robustness against adversarial traffic shaping and maintains high detection efficacy in few-shot scenarios, establishing a new paradigm for scalable and resilient network traffic analysis.

</details>


### [10] [PatchBlock: A Lightweight Defense Against Adversarial Patches for Embedded EdgeAI Devices](https://arxiv.org/abs/2601.00367)
*Nandish Chattopadhyay,Abdul Basit,Amira Guesmi,Muhammad Abdullah Hanif,Bassem Ouni,Muhammad Shafique*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Adversarial attacks pose a significant challenge to the reliable deployment of machine learning models in EdgeAI applications, such as autonomous driving and surveillance, which rely on resource-constrained devices for real-time inference. Among these, patch-based adversarial attacks, where small malicious patches (e.g., stickers) are applied to objects, can deceive neural networks into making incorrect predictions with potentially severe consequences. In this paper, we present PatchBlock, a lightweight framework designed to detect and neutralize adversarial patches in images. Leveraging outlier detection and dimensionality reduction, PatchBlock identifies regions affected by adversarial noise and suppresses their impact. It operates as a pre-processing module at the sensor level, efficiently running on CPUs in parallel with GPU inference, thus preserving system throughput while avoiding additional GPU overhead. The framework follows a three-stage pipeline: splitting the input into chunks (Chunking), detecting anomalous regions via a redesigned isolation forest with targeted cuts for faster convergence (Separating), and applying dimensionality reduction on the identified outliers (Mitigating). PatchBlock is both model- and patch-agnostic, can be retrofitted to existing pipelines, and integrates seamlessly between sensor inputs and downstream models. Evaluations across multiple neural architectures, benchmark datasets, attack types, and diverse edge devices demonstrate that PatchBlock consistently improves robustness, recovering up to 77% of model accuracy under strong patch attacks such as the Google Adversarial Patch, while maintaining high portability and minimal clean accuracy loss. Additionally, PatchBlock outperforms the state-of-the-art defenses in efficiency, in terms of computation time and energy consumption per sample, making it suitable for EdgeAI applications.

</details>


### [11] [Ouroboros AutoSyn: Time Based Permissionless Synchrony Model for PoS](https://arxiv.org/abs/2601.00370)
*Joshua Shen*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Blockchain as a promising technology is gaining its popularity ever since proof-of-work based Bitcoin came to the world. Nevertheless, Bitcoin achieves consensus at an expensive cost of energy. Proof-of-stake is one of the solutions for such a problem. Participants of PoS protocols achieve dynamic-availability in permissionless settings. Parties can join and leave the protocol at their will without notifying others. However, such protocol relies heavily on a central clock, providing the function of synchrony by collecting the finish status of every honest participant.
  In our protocol, the global function maintains the round information for each participant no longer needed. We analyze and modify the round into real-time based round model. Message delivery delay is also taken into consideration of the round length. However, participant need the connection of a real-world time global clock which is crucial to calculate the current round. And round length also is adjusted due to the changing network situation at the start of every new epoch.

</details>


### [12] [LLM-Powered Analysis of IoT User Reviews: Tracking and Ranking Security and Privacy Concerns](https://arxiv.org/abs/2601.00372)
*Taufiq Islam Protick,Sai Teja Peddinti,Nina Taft,Anupam Das*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Being able to understand the security and privacy (S&P) concerns of IoT users brings benefits to both developers and users. To learn about users' views, we examine Amazon IoT reviews - one of the biggest IoT markets. This work presents a state-of-the-art methodology to identify and categorize reviews in which users express S&P concerns. We developed an automated pipeline by fine-tuning GPT-3.5-Turbo to build two models: the Classifier-Rationalizer-Categorizer and the Thematic Mapper. By leveraging dynamic few-shot prompting and the model's large context size, our pipeline achieved over 97% precision and recall, significantly outperforming keyword-based and classical ML methods. We applied our pipeline to 91K Amazon reviews about fitness trackers, smart speakers and cameras, over multiple years. We found that on average 5% contained S&P concerns, while security camera exhibited the highest prevalence at 10%. Our method detected significantly more S&P-relevant reviews than prior works: 15x more for fitness trackers, 29% more for smart speakers, and 70% more for cameras. Our longitudinal analysis reveals that concerns like surveillance and data control have persisted for years, suggesting limited industry progress. We demonstrate that across all device types, users consistently demand more precise control over what data is collected and shared. We uncover challenges in multi-user and multi-device interactions, identifying two previously unreported themes concerning inadequate controls for account separation and data access. These findings, ranging from broad persistent trends to specific instances of customer loss, offer actionable insights for developers to improve user satisfaction and trust.

</details>


### [13] [Engineering Attack Vectors and Detecting Anomalies in Additive Manufacturing](https://arxiv.org/abs/2601.00384)
*Md Mahbub Hasan,Marcus Sternhagen,Krishna Chandra Roy*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Additive manufacturing (AM) is rapidly integrating into critical sectors such as aerospace, automotive, and healthcare. However, this cyber-physical convergence introduces new attack surfaces, especially at the interface between computer-aided design (CAD) and machine execution layers. In this work, we investigate targeted cyberattacks on two widely used fused deposition modeling (FDM) systems, Creality's flagship model K1 Max, and Ender 3. Our threat model is a multi-layered Man-in-the-Middle (MitM) intrusion, where the adversary intercepts and manipulates G-code files during upload from the user interface to the printer firmware. The MitM intrusion chain enables several stealthy sabotage scenarios. These attacks remain undetectable by conventional slicer software or runtime interfaces, resulting in structurally defective yet externally plausible printed parts. To counter these stealthy threats, we propose an unsupervised Intrusion Detection System (IDS) that analyzes structured machine logs generated during live printing. Our defense mechanism uses a frozen Transformer-based encoder (a BERT variant) to extract semantic representations of system behavior, followed by a contrastively trained projection head that learns anomaly-sensitive embeddings. Later, a clustering-based approach and a self-attention autoencoder are used for classification. Experimental results demonstrate that our approach effectively distinguishes between benign and compromised executions.

</details>


### [14] [Exploring the Integration of Differential Privacy in Cybersecurity Analytics: Balancing Data Utility and Privacy in Threat Intelligence](https://arxiv.org/abs/2601.00385)
*Brahim Khalil Sedraoui,Abdelmadjid Benmachiche,Amina Makhlouf,Chaouki Chemam*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: To resolve the acute problem of privacy protection and guarantee that data can be used in the context of threat intelligence, this paper considers the implementation of Differential Privacy (DP) in cybersecurity analytics. DP, which is a sound mathematical framework, ensures privacy by adding a controlled noise to data outputs and thus avoids sensitive information disclosure even with auxiliary datasets. The use of DP in Security Information and Event Management (SIEM) systems is highlighted, and it can be seen that DP has the capability to protect event log and threat data analysis without interfering with the analytical efficiency. The utility versus privacy trade-offs linked to the maximization of the epsilon parameter, which is one of the critical components of DP mechanisms, is pointed out. The article shows the transformative power of DP in promoting safe sharing of data and joint threat intelligence through real-world systems and case studies. Finally, this paper makes DP one of the key strategies to improve privacy-preserving analytics in the field of cybersecurity.

</details>


### [15] [NOS-Gate: Queue-Aware Streaming IDS for Consumer Gateways under Timing-Controlled Evasion](https://arxiv.org/abs/2601.00389)
*Muhammad Bilal,Omer Tariq,Hasan Ahmed*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Timing and burst patterns can leak through encryption, and an adaptive adversary can exploit them. This undermines metadata-only detection in a stand-alone consumer gateway. Therefore, consumer gateways need streaming intrusion detection on encrypted traffic using metadata only, under tight CPU and latency budgets. We present a streaming IDS for stand-alone gateways that instantiates a lightweight two-state unit derived from Network-Optimised Spiking (NOS) dynamics per flow, named NOS-Gate. NOS-Gate scores fixed-length windows of metadata features and, under a $K$-of-$M$ persistence rule, triggers a reversible mitigation that temporarily reduces the flow's weight under weighted fair queueing (WFQ). We evaluate NOS-Gate under timing-controlled evasion using an executable 'worlds' benchmark that specifies benign device processes, auditable attacker budgets, contention structure, and packet-level WFQ replay to quantify queue impact. All methods are calibrated label-free via burn-in quantile thresholding. Across multiple reproducible worlds and malicious episodes, at an achieved $0.1%$ false-positive operating point, NOS-Gate attains 0.952 incident recall versus 0.857 for the best baseline in these runs. Under gating, it reduces p99.9 queueing delay and p99.9 collateral delay with a mean scoring cost of ~ 2.09 μs per flow-window on CPU.

</details>


### [16] [Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution](https://arxiv.org/abs/2601.00418)
*Prajwal Panth,Sahaj Raj Malla*

Main category: cs.CR

TL;DR: 提出了一种轻量级且无持续协调的自管理协议CPPDD，用于安全的多方数据聚合，支持多种数据类型，具有高效的计算和通信复杂度，能够检测恶意行为并正确恢复数据。


<details>
  <summary>Details</summary>
Motivation: 当前多方安全计算和同态加密等技术在规模化应用中的局限性，如缺乏轻量高效的组件，难以在不信任第三方的环境中实现多方安全协作。

Method: 通过引入一种双重保护机制——客户端特定的仿射遮掩与优先级驱动的顺序共识锁定，结合检查和（sigma_S）与数据（sigma_D）校验和，来实现共识依赖的完整性与公平性，确保在N-1篡改下的安全。

Result: 该框架在MNIST数据上的实验证明了在客户端可以实现毫秒级的计算时间，并具有线性可扩展性。相比MPC和HE等基线技术，该框架在FLOPs上低了3-4个数量级，能有效检测恶意行径并恢复所有数据。

Conclusion: CPPDD为安全多方计算提供了一种新的解决方案，特别适用于监管严格和资源有限的环境下的原子合作，如安全投票，联盟联邦学习，区块链托管，地理信息能力提升等领域，填补了上述技术在可伸缩性，最小信任和多方计算验证等方面的关键空白。

Abstract: We propose the Consensus-Based Privacy-Preserving Data Distribution (CPPDD) framework, a lightweight and post-setup autonomous protocol for secure multi-client data aggregation. The framework enforces unanimous-release confidentiality through a dual-layer protection mechanism that combines per-client affine masking with priority-driven sequential consensus locking. Decentralized integrity is verified via step (sigma_S) and data (sigma_D) checksums, facilitating autonomous malicious deviation detection and atomic abort without requiring persistent coordination. The design supports scalar, vector, and matrix payloads with O(N*D) computation and communication complexity, optional edge-server offloading, and resistance to collusion under N-1 corruptions. Formal analysis proves correctness, Consensus-Dependent Integrity and Fairness (CDIF) with overwhelming-probability abort on deviation, and IND-CPA security assuming a pseudorandom function family. Empirical evaluations on MNIST-derived vectors demonstrate linear scalability up to N = 500 with sub-millisecond per-client computation times. The framework achieves 100% malicious deviation detection, exact data recovery, and three-to-four orders of magnitude lower FLOPs compared to MPC and HE baselines. CPPDD enables atomic collaboration in secure voting, consortium federated learning, blockchain escrows, and geo-information capacity building, addressing critical gaps in scalability, trust minimization, and verifiable multi-party computation for regulated and resource-constrained environments.

</details>


### [17] [Improving LLM-Assisted Secure Code Generation through Retrieval-Augmented-Generation and Multi-Tool Feedback](https://arxiv.org/abs/2601.00509)
*Vidyut Sriram,Sawan Pandita,Achintya Lakshmanan,Aneesh Shamraj,Suman Saha*

Main category: cs.CR

TL;DR: 该研究提出了一种使用检索增强的多工具修复工作流，通过编译器诊断、CodeQL安全扫描和KLEE符号执行迭代修复代码生成LLMs的输出。系统在3,242个程序上进行了评估，显著提高了安全性，特别是在CodeLlama模型中表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前的LLMs在生成代码方面存在安全漏洞、逻辑不一致和编译错误的问题，研究人员提出该方法旨在提高代码生成的鲁棒性，特别是增强安全性。

Method: 研究采用了检索增强、多工具修复的工作流。具体步骤包括利用编译器诊断发现错误、使用CodeQL进行安全扫描识别潜在漏洞、以及通过KLEE进行符号执行以保证逻辑正确性。此外，通过轻量级嵌入式模型检索历史成功的修复方法，提供安全导向示例来指导代码生成。

Result: 该系统在DeepSeek和CodeLlama两个模型上进行了评估，结果显示安全漏洞显著减少。对于DeepSeek，安全漏洞减少了96%；对于CodeLlama，关键安全缺陷率从58.55%降至22.19%，显示出即使在“顽固”模型上也具有工具辅助自我修复的有效性。

Conclusion: 该工作提出的方法显示了通过一系列自动工具修复LLMs生成代码的有效性，特别是在提高安全性方面表现出显著改进，证明了工具辅助自我修复在提升大语言模型生成代码的可靠性上的潜力。

Abstract: Large Language Models (LLMs) can generate code but often introduce security vulnerabilities, logical inconsistencies, and compilation errors. Prior work demonstrates that LLMs benefit substantially from structured feedback, static analysis, retrieval augmentation, and execution-based refinement. We propose a retrieval-augmented, multi-tool repair workflow in which a single code-generating LLM iteratively refines its outputs using compiler diagnostics, CodeQL security scanning, and KLEE symbolic execution. A lightweight embedding model is used for semantic retrieval of previously successful repairs, providing security-focused examples that guide generation. Evaluated on a combined dataset of 3,242 programs generated by DeepSeek-Coder-1.3B and CodeLlama-7B, the system demonstrates significant improvements in robustness. For DeepSeek, security vulnerabilities were reduced by 96%. For the larger CodeLlama model, the critical security defect rate was decreased from 58.55% to 22.19%, highlighting the efficacy of tool-assisted self-repair even on "stubborn" models.

</details>


### [18] [Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?](https://arxiv.org/abs/2601.00559)
*Jason Quantrill,Noura Khajehnouri,Zihan Guo,Manar H. Alalfi*

Main category: cs.CR

TL;DR: 研究评估了大型语言模型（LLMs）在智能家庭IoT平台的交互威胁检测中的表现，发现虽然LLMs在理解和处理与直浇行为和条件相关的问题上表现出色，但在需要跨规则结构推理的问题上表现出色，特别是经过变异规则形式的测试，其准确性显著下降。


<details>
  <summary>Details</summary>
Motivation: 鉴于智能家庭IoT平台依赖于触发行为条件（TAC）规则来自动化设备行为，可能存在交互威胁和无意的安全风险。本文旨在评估LLMs在智能家庭场景中的适用性，以提高IoT设备安全性和可靠性。

Method: 研究团队使用多种LLM模型对原始openHAB数据集和结构挑战测试数据集进行了全面评估，通过零、一、两轮设置进行基准测试，与手动验证的基准进行比较。

Result: 实验结果显示，尽管LLMs在理解和处理与行动和条件相关的问题上表现出色，但在需要跨规则结构推理的问题上表现较差，特别是经过变异规则形式的测试，模型准确性显著下降。不同模型在不同威胁类别和提示设置下的性能差异显著。

Conclusion: 研究指出，单独使用LLMs在IoT环境中的安全关键交互威胁检测尚不可靠，建议结合符号分析和基于LLM的语义解释的混合架构，以减少误报并保持结构严谨。

Abstract: Smart home IoT platforms such as openHAB rely on Trigger Action Condition (TAC) rules to automate device behavior, but the interplay among these rules can give rise to interaction threats, unintended or unsafe behaviors emerging from implicit dependencies, conflicting triggers, or overlapping conditions. Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. We benchmark Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, and DeepSeek-R1 across zero-, one-, and two-shot settings, comparing their results against oHIT's manually validated ground truth. Our findings show that while LLMs exhibit promising semantic understanding, particularly on action- and condition-related threats, their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms. Model performance varies widely across threat categories and prompt settings, with no model providing consistent reliability. In contrast, the symbolic reasoning baseline maintains stable detection across both datasets, unaffected by rule rewrites or structural perturbations. These results underscore that LLMs alone are not yet dependable for safety critical interaction-threat detection in IoT environments. We discuss the implications for tool design and highlight the potential of hybrid architectures that combine symbolic analysis with LLM-based semantic interpretation to reduce false positives while maintaining structural rigor.

</details>


### [19] [Low Rank Comes with Low Security: Gradient Assembly Poisoning Attacks against Distributed LoRA-based LLM Systems](https://arxiv.org/abs/2601.00566)
*Yueyan Dong,Minghui Xu,Qin Hu,Yinhao Xiao,Qi Luo,Yechao Zhang,Yue Zhang,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: 文章提出了一种名为Gradient Assembly Poisoning (GAP)的新攻击方法，针对在联邦设置中使用的低秩适应（LoRA）技术的漏洞，导致模型更新受到恶意影响，同时保持表面流畅性。


<details>
  <summary>Details</summary>
Motivation: LoRA技术在联邦学习环境中被用来减少大型语言模型的微调成本，但这种方式引入了一个安全漏洞，即用户仅提交矩阵A和B，而仅其乘积AB决定模型更新，该过程未进行直接验证。

Method: GAP攻击方法通过对非恶意的单独提交矩阵A和B进行精确设计，使其乘积AB导致恶意更新。攻击者无需访问训练数据或进行客户端间协调，并且能够躲避标准异常检测器。

Result: 经过GAP攻击后，模型产生了退化或偏差的结果，同时保持了表面流畅性。细粒度对比显示，BLEU得分降低了14.5%，事实性和语法错误增加了超过800%，同时长文本长度保留在92.6%。

Conclusion: GAP攻击揭示了LoRA基于的联邦系统的隐蔽且持久的新类威胁，强调了在联邦学习设置中引入此类安全检查的必要性。

Abstract: Low-Rank Adaptation (LoRA) has become a popular solution for fine-tuning large language models (LLMs) in federated settings, dramatically reducing update costs by introducing trainable low-rank matrices. However, when integrated with frameworks like FedIT, LoRA introduces a critical vulnerability: clients submit $A$ and $B$ matrices separately, while only their product $AB$ determines the model update, yet this composite is never directly verified. We propose Gradient Assembly Poisoning (GAP), a novel attack that exploits this blind spot by crafting individually benign $A$ and $B$ matrices whose product yields malicious updates. GAP operates without access to training data or inter-client coordination and remains undetected by standard anomaly detectors. We identify four systemic vulnerabilities in LoRA-based federated systems and validate GAP across LLaMA, ChatGLM, and GPT-2. GAP consistently induces degraded or biased outputs while preserving surface fluency, reducing BLEU by up to 14.5\%, increasing factual and grammatical errors by over 800\%, and maintaining 92.6\% long-form response length. These results reveal a new class of stealthy, persistent threats in distributed LoRA fine-tuning.

</details>


### [20] [Towards Understanding and Characterizing Vulnerabilities in Intelligent Connected Vehicles through Real-World Exploits](https://arxiv.org/abs/2601.00627)
*Yuelin Wang,Yuqiao Ning,Yanbang Sun,Xiaofei Xie,Zhihua Xie,Yang Chen,Zhen Guo,Shihao Xue,Junjie Wang,Sen Chen*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Intelligent Connected Vehicles (ICVs) are a core component of modern transportation systems, and their security is crucial as it directly relates to user safety. Despite prior research, most existing studies focus only on specific sub-components of ICVs due to their inherent complexity. As a result, there is a lack of systematic understanding of ICV vulnerabilities. Moreover, much of the current literature relies on human subjective analysis, such as surveys and interviews, which tends to be high-level and unvalidated, leaving a significant gap between theoretical findings and real-world attacks. To address this issue, we conducted the first large-scale empirical study on ICV vulnerabilities. We began by analyzing existing ICV security literature and summarizing the prevailing taxonomies in terms of vulnerability locations and types. To evaluate their real-world relevance, we collected a total of 649 exploitable vulnerabilities, including 592 from eight ICV vulnerability discovery competitions, Anonymous Cup, between January 2023 and April 2024, covering 48 different vehicles. The remaining 57 vulnerabilities were submitted daily by researchers. Based on this dataset, we assessed the coverage of existing taxonomies and identified several gaps, discovering one new vulnerability location and 13 new vulnerability types. We further categorized these vulnerabilities into 6 threat types (e.g., privacy data breach) and 4 risk levels (ranging from low to critical) and analyzed participants' skills and the types of ICVs involved in the competitions. This study provides a comprehensive and data-driven analysis of ICV vulnerabilities, offering actionable insights for researchers, industry practitioners, and policymakers. To support future research, we have made our vulnerability dataset publicly available.

</details>


### [21] [Improving Router Security using BERT](https://arxiv.org/abs/2601.00783)
*John Carter,Spiros Mancoridis,Pavlos Protopapas,Brian Mitchell,Benji Lilley*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Previous work on home router security has shown that using system calls to train a transformer-based language model built on a BERT-style encoder using contrastive learning is effective in detecting several types of malware, but the performance remains limited at low false positive rates. In this work, we demonstrate that using a high-fidelity eBPF-based system call sensor, together with contrastive augmented learning (which introduces controlled mutations of negative samples), improves detection performance at a low false positive rate. In addition, we introduce a network packet abstraction language that enables the creation of a pipeline similar to network packet data, and we show that network behavior provides complementary detection signals-yielding improved performance for network-focused malware at low false positive rates. Lastly, we implement these methods in an online router anomaly detection framework to validate the approach in an Internet of Things (IoT) deployment environment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [22] [Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models](https://arxiv.org/abs/2601.00003)
*Shuqi Liu,Bowei He,Chen Ma,Linqi Song*

Main category: cs.AI

TL;DR: 该研究提出了一种增强型知识检索方法，通过两阶段的精炼过程和蒙特卡洛树搜索启发式方法，使语言模型更贴近人类对话的推理逻辑，提高了知识的多样性和应答的质量。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型在增强性能方面主要依赖于信息检索和推理能力的提升，但缺乏有效结合两者以优化模型性能的方法。本文旨在通过一种推理感知的知识检索方法，解决这一问题。

Method: 该方法采用从粗到细的知识检索策略，首先根据对话的背景语境缩小知识库的检索范围，然后在该范围内进一步提取与推理相关的知识。在整个过程中，使用了蒙特卡洛树搜索启发式的搜索方法，通过常用关键词导航知识句子。

Result: 实验结果显示，该方法不仅使检索的知识与人类对话的推理过程更为契合，还显著提升了检索知识的多样性，从而生成更有信息量和创造性的回复。

Conclusion: 本文提出的方法能够改进语言模型的知识检索机制，增强其生成高质量对话回答的能力。

Abstract: Large language models (LLMs) typically enhance their performance through either the retrieval of semantically similar information or the improvement of their reasoning capabilities. However, a significant challenge remains in effectively integrating both retrieval and reasoning strategies to optimize LLM performance. In this paper, we introduce a reasoning-aware knowledge retrieval method that enriches LLMs with information aligned to the logical structure of conversations, moving beyond surface-level semantic similarity. We follow a coarse-to-fine approach for knowledge retrieval. First, we identify a contextually relevant sub-region of the knowledge base, ensuring that all sentences within it are relevant to the context topic. Next, we refine our search within this sub-region to extract knowledge that is specifically relevant to the reasoning process. Throughout both phases, we employ the Monte Carlo Tree Search-inspired search method to effectively navigate through knowledge sentences using common keywords. Experiments on two multi-turn dialogue datasets demonstrate that our knowledge retrieval approach not only aligns more closely with the underlying reasoning in human conversations but also significantly enhances the diversity of the retrieved knowledge, resulting in more informative and creative responses.

</details>


### [23] [Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study](https://arxiv.org/abs/2601.00004)
*Isaac Iyinoluwa Olufadewa,Miracle Ayomikun Adesina,Ezekiel Ayodeji Oladejo,Uthman Babatunde Usman,Owen Kolade Adeniyi,Matthew Tolulope Olawoyin*

Main category: cs.AI

TL;DR: 本文介绍了使用大型语言模型（LLM）对尼日利亚年轻人进行抑郁筛查的新型方法，特别是针对尼日利亚Pidgin语言的定制模型，在预测严重程度和文化适宜性方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 鉴于尼日利亚抑郁症负担重大且筛查覆盖有限，现有筛查工具可能因语言和文化差异而无法有效应用于该地区。

Method: 研究人员收集了432份尼日利亚年轻人的Pidgin语言音频回答，针对与PHQ-9项目相关的心理体验进行标注，并使用Phi-3-mini-4k-instruct、Gemma-3-4B-it和GPT-4.1三种LM对这些数据进行了微调，最终评估了各个模型的性能。

Result: GPT-4.1在定量测试中以94.5%的准确率在抑郁症严重程度预测中表现最佳，且在定性评估中提供了最符合文化、清晰和上下文相关性的回答。

Conclusion: 这项研究表明，使用针对尼日利亚Pidgin语言微调的LLM可以在语言和文化多样性较高的环境中有效进行抑郁症筛查。

Abstract: Depression is a major contributor to the mental-health burden in Nigeria, yet screening coverage remains limited due to low access to clinicians, stigma, and language barriers. Traditional tools like the Patient Health Questionnaire-9 (PHQ-9) were validated in high-income countries but may be linguistically or culturally inaccessible for low- and middle-income countries and communities such as Nigeria where people communicate in Nigerian Pidgin and more than 520 local languages. This study presents a novel approach to automated depression screening using fine-tuned large language models (LLMs) adapted for conversational Nigerian Pidgin. We collected a dataset of 432 Pidgin-language audio responses from Nigerian young adults aged 18-40 to prompts assessing psychological experiences aligned with PHQ-9 items, performed transcription, rigorous preprocessing and annotation, including semantic labeling, slang and idiom interpretation, and PHQ-9 severity scoring. Three LLMs - Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1 - were fine-tuned on this annotated dataset, and their performance was evaluated quantitatively (accuracy, precision and semantic alignment) and qualitatively (clarity, relevance, and cultural appropriateness). GPT-4.1 achieved the highest quantitative performance, with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses. AI-mediated depression screening for underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments.

</details>


### [24] [Toward a Physical Theory of Intelligence](https://arxiv.org/abs/2601.00021)
*Peter David Fagan*

Main category: cs.AI

TL;DR: 本文提出了一种基于不可逆信息处理的物理智能理论，结合守恒定律，定义了智能系统并通过体系结构开发了一个全新的理论框架。该理论将智能定义为每单位不可逆处理信息产生的目标导向工作的量，并揭示了效率和适应性的自调节机制。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能领域缺乏一种统一的理论框架来描述智能的本质和运作过程，本文旨在填补这一空白，提供一个跨领域的物理智能理论。

Method: 作者通过引入保守兼容编码（CCE）框架，将信息编码与守恒定律联系起来，定义了一种新的智能度量标准，并通过理论推导和生物系统分析揭示了智能系统的工作机制。

Result: 提出了智能定义的新观点，即每单位不可逆处理信息产生的目标导向工作的量；揭示了长期效率和自调节机制之间的关系；分析了生物系统中的有效操作模式。

Conclusion: 该论文为智能提供了一个物理统一的解释，涵盖了从信息处理到生成有用工作的全过程，并提出了安全性视角下的物理 AI 方法论。

Abstract: We present a physical theory of intelligence grounded in irreversible information processing in systems constrained by conservation laws. An intelligent system is modelled as a coupled agent-environment process whose evolution transforms information into goal-directed work. To connect information to physical state, we introduce the Conservation-Congruent Encoding (CCE) framework, in which encodings correspond to metastable basins of attraction whose separability is enforced by conservation laws. Within this framework, intelligence is defined as the amount of goal-directed work produced per nat of irreversibly processed information. From this definition we derive a hierarchy of physical constraints governing information intake, irreversible computation, and work extraction in open systems. The framework reveals how long-horizon efficiency requires the preservation of internal informational structure, giving rise to self-modelling, and it establishes that physically embodied intelligent systems possess intrinsic epistemic limits analogous to incompleteness phenomena. Applying the theory to biological systems, we analyse how oscillatory and near-critical dynamics optimise the trade-off between information preservation, dissipation, and useful work, placing the brain near an efficient operating regime predicted by the framework. At the architectural level, we develop a theory of continuous dynamical circuits in which classical Boolean logic emerges as a special case of attractor selection, while more general invariant geometries support computational modes beyond fixed-point logic. Finally, we propose a physically grounded perspective on artificial intelligence safety based on irreversible information flow and structural homeostasis. Together, these results provide a unified, substrate-neutral account of intelligence as a physical phenomenon.

</details>


### [25] [A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system](https://arxiv.org/abs/2601.00023)
*Luis M. Moreno-Saavedra,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,David Casillas-Perez,Sancho Salcedo-Sanz*

Main category: cs.AI

TL;DR: 提出了一个多算法方法来优化最后一英里城市包裹递送系统中的人力资源工作负载平衡。


<details>
  <summary>Details</summary>
Motivation: 传统方法基于地理位置分配包裹，可能导致工作负载不平衡，本文提出优化分配方法以平衡所有员工的工作量。

Method: 使用多算法方法结合距离和工作量考虑来优化包裹分配。

Result: 在西班牙Azuqueca de Henares市的现实包裹递送工作中验证了所提出方法的性能。

Conclusion: 所提出的多算法方法在时间和工作量分配上表现出色，实现了更均衡的工作负载分配。

Abstract: Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.

</details>


### [26] [From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers](https://arxiv.org/abs/2601.00029)
*Abolhassan Pishahang,Maryam Badiei*

Main category: cs.AI

TL;DR: 本研究通过伊朗鸽塔案例，使用三种扩散模型测试了AI对传统建筑设计的认知，结果显示AI在几何模式上表现良好，但在材料和气候逻辑上存在误差。参考图像提高了现实感但限制了创意，而摆脱参考则产生了具有创造性的但文化上模糊的结果。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨生成型AI系统如何理解和解释存在于传统形式中的建筑智能，通过对伊朗鸽塔的案例研究来验证和分析不同扩散模型在不同时段下的表现。

Method: 研究采用了伊朗鸽塔作为案例，应用了三种基于扩散模型的不同版本即 Midjourney v6、DALL-E 3 和基于 Stable Diffusion XL (SDXL) 的 DreamStudio，针对描述性、适应性和推测性三个阶段生成图像。通过一套基于五项标准（类型、材质、环境、现实主义和文化特异性）的评估框架，评估AI 对各个模型的生成效果。

Result: 研究结果显示，AI 在几何图案方面的生成能力相当可靠，但在解读材料与气候原理时出现了偏差。使用参考图像提高图像的真实感，但限制了创造性思维的空间；脱离参考则能激发更多创新，但有时缺乏文化背景。

Conclusion: 研究为视觉相似性和建筑逻辑之间的界限提供了新的认识，并将计算传统推理作为分析AI如何感知、扭曲和重新想象传统设计智慧的框架。

Abstract: This study investigates how generative AI systems interpret the architectural intelligence embedded in vernacular form. Using the Iranian pigeon tower as a case study, the research tests three diffusion models, Midjourney v6, DALL-E 3, and DreamStudio based on Stable Diffusion XL (SDXL), across three prompt stages: referential, adaptive, and speculative. A five-criteria evaluation framework assesses how each system reconstructs typology, materiality, environment, realism, and cultural specificity. Results show that AI reliably reproduces geometric patterns but misreads material and climatic reasoning. Reference imagery improves realism yet limits creativity, while freedom from reference generates inventive but culturally ambiguous outcomes. The findings define a boundary between visual resemblance and architectural reasoning, positioning computational vernacular reasoning as a framework for analyzing how AI perceives, distorts, and reimagines traditional design intelligence.

</details>


### [27] [Mortar: Evolving Mechanics for Automatic Game Design](https://arxiv.org/abs/2601.00105)
*Muhammad U. Nasir,Yuchen Li,Steven James,Julian Togelius*

Main category: cs.AI

TL;DR: Mortar自动生成游戏机制，通过质量多样性算法与大型语言模型相结合，探索多样化的游戏规则，这些规则被用来合成完整的游戏，评估游戏机制是否有助于技能排序。


<details>
  <summary>Details</summary>
Motivation: 通过对游戏机制的自动设计，以减少设计过程中的时间和专家依赖。

Method: Mortar系统使用质量多样性算法和大型语言模型来探索多样化的游戏规则。这些规则通过合成完整的游戏来进行评估，游戏的评价标准是是否能保持玩家在技能上的排序。

Result: 生成的游戏多样化且可玩，游戏机制对保持技能排序有贡献。通过消融研究和用户研究验证了系统的各个组成部分的作用。

Conclusion: Mortar系统证明了其在自动游戏设计中的潜力，为游戏开发提供了新的可能性。

Abstract: We present Mortar, a system for autonomously evolving game mechanics for automatic game design. Game mechanics define the rules and interactions that govern gameplay, and designing them manually is a time-consuming and expert-driven process. Mortar combines a quality-diversity algorithm with a large language model to explore a diverse set of mechanics, which are evaluated by synthesising complete games that incorporate both evolved mechanics and those drawn from an archive. The mechanics are evaluated by composing complete games through a tree search procedure, where the resulting games are evaluated by their ability to preserve a skill-based ordering over players -- that is, whether stronger players consistently outperform weaker ones. We assess the mechanics based on their contribution towards the skill-based ordering score in the game. We demonstrate that Mortar produces games that appear diverse and playable, and mechanics that contribute more towards the skill-based ordering score in the game. We perform ablation studies to assess the role of each system component and a user study to evaluate the games based on human feedback.

</details>


### [28] [Constructing a Neuro-Symbolic Mathematician from First Principles](https://arxiv.org/abs/2601.00125)
*Keqin Xie*

Main category: cs.AI

TL;DR: 该研究提出了一种名为Mathesis的神经符号架构，通过将数学状态编码为高阶超图，并结合符号推理内核（SRK）将约束映射到连续能量景观，实现逻辑一致性的训练与多步推理，从而解决大语言模型在复杂推理中的一贯逻辑失败问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理复杂推理时存在固有的逻辑缺陷，因为没有内部公理框架指导。为了解决这一问题，研究提出Mathesis架构，利用神经符号技术增强模型的推理能力。

Method: Mathesis采用高阶超图编码数学状态，并通过符号推理内核（SRK）将约束与连续能量景观相对应，从而定义一个全局能量函数E(G)，该函数在零能量下表明逻辑一致性。SRK提供梯度信号用于训练高阶变压器大脑，使证明搜索转化为能量最小化。此外，通过蒙特卡洛树搜索和进化证明搜索实现多步推理，利用学习的价值函数和语义统一进行引导。

Result: 通过该模型，实现了在多步骤推理过程中由证明搜索向能量最小化转变的技术，提高了模型在处理复杂推理任务时的准确性和鲁棒性。

Conclusion: 研究证明了Mathesis架构的有效性，克服了传统大语言模型在复杂推理上的逻辑缺陷，展示了其在增强模型推理能力和解决逻辑一致性的潜力。

Abstract: Large Language Models (LLMs) exhibit persistent logical failures in complex reasoning due to the lack of an internal axiomatic framework. We propose Mathesis, a neuro-symbolic architecture that encodes mathematical states as higher-order hypergraphs and uses a Symbolic Reasoning Kernel (SRK)--a differentiable logic engine that maps constraints to a continuous energy landscape. By defining a global energy function E(G), where zero energy implies logical consistency, the SRK yields gradient-based signals to train a Hypergraph Transformer Brain, turning proof search into energy minimization. Multi-step deduction is enabled via Monte Carlo Tree Search and Evolutionary Proof Search, guided by learned value functions and semantic unification.

</details>


### [29] [Explicit Abstention Knobs for Predictable Reliability in Video Question Answering](https://arxiv.org/abs/2601.00138)
*Jorge Ortiz*

Main category: cs.AI

TL;DR: 研究发现，在视频问答任务中通过置信度阈值化实现的置信区间避选可以有效控制错误率，并且这种控制在分布迁移时依然稳健。


<details>
  <summary>Details</summary>
Motivation: 高风险场景中部署视觉-语言模型（VLMs）需要选择性预测，即当模型不确定时选择回避而不是冒险犯错。作者研究了基于置信度的避选方法在视频问答中的可靠性和普适性。

Method: 利用NExT-QA和Gemini 2.0 Flash两个数据集，作者通过对置信度阈值ε进行调整，探索置信避选策略在不同置信度阈值下的风险-覆盖率权衡。

Result: 研究结果表明，在分布内，通过改变置信度阈值可以平滑地改变风险—覆盖率平衡，从而降低错误率。同时确认，这种控制机制在分布迁移时依然稳健。

Conclusion: 基于置信度的避选策略在视频问答任务中可以有效地控制错误率，并且在数据分布发生变化的情况下依然保持其有效性。

Abstract: High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f

</details>


### [30] [FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems](https://arxiv.org/abs/2601.00227)
*Shanli Xing,Yiyan Zhai,Alexander Jiang,Yixin Dong,Yong Wu,Zihao Ye,Charlie Ruan,Yingyi Huang,Yineng Zhang,Liangsheng Yin,Aksara Bayyapu,Luis Ceze,Tianqi Chen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design. FlashInfer-Bench thus establishes a practical, reproducible pathway for continuously improving AI-generated kernels and deploying them into large-scale LLM inference.

</details>


### [31] [Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability](https://arxiv.org/abs/2601.00240)
*Zongwei Wang,Bincheng Gu,Hongyu Yu,Junliang Yu,Tao He,Jiayin Feng,Min Gao*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal "us" versus "them" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.

</details>


### [32] [ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization](https://arxiv.org/abs/2601.00290)
*Sixue Xing,Xuanye Xia,Kerui Wu,Meng Jiang,Jintai Chen,Tianfan Fu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Clinical trial failure remains a central bottleneck in drug development, where minor protocol design flaws can irreversibly compromise outcomes despite promising therapeutics. Although cutting-edge AI methods achieve strong performance in predicting trial success, they are inherently reactive for merely diagnosing risk without offering actionable remedies once failure is anticipated. To fill this gap, this paper proposes ClinicalReTrial, a self-evolving AI agent framework that addresses this gap by casting clinical trial reasoning as an iterative protocol redesign problem. Our method integrates failure diagnosis, safety-aware modification, and candidate evaluation in a closed-loop, reward-driven optimization framework. Serving the outcome prediction model as a simulation environment, ClinicalReTrial enables low-cost evaluation of protocol modifications and provides dense reward signals for continuous self-improvement. To support efficient exploration, the framework maintains hierarchical memory that captures iteration-level feedback within trials and distills transferable redesign patterns across trials. Empirically, ClinicalReTrial improves 83.3% of trial protocols with a mean success probability gain of 5.7%, and retrospective case studies demonstrate strong alignment between the discovered redesign strategies and real-world clinical trial modifications.

</details>


### [33] [Multiagent Reinforcement Learning for Liquidity Games](https://arxiv.org/abs/2601.00324)
*Alicia Vidler,Gal A. Kaminka*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Making use of swarm methods in financial market modeling of liquidity, and techniques from financial analysis in swarm analysis, holds the potential to advance both research areas. In swarm research, the use of game theory methods holds the promise of explaining observed phenomena of collective utility adherence with rational self-interested swarm participants. In financial markets, a better understanding of how independent financial agents may self-organize for the betterment and stability of the marketplace would be a boon for market design researchers. This paper unifies Liquidity Games, where trader payoffs depend on aggregate liquidity within a trade, with Rational Swarms, where decentralized agents use difference rewards to align self-interested learning with global objectives. We offer a theoretical frameworks where we define a swarm of traders whose collective objective is market liquidity provision while maintaining agent independence. Using difference rewards within a Markov team games framework, we show that individual liquidity-maximizing behaviors contribute to overall market liquidity without requiring coordination or collusion. This Financial Swarm model provides a framework for modeling rational, independent agents where they achieve both individual profitability and collective market efficiency in bilateral asset markets.

</details>


### [34] [Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems](https://arxiv.org/abs/2601.00339)
*Alaa Saleh,Praveen Kumar Donta,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Qiyang Zhang,Schahram Dustdar Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.

</details>


### [35] [Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning](https://arxiv.org/abs/2601.00400)
*Weng Ding,Yi Han,Mu-Jiang-Shan Wang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Detecting coordinated inauthentic behavior on social media remains a critical and persistent challenge, as most existing approaches rely on superficial correlation analysis, employ static parameter settings, and demand extensive and labor-intensive manual annotation. To address these limitations systematically, we propose the Adaptive Causal Coordination Detection (ACCD) framework. ACCD adopts a three-stage, progressive architecture that leverages a memory-guided adaptive mechanism to dynamically learn and retain optimal detection configurations for diverse coordination scenarios. Specifically, in the first stage, ACCD introduces an adaptive Convergent Cross Mapping (CCM) technique to deeply identify genuine causal relationships between accounts. The second stage integrates active learning with uncertainty sampling within a semi-supervised classification scheme, significantly reducing the burden of manual labeling. The third stage deploys an automated validation module driven by historical detection experience, enabling self-verification and optimization of the detection outcomes. We conduct a comprehensive evaluation using real-world datasets, including the Twitter IRA dataset, Reddit coordination traces, and several widely-adopted bot detection benchmarks. Experimental results demonstrate that ACCD achieves an F1-score of 87.3\% in coordinated attack detection, representing a 15.2\% improvement over the strongest existing baseline. Furthermore, the system reduces manual annotation requirements by 68\% and achieves a 2.8x speedup in processing through hierarchical clustering optimization. In summary, ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.

</details>


### [36] [Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications](https://arxiv.org/abs/2601.00421)
*Alessio Di Rubbo,Mattia Neri,Remo Pareschi,Marco Pedroni,Roberto Valtancoli,Paolino Zica*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper explores how semantic-space reasoning, traditionally used in computational linguistics, can be extended to tactical decision-making in team sports. Building on the analogy between texts and teams -- where players act as words and collective play conveys meaning -- the proposed methodology models tactical configurations as compositional semantic structures. Each player is represented as a multidimensional vector integrating technical, physical, and psychological attributes; team profiles are aggregated through contextual weighting into a higher-level semantic representation. Within this shared vector space, tactical templates such as high press, counterattack, or possession build-up are encoded analogously to linguistic concepts. Their alignment with team profiles is evaluated using vector-distance metrics, enabling the computation of tactical ``fit'' and opponent-exploitation potential. A Python-based prototype demonstrates how these methods can generate interpretable, dynamically adaptive strategy recommendations, accompanied by fine-grained diagnostic insights at the attribute level. Beyond football, the approach offers a generalizable framework for collective decision-making and performance optimization in team-based domains -- ranging from basketball and hockey to cooperative robotics and human-AI coordination systems. The paper concludes by outlining future directions toward real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.

</details>


### [37] [Progressive Ideation using an Agentic AI Framework for Human-AI Co-Creation](https://arxiv.org/abs/2601.00475)
*Sankar B,Srinidhi Ranjini Girish,Aadya Bharti,Dibakar Sen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The generation of truly novel and diverse ideas is important for contemporary engineering design, yet it remains a significant cognitive challenge for novice designers. Current 'single-spurt' AI systems exacerbate this challenge by producing a high volume of semantically clustered ideas. We propose MIDAS (Meta-cognitive Ideation through Distributed Agentic AI System), a novel framework that replaces the single-AI paradigm with a distributed 'team' of specialized AI agents designed to emulate the human meta-cognitive ideation workflow. This agentic system progressively refines ideas and assesses each one for both global novelty (against existing solutions) and local novelty (against previously generated ideas). MIDAS, therefore, demonstrates a viable and progressive paradigm for true human-AI co-creation, elevating the human designer from a passive filterer to a participatory, active, collaborative partner.

</details>


### [38] [The Illusion of Insight in Reasoning Models](https://arxiv.org/abs/2601.00514)
*Liv G. d'Aliberti,Manoel Horta Ribeiro*

Main category: cs.AI

TL;DR: 研究发现，推理中的突变并不频繁，且通常不会提升准确性，表明它们并非自我纠正的内在机制。通过引发高不确定性下的外部突变，可以提升准确性。


<details>
  <summary>Details</summary>
Motivation: 探讨推理模型在推理过程中是否会出现“恍然大悟”的时刻，及其对模型性能的影响。

Method: 分析超过100万条推理轨迹，从数百个训练检查点的不同推理领域入手，研究推理中的突变，并通过触发高不确定性下的外部突变来观察其效果。

Result: 推理突变的发生率低，不会因训练而增加，且在多数情况下不提高准确性。高不确定性下的外部突变能够提升准确性。

Conclusion: 推理中的突变是不稳定推理行为的症状，而非自我纠正的内在机制。

Abstract: Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.

</details>


### [39] [DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations](https://arxiv.org/abs/2601.00623)
*Longtian Qiu,Shan Ning,Chuyu Zhang,Jiaxuan Sun,Xuming He*

Main category: cs.AI

TL;DR: DA-DPO 提出了一种成本效益高的框架，通过难度估计和难度感知训练平衡学习过程，提高多模态偏好优化的效果，增强鲁棒性并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大规模多模态语言模型中的偏好优化方法容易因偏好数据不平衡而导致过拟合，无法有效地抑制细粒度幻觉。

Method: DA-DPO 通过难度估计模块利用预训练的视觉-语言模型进行难度评分，并在训练过程中根据难度重权处理偏好样本，优先处理难度高的样本，从而更加有效的进行偏好优化。

Result: 实验结果表明，DA-DPO 能够提高多模态偏好优化的效果，增强对抗幻觉的鲁棒性，同时保持计算效率。

Conclusion: DA-DPO 提出了一种新颖的多模态偏好优化方法，有效解决了过拟合问题，并在多个基准测试中展示了优越性能。

Abstract: Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.

</details>


### [40] [A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference](https://arxiv.org/abs/2601.00694)
*Qingwen Pu,Kun Xie,Hong Yang,Guocong Zhai*

Main category: cs.AI

TL;DR: PedX-LLM 提出了一种结合视觉特征和领域知识的框架，通过低秩适应微调 LLaMA-2-7B 基础模型，实现了82.0%的平衡准确率，优于现有统计和监督学习方法，并展示了在未见过的环境中的强大泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前的行人过街行为推断方法，如统计模型和监督学习方法，表现出有限的泛化能力，尤其是在新地点上性能不佳。语言模型的进步提供了从数值模式拟合到语义、上下文感知的行为推理的转变，但现有的语言模型应用缺乏领域特定适应和视觉上下文。

Method: PedX-LLM 通过结合 LLaVA 提取的视觉特征、文本数据和交通领域的知识，fine-tune LLaMA-2-7B 基础模型通过低秩适应（LoRA）来推断过街决定。一个零样本配置在五个未见过的测试站点上实现了66.9%的平衡准确率，通过少量样本学习进一步提高到72.2%。

Result: PedX-LLM 达到82.0%的平衡准确率，优于最佳统计和监督学习方法。初步验证表明，视觉增强模块增加了2.9%的性能，通过集成领域知识，额外提升了4.1%。零样本设置在五个未见过的测试站点上的准确率为66.9%，优于基线数据驱动方法至少18个百分点。通过少量样本学习进一步提升到72.2%。

Conclusion: PedX-LLM 证实了视觉和知识增强的推理可使模型模仿人类的决策逻辑，从而超越纯粹数据驱动的方法。

Abstract: Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.

</details>
