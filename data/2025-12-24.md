<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Branch-and-Price Algorithm for Fast and Equitable Last-Mile Relief Aid Distribution](https://arxiv.org/abs/2512.19882)
*Mahdi Mostajabdaveh,F. Sibel Salman,Walter J. Gutjahr*

Main category: cs.AI

TL;DR: 该研究通过混合整数规划模型和ε约束法解决灾害后将物资分配给避难所的车辆路线规划问题，提出了一种兼顾效率和公平性的双目标优化方法，并通过分支定价算法提高了求解效率。


<details>
  <summary>Details</summary>
Motivation: 在重大灾害中，预备物资往往不足以满足所有需求，因此需要优化物资分配，确保灾后救援的效率和公平性。

Method: 研究提出了一个双目标优化模型，使用ε约束法处理目标间冲突，并通过数学性质引入有效不等式，开发了分支定价算法。

Result: 分支定价算法在真实数据集上的表现优于商用混合整数规划求解器。双目标方法在满足效率的基础上减少了34%的援助分配不公。

Conclusion: 研究提出了一个有效的算法来平衡救援物流中的效率和公平性，在时间约束不同的情况下，不同的优化策略有不同的效果。

Abstract: The distribution of relief supplies to shelters is a critical aspect of post-disaster humanitarian logistics. In major disasters, prepositioned supplies often fall short of meeting all demands. We address the problem of planning vehicle routes from a distribution center to shelters while allocating limited relief supplies. To balance efficiency and equity, we formulate a bi-objective problem: minimizing a Gini-index-based measure of inequity in unsatisfied demand for fair distribution and minimizing total travel time for timely delivery. We propose a Mixed Integer Programming (MIP) model and use the $ε$-constraint method to handle the bi-objective nature. By deriving mathematical properties of the optimal solution, we introduce valid inequalities and design an algorithm for optimal delivery allocations given feasible vehicle routes. A branch-and-price (B&P) algorithm is developed to solve the problem efficiently. Computational tests on realistic datasets from a past earthquake in Van, Turkey, and predicted data for Istanbul's Kartal region show that the B&P algorithm significantly outperforms commercial MIP solvers. Our bi-objective approach reduces aid distribution inequity by 34% without compromising efficiency. Results indicate that when time constraints are very loose or tight, lexicographic optimization prioritizing demand coverage over fairness is effective. For moderately restrictive time constraints, a balanced approach is essential to avoid inequitable outcomes.

</details>


### [2] [Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification](https://arxiv.org/abs/2512.19957)
*Luciano Araujo Dourado Filho,Almir Moreira da Silva Neto,Rodrigo Pereira David,Rodrigo Tripodi Calumby*

Main category: cs.AI

TL;DR: 该研究提出了一种方法，利用训练集中的类原型指导测试集图像的分割Vision Transformer（ViT）训练，以实现多标签细粒度物种识别。该方法在PlantClef 2025挑战中获得第五名，F1分为0.33331。


<details>
  <summary>Details</summary>
Motivation: 为了解决PlantClef 2025挑战中关于高分辨率图像上的多标签物种识别问题，需要一种能够有效利用大量物种图像数据进行模型训练的方法。

Method: 该研究采用K-Means聚类方法从训练集中提取特征并创建类原型，然后使用预训练的DinoV2替换ViT的 patch embedding 层，训练一个定制的窄ViT模型，使其能够在测试数据集中重建训练集的类原型，进而通过得到的注意力分数识别和定位图像中的兴趣区域，引导分类过程。

Result: 这种方法在PlantClef 2025挑战的私人排行榜中获得了第五名的好成绩，F1分数为0.33331。相对于最高得分提交，其绝对值低0.03，表明该方法在基准任务上具有竞争力。

Conclusion: 该研究提出的方法成功实现了从多类别识别到高分辨率植被图谱上的多标签分类的领域适应，虽然在挑战中的表现不是最优，但显示了其在复杂任务中的潜力。

Abstract: This paper presents an approach developed to address the PlantClef 2025 challenge, which consists of a fine-grained multi-label species identification, over high-resolution images. Our solution focused on employing class prototypes obtained from the training dataset as a proxy guidance for training a segmentation Vision Transformer (ViT) on the test set images. To obtain these representations, the proposed method extracts features from training dataset images and create clusters, by applying K-Means, with $K$ equals to the number of classes in the dataset. The segmentation model is a customized narrow ViT, built by replacing the patch embedding layer with a frozen DinoV2, pre-trained on the training dataset for individual species classification. This model is trained to reconstruct the class prototypes of the training dataset from the test dataset images. We then use this model to obtain attention scores that enable to identify and localize areas of interest and consequently guide the classification process. The proposed approach enabled a domain-adaptation from multi-class identification with individual species, into multi-label classification from high-resolution vegetation plots. Our method achieved fifth place in the PlantCLEF 2025 challenge on the private leaderboard, with an F1 score of 0.33331. Besides that, in absolute terms our method scored 0.03 lower than the top-performing submission, suggesting that it may achieved competitive performance in the benchmark task. Our code is available at \href{https://github.com/ADAM-UEFS/PlantCLEF2025}{https://github.com/ADAM-UEFS/PlantCLEF2025}.

</details>


### [3] [FGDCC: Fine-Grained Deep Cluster Categorization -- A Framework for Intra-Class Variability Problems in Plant Classification](https://arxiv.org/abs/2512.19960)
*Luciano Araujo Dourado Filho,Rodrigo Tripodi Calumby*

Main category: cs.AI

TL;DR: 本文提出了一种新方法，通过学习类别内聚类分配来进行细粒度类别的分类，旨在减轻FGVC任务中的类别内变异性，实验表明该方法在PlantNet300k数据集上达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在细粒度视觉分类(FGVC)任务中，类别内变异性严重干扰DL模型的学习，尤其当某些类别样本不足时。因此，本文旨在通过类别内聚类学习细粒度特征，以提升分类性能。

Method: 本文方法通过在每类别单独应用聚类，发现潜在的伪标签以编码图像间的潜在相似度。这些标签随后用于层次分类过程，以学习更细粒度的视觉特征。

Result: 在PlantNet300k数据集上，尽管方法中的某些组件未完全优化，但该方法仍达到了最先进的性能。

Conclusion: 该方法为FGVC任务提供了一种新的视角，但仍需进一步研究以证明其有效性。

Abstract: Intra-class variability is given according to the significance in the degree of dissimilarity between images within a class. In that sense, depending on its intensity, intra-class variability can hinder the learning process for DL models, specially when such classes are also underrepresented, which is a very common scenario in Fine-Grained Visual Categorization (FGVC) tasks. This paper proposes a novel method that aims at leveraging classification performance in FGVC tasks by learning fine-grained features via classification of class-wise cluster assignments. Our goal is to apply clustering over each class individually, which can allow to discover pseudo-labels that encodes a latent degree of similarity between images. In turn, those labels can be employed in a hierarchical classification process that allows to learn more fine-grained visual features and thereby mitigating intra-class variability issues. Initial experiments over the PlantNet300k enabled to shed light upon several key points in which future work will have to be developed in order to find more conclusive evidence regarding the effectiveness of our method. Our method still achieves state-of-the-art performance on the PlantNet300k dataset even though some of its components haven't been shown to be fully optimized. Our code is available at \href{https://github.com/ADAM-UEFS/FGDCC}{https://github.com/ADAM-UEFS/FGDCC}.

</details>


### [4] [Discovering Lie Groups with Flow Matching](https://arxiv.org/abs/2512.20043)
*Jung Yeon Park,Yuxuan Chen,Floor Eijkelboom,Jan-Willem van de Meent,Lawson L. S. Wong,Robin Walters*

Main category: cs.AI

TL;DR: 本文提出了一种通过Lie群上的流匹配直接从数据中学得对称性的方法，称为LieFlow，能够发现更多的对称类型，并且减少假设要求。实验表明LieFlow能够在2D和3D点云中成功发现离散群，包括通过复域上的流匹配发现反射。


<details>
  <summary>Details</summary>
Motivation: 为了改善物理系统理解和机器学习中的性能与样本效率，通过从数据中学得对称性有助于更好地描述系统并提升学习算法的效果。

Method: 提出了一种新的方法来直接从数据中学习对称性，方法是通过在Lie群上进行流匹配来学习一个较大假设群上的分布，使得此处学习到的分布与数据观察到的对称性匹配。

Result: 实验结果显示，LieFlow能够在2D和3D点云中成功发现离散群，特别是通过复域上的流匹配来发现反射。

Conclusion: 本文提出的方法可以在减少假设要求下更灵活地发现对称性类型，并且在未来研究中可能对对称性在不同领域的应用提供新的见解。

Abstract: Symmetry is fundamental to understanding physical systems, and at the same time, can improve performance and sample efficiency in machine learning. Both pursuits require knowledge of the underlying symmetries in data. To address this, we propose learning symmetries directly from data via flow matching on Lie groups. We formulate symmetry discovery as learning a distribution over a larger hypothesis group, such that the learned distribution matches the symmetries observed in data. Relative to previous works, our method, \lieflow, is more flexible in terms of the types of groups it can discover and requires fewer assumptions. Experiments on 2D and 3D point clouds demonstrate the successful discovery of discrete groups, including reflections by flow matching over the complex domain. We identify a key challenge where the symmetric arrangement of the target modes causes ``last-minute convergence,'' where samples remain stationary until relatively late in the flow, and introduce a novel interpolation scheme for flow matching for symmetry discovery.

</details>


### [5] [Learning Skills from Action-Free Videos](https://arxiv.org/abs/2512.20052)
*Hung-Chieh Fang,Kuo-Han Hung,Chu-Rong Chen,Po-Jung Chou,Chun-Kai Yang,Po-Chen Ko,Yu-Chiang Wang,Yueh-Hua Wu,Min-Hung Chen,Shao-Hua Sun*

Main category: cs.AI

TL;DR: SOF通过学习基于光流的潜在技能空间，使低级动作能够从视频数据中学习高级计划和技能组成，从而改善了多任务和长时序环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的视频生成模型在生成视觉预测方面很出色，但难以转化为低级动作；而传统的潜行动作模型则受限于单步操作且缺乏高级规划能力。因此，引入SOF以弥合这两个方面的差距。

Method: SOF框架采用光流作为中介表示，学习从大量无动作视频中获取的潜在技能，从而推断出与视频动态和机器人行动对齐的运动信息。通过在这个基于光流的潜在空间中学习技能，SOF能够实现视频技能的高级规划，并使这些技能更易转换为实际动作。

Result: 实验表明，SOF在多任务和长时序任务中表现出色，能够直接从原始视觉数据中学习并组合技能。

Conclusion: SOF提出了一种适用于视频中学习高级技能的新框架，这为通用机器人提供了更丰富的时间和视觉先验，显著提升了实际应用中的效果。

Abstract: Learning from videos offers a promising path toward generalist robots by providing rich visual and temporal priors beyond what real robot datasets contain. While existing video generative models produce impressive visual predictions, they are difficult to translate into low-level actions. Conversely, latent-action models better align videos with actions, but they typically operate at the single-step level and lack high-level planning capabilities. We bridge this gap by introducing Skill Abstraction from Optical Flow (SOF), a framework that learns latent skills from large collections of action-free videos. Our key idea is to learn a latent skill space through an intermediate representation based on optical flow that captures motion information aligned with both video dynamics and robot actions. By learning skills in this flow-based latent space, SOF enables high-level planning over video-derived skills and allows for easier translation of these skills into actions. Experiments show that our approach consistently improves performance in both multitask and long-horizon settings, demonstrating the ability to acquire and compose skills directly from raw visual data.

</details>


### [6] [Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach](https://arxiv.org/abs/2512.20056)
*Hao Li,Fabian Deuser,Wenping Yin,Steffen Knoblauch,Wufan Zhao,Filip Biljecki,Yong Xue,Wei Huang*

Main category: cs.AI

TL;DR: 本文提出了一种名为ProbGLC的概率跨视角地理定位方法，结合概率和确定性地理定位模型，用于应对灾害事件并提高决策效率和资源分配。


<details>
  <summary>Details</summary>
Motivation: 随着气候变化，灾害和极端天气事件变得越来越频繁和强烈。快速和高效地响应灾害事件对于气候韧性和可持续性至关重要。现有的地理定位模型可能无法满足快速灾害响应的需求，因此本文旨在提出一种新的地理定位方法，以提高地理定位准确性和解释性。

Method: 所提出的方法ProbGLC结合了概率和确定性地理定位模型，形成一个统一框架，以提高模型的可解释性（通过不确定性量化），同时实现最先进的地理定位性能。

Result: 在两个跨视角灾害数据集（MultiIAN和SAIGENDisaster）上进行的实验表明，ProbGLC在1公里和25公里的准确度上分别达到了0.86和0.97，同时通过概率分布和局部化评分提高了模型的解释性。

Conclusion: 研究结果表明，利用生成跨视角方法可以增强位置感知能力，从而改善和加快灾害响应。

Abstract: As Earth's climate changes, it is impacting disasters and extreme weather events across the planet. Record-breaking heat waves, drenching rainfalls, extreme wildfires, and widespread flooding during hurricanes are all becoming more frequent and more intense. Rapid and efficient response to disaster events is essential for climate resilience and sustainability. A key challenge in disaster response is to accurately and quickly identify disaster locations to support decision-making and resources allocation. In this paper, we propose a Probabilistic Cross-view Geolocalization approach, called ProbGLC, exploring new pathways towards generative location awareness for rapid disaster response. Herein, we combine probabilistic and deterministic geolocalization models into a unified framework to simultaneously enhance model explainability (via uncertainty quantification) and achieve state-of-the-art geolocalization performance. Designed for rapid diaster response, the ProbGLC is able to address cross-view geolocalization across multiple disaster events as well as to offer unique features of probabilistic distribution and localizability score. To evaluate the ProbGLC, we conduct extensive experiments on two cross-view disaster datasets (i.e., MultiIAN and SAGAINDisaster), consisting diverse cross-view imagery pairs of multiple disaster types (e.g., hurricanes, wildfires, floods, to tornadoes). Preliminary results confirms the superior geolocalization accuracy (i.e., 0.86 in Acc@1km and 0.97 in Acc@25km) and model explainability (i.e., via probabilistic distributions and localizability scores) of the proposed ProbGLC approach, highlighting the great potential of leveraging generative cross-view approach to facilitate location awareness for better and faster disaster response. The data and code is publicly available at https://github.com/bobleegogogo/ProbGLC

</details>


### [7] [Scaling Reinforcement Learning for Content Moderation with Large Language Models](https://arxiv.org/abs/2512.20061)
*Hamed Firooz,Rui Liu,Yuchen Lu,Zhenyu Hou,Fangzhou Xiong,Xiaoyang Zhang,Changshu Jian,Zhicheng Zhu,Jiayuan Ma,Jacob Tao,Chaitali Gupta,Xiaochang Peng,Shike Mei,Hang Cui,Yang Qin,Shuo Tang,Jason Gaedtke,Arpit Mittal*

Main category: cs.AI

TL;DR: 本文通过实证研究探讨了扩展强化学习在内容分类中的应用，评估了多种强化学习训练方法和奖励塑造策略，证明了在资源有限和政策定义不断演化的环境中，强化学习能够显著提升复杂政策导向推理任务的性能，相较于监督微调，具有更高的数据效率。


<details>
  <summary>Details</summary>
Motivation: 文中指出，内容审核是数字生态系统中的重要挑战，传统的审核方法难以应对巨大的用户生成和AI生成内容。因此，研究者旨在探索强化学习在内容审核中的适用性，尤其是在稀疏标签、政策定义不断演化和需要深入推理的历史背景下。

Method: 研究采用实证方法，系统性地评估多种强化学习训练和技术（包括可验证奖励和以语言模型为裁判的框架）的效果，将通用语言模型转化为专门的、政策对齐的内容分类器。

Result: 研究发现，强化学习在内容审核领域的性能随训练数据、模拟卷出和优化步骤的增加而平滑提高，并在一定点后逐渐饱和。此外，强化学习在需要复杂政策导向推理的任务中表现出色，相较于监督微调，具有100倍更高的数据效率。

Conclusion: 研究最终得出结论，强化学习是处理资源有限和政策定义不断演化的场景下，实现高效内容审核的有效方法，提供了一定的实用见解，适用于大规模工业应用。

Abstract: Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly.

</details>


### [8] [Reason2Decide: Rationale-Driven Multi-Task Learning](https://arxiv.org/abs/2512.20074)
*H M Quamran Hasan,Housam Khalifa Bashier,Jiayi Dai,Mi-Young Kim,Randy Goebel*

Main category: cs.AI

TL;DR: Reason2Decide 是一种两阶段训练框架，用于提高临床上预测准确性和解释一致性。通过逐步过渡到模型预测，该框架避免了暴露偏差并在小模型中实现了卓越性能。


<details>
  <summary>Details</summary>
Motivation: 当前临床上常用的大型语言模型（LLM）存在解释与预测不符的问题，提出了Reason2Decide框架以解决这一挑战。

Method: Reason2Decide分为两个阶段：第一阶段仅训练推理生成，第二阶段联合训练标签预测与推理生成，采用调度采样逐步过渡到模型预测。

Result: Reason2Decide在三个医学数据集上表现优于其他微调基线和部分零样本 LLM，在预测准确性和解释准确性方面均表现出色。特别是在分流任务中，使用LLM生成的推理进行训练时，该框架仍能超越其他微调变体。

Conclusion: Reason2Decide框架能够在模型规模小却提供解释性决策支持方面实现显著改进，使其更适合资源受限环境下的临床推理应用。

Abstract: Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.

</details>


### [9] [Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection](https://arxiv.org/abs/2512.20140)
*Xingyou Yin,Ceyao Zhang,Min Hu,Kai Chen*

Main category: cs.AI

TL;DR: 本文提出了一种简单而有效的策略，通过在时间序列数据分词前注入噪声，增强未经过微调的语言模型在时间序列预测任务中的表现。研究还构建了两个新的时间序列数据集，进一步验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在利用大语言模型（LLMs）进行时间序列预测时通常需要微调模型以适应时间序列数据的特性，这限制了模型的通用性。作者旨在探索如何利用未经过微调的LLMs进行时间序列预测，提出了在分词前注入噪声的方法来解决这一问题。

Method: 该研究提出了一种非侵入性的方法，在分词前向时间序列数据注入噪声，以此作为推理时的数据增强手段。该方法不依赖于模型微调，而是通过改变输入数据的形式来增强模型的适应性。

Result: 实验结果显示，通过在分词前注入噪声的策略有效提高了未经过微调的LLMs在多种基准上的时间序列预测性能。此外，构建的新数据集进一步证明了该方法在去除潜在数据污染偏见方面的优越性。

Conclusion: 该研究提供了一种直接利用未经过微调的大语言模型进行时间序列预测的有效方法，展示了在此类任务中使用简单且有效的预处理策略的可能性。

Abstract: Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge this gap, a distinct, yet challenging, paradigm aims to leverage truly off-the-shelf LLMs without any fine-tuning whatsoever, relying solely on strategic tokenization of numerical sequences. The performance of these fully frozen models is acutely sensitive to the textual representation of the input data, as their parameters cannot adapt to distribution shifts. In this paper, we introduce a simple yet highly effective strategy to overcome this brittleness: injecting noise into the raw time series before tokenization. This non-invasive intervention acts as a form of inference-time augmentation, compelling the frozen LLM to extrapolate based on robust underlying temporal patterns rather than superficial numerical artifacts. We theoretically analyze this phenomenon and empirically validate its effectiveness across diverse benchmarks. Notably, to fully eliminate potential biases from data contamination during LLM pre-training, we introduce two novel TS datasets that fall outside all utilized LLMs' pre-training scopes, and consistently observe improved performance. This study provides a further step in directly leveraging off-the-shelf LLMs for time series forecasting.

</details>


### [10] [Concept Generalization in Humans and Large Language Models: Insights from the Number Game](https://arxiv.org/abs/2512.20162)
*Arghavan Bazigaran,Hansem Sohn*

Main category: cs.AI

TL;DR: 本研究通过贝叶斯模型分析了人类与大型语言模型在数字游戏中进行概念推理的方式和能力差异。


<details>
  <summary>Details</summary>
Motivation: 探讨人类与大型语言模型在概念推理任务中的表现差异，旨在理解两者在处理数学概念上的内在偏差及策略。

Method: 使用贝叶斯模型作为分析框架，研究了人类和大型语言模型的概念推理偏差和策略。

Result: 研究发现，人类比大型语言模型更能灵活地进行基于规则和相似性的概念推理，并且在少量甚至单一示例的情况下能够进行泛化。

Conclusion: 研究结果表明，人类和大型语言模型在数学概念推理和泛化方面存在根本性的差异。

Abstract: We compare human and large language model (LLM) generalization in the number game, a concept inference task. Using a Bayesian model as an analytical framework, we examined the inductive biases and inference strategies of humans and LLMs. The Bayesian model captured human behavior better than LLMs in that humans flexibly infer rule-based and similarity-based concepts, whereas LLMs rely more on mathematical rules. Humans also demonstrated a few-shot generalization, even from a single example, while LLMs required more samples to generalize. These contrasts highlight the fundamental differences in how humans and LLMs infer and generalize mathematical concepts.

</details>


### [11] [Offline Safe Policy Optimization From Heterogeneous Feedback](https://arxiv.org/abs/2512.20173)
*Ze Gong,Pradeep Varakantham,Akshat Kumar*

Main category: cs.AI

TL;DR: This paper提出了一种直接从顺序偏好和安全标签中学习安全策略的方法，这种方法避免了需要构建奖励和成本模型，从而提高了在长时持续控制任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的安全强化学习方法从离线数据学习奖励和成本模型，再使用约束性强化学习来优化一个安全策略。但在长时持续控制任务中，这种误差累积影响了性能。

Method: 该方法提供了一种名为PreSa的新框架，直接基于行为奖励的顺序偏好和轨迹部分的安全标签学习策略。通过拉格朗日范式结合偏好学习模块和安全对齐在约束优化问题中直接学习奖金额最大化并安全的策略。

Result: 该方法在一系列连续控制任务上进行了评估，展示了学习高奖励的安全策略并优于最新的基线方法和基于真实奖励和成本的离线安全RL方法。

Conclusion: 该研究提出了一个有效的学习框架，能够同时最大化奖励和确保安全性，显著改进了一些关键任务的表现。

Abstract: Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost.

</details>


### [12] [MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents](https://arxiv.org/abs/2512.20237)
*Xingbo Du,Loka Li,Duzhen Zhang,Le Song*

Main category: cs.AI

TL;DR: MemR$^3$ 是一种自主的记忆检索系统，通过引入选择检索、反思和回答机制以及证据缺口跟踪器，提升了现有语言模型的性能，尤其是在LoCoMo基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前许多记忆系统的重点在于压缩和存储，而忽略了对记忆检索的明确定义和自主控制。为此，本文提出了MemR$^3$，旨在通过自主选择和优化检索策略，提高记忆检索性能。

Method: MemR$^3$ 包含两个核心机制：一是作为中枢的路由器，根据优化答案质量选择检索、反思或回答操作；二是全局证据缺口追踪器，追踪记忆检索过程中的证据收集情况。

Result: 在LoCoMo基准测试中，MemR$^3$ 显著超越了强 baseline，在LLM-as-a-Judge评分上表现出色，特别是在使用GPT-4.1-mini后，对RAG (+7.29%) 和 Zep (+1.94%) 的现有检索器进行了提升。

Conclusion: MemR$^3$ 提供了一种插件式控制器，可以很好地与现有的记忆存储系统集成，提升了整体系统的性能。

Abstract: Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.

</details>


### [13] [ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge](https://arxiv.org/abs/2512.20276)
*Yuntao Dai,Hang Gu,Teng Wang,Qianyu Cheng,Yifei Zheng,Zhiyong Qiu,Lei Gong,Wenqi Lou,Xuehai Zhou*

Main category: cs.AI

TL;DR: ActionFlow 提出了一种针对受限边缘平台的统一推理框架，通过跨请求流水线策略和优化内存操作来提升 VLA 模型的推理速度，实现在边缘设备上实时执行动态操作。


<details>
  <summary>Details</summary>
Motivation: 当前的 VLA 模型在边缘设备上由于记忆约束的自回归解码导致推理延迟较高，不适用于需要高频控制的动态实时场景。

Method: ActionFlow 采用跨请求流水线策略与优化内存操作的方法，智能地组合解码与预填充阶段，提升硬件利用率；提出跨请求状态打包前向运算和统一的 Key-Value 环形缓冲区来减少内存操作，推进 VLA 模型在无重新训练情况下提高帧率。

Result: 实验证明 ActionFlow 在不重新训练的情况下，实现了 OpenVLA-7B 模型2.55倍的 FPS 提升，使边缘硬件上能够实时执行动态操纵任务。

Conclusion: ActionFlow 提供了一种系统级别的推理框架，使得 VLA 模型可以在边缘设备上达到所需的操作频率，解决了实际部署中的延迟问题。

Abstract: Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.

</details>


### [14] [Synthesizing Procedural Memory: Challenges and Architectures in Automated Workflow Generation](https://arxiv.org/abs/2512.20278)
*Nishant Gaurav,Adit Akarsh,Ankit Ranjan,Manoj Bajaj*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While CodeMem establishes executable code as the optimal representation for agentic procedural memory, the mechanism for autonomously synthesizing this memory from a blank slate remains underexplored. This paper operationalizes the transition of Large Language Models from passive tool-users to active workflow architects. Through a high-fidelity case study of a cross-service orchestration task involving Outlook and OneDrive, we identify and address four structural bottlenecks in automated skill generation: the Discovery Gap involving navigation of large tool registries, the Verification Gap regarding grounding tool response structures, the Decomposition Gap which replaces inefficient search with Linear State Anchoring, and the Scaling Gap focused on concurrency and persistence. We demonstrate that by enforcing a scientific methodology of hypothesize, probe, and code, agents can autonomously write robust, production-grade code skills.

</details>


### [15] [A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice](https://arxiv.org/abs/2512.20344)
*Yaowei Bai,Ruiheng Zhang,Yu Lei,Xuhua Duan,Jingfeng Yao,Shuguang Ju,Chaoyang Wang,Wei Yao,Yiwan Guo,Guilin Zhang,Chao Wan,Qian Yuan,Lei Chen,Wenjuan Tang,Biqiang Zhu,Xinggang Wang,Tao Sun,Wei Zhou,Dacheng Tao,Yongchao Xu,Chuansheng Zheng,Huangxuan Zhao,Bo Du*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT07117266). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating reliable detection of six clinically critical radiographic findings. Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by a majority of experts in 54.3% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions.

</details>


### [16] [Bohrium + SciMaster: Building the Infrastructure and Ecosystem for Agentic Science at Scale](https://arxiv.org/abs/2512.20469)
*Linfeng Zhang,Siheng Chen,Yuzhu Cai,Jingyi Chai,Junhan Chang,Kun Chen,Zhi X. Chen,Zhaohan Ding,Yuwen Du,Yuanpeng Gao,Yuan Gao,Jing Gao,Zhifeng Gao,Qiangqiang Gu,Yanhui Hong,Yuan Huang,Xi Fang,Xiaohong Ji,Guolin Ke,Zixing Lei,Xinyu Li,Yongge Li,Ruoxue Liao,Hang Lin,Xiaolu Lin,Yuxiang Liu,Xinzijian Liu,Zexi Liu,Jintan Lu,Tingjia Miao,Haohui Que,Weijie Sun,Yanfeng Wang,Bingyang Wu,Tianju Xue,Rui Ye,Jinzhe Zeng,Duo Zhang,Jiahui Zhang,Linfeng Zhang,Tianhan Zhang,Wenchang Zhang,Yuzhi Zhang,Zezhong Zhang,Hang Zheng,Hui Zhou,Tong Zhu,Xinyu Zhu,Qingguo Zhou,Weinan E*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: AI agents are emerging as a practical way to run multi-step scientific workflows that interleave reasoning with tool use and verification, pointing to a shift from isolated AI-assisted steps toward \emph{agentic science at scale}. This shift is increasingly feasible, as scientific tools and models can be invoked through stable interfaces and verified with recorded execution traces, and increasingly necessary, as AI accelerates scientific output and stresses the peer-review and publication pipeline, raising the bar for traceability and credible evaluation.
  However, scaling agentic science remains difficult: workflows are hard to observe and reproduce; many tools and laboratory systems are not agent-ready; execution is hard to trace and govern; and prototype AI Scientist systems are often bespoke, limiting reuse and systematic improvement from real workflow signals.
  We argue that scaling agentic science requires an infrastructure-and-ecosystem approach, instantiated in Bohrium+SciMaster. Bohrium acts as a managed, traceable hub for AI4S assets -- akin to a HuggingFace of AI for Science -- that turns diverse scientific data, software, compute, and laboratory systems into agent-ready capabilities. SciMaster orchestrates these capabilities into long-horizon scientific workflows, on which scientific agents can be composed and executed. Between infrastructure and orchestration, a \emph{scientific intelligence substrate} organizes reusable models, knowledge, and components into executable building blocks for workflow reasoning and action, enabling composition, auditability, and improvement through use.
  We demonstrate this stack with eleven representative master agents in real workflows, achieving orders-of-magnitude reductions in end-to-end scientific cycle time and generating execution-grounded signals from real workloads at multi-million scale.

</details>


### [17] [Benchmarking LLMs for Predictive Applications in the Intensive Care Units](https://arxiv.org/abs/2512.20520)
*Chehak Malhotra,Mehak Gopal,Akshaya Devadiga,Pradeep Singh,Ridam Pal,Ritwik Kashyap,Tavpritesh Sethi*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the advent of LLMs, various tasks across the natural language processing domain have been transformed. However, their application in predictive tasks remains less researched. This study compares large language models, including GatorTron-Base (trained on clinical data), Llama 8B, and Mistral 7B, against models like BioBERT, DocBERT, BioClinicalBERT, Word2Vec, and Doc2Vec, setting benchmarks for predicting Shock in critically ill patients. Timely prediction of shock can enable early interventions, thus improving patient outcomes. Text data from 17,294 ICU stays of patients in the MIMIC III database were scored for length of stay > 24 hours and shock index (SI) > 0.7 to yield 355 and 87 patients with normal and abnormal SI-index, respectively. Both focal and cross-entropy losses were used during finetuning to address class imbalances. Our findings indicate that while GatorTron Base achieved the highest weighted recall of 80.5%, the overall performance metrics were comparable between SLMs and LLMs. This suggests that LLMs are not inherently superior to SLMs in predicting future clinical events despite their strong performance on text-based tasks. To achieve meaningful clinical outcomes, future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping.

</details>


### [18] [Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model](https://arxiv.org/abs/2512.20548)
*Zhiyi Duan,Xiangren Wang,Hongyu Yuan,Qianli Xing*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Teachers' emotional states are critical in educational scenarios, profoundly impacting teaching efficacy, student engagement, and learning achievements. However, existing studies often fail to accurately capture teachers' emotions due to the performative nature and overlook the critical impact of instructional information on emotional expression.In this paper, we systematically investigate teacher sentiment analysis by building both the dataset and the model accordingly. We construct the first large-scale teacher multimodal sentiment analysis dataset, T-MED.To ensure labeling accuracy and efficiency, we employ a human-machine collaborative labeling process.The T-MED dataset includes 14,938 instances of teacher emotional data from 250 real classrooms across 11 subjects ranging from K-12 to higher education, integrating multimodal text, audio, video, and instructional information.Furthermore, we propose a novel asymmetric attention-based multimodal teacher sentiment analysis model, AAM-TSA.AAM-TSA introduces an asymmetric attention mechanism and hierarchical gating unit to enable differentiated cross-modal feature fusion and precise emotional classification. Experimental results demonstrate that AAM-TSA significantly outperforms existing state-of-the-art methods in terms of accuracy and interpretability on the T-MED dataset.

</details>


### [19] [Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent](https://arxiv.org/abs/2512.20586)
*Humza Nusrat,Luke Francisco,Bing Luo,Hassan Bagher-Ebadian,Joshua Kim,Karen Chin-Snyder,Salim Siddiqui,Mira Shah,Eric Mellon,Mohammad Ghassemi,Anthony Doemer,Benjamin Movsas,Kundan Thind*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.

</details>


### [20] [LongVideoAgent: Multi-Agent Reasoning with Long Videos](https://arxiv.org/abs/2512.20618)
*Runtao Liu,Ziyi Liu,Jiaqi Tang,Yue Ma,Renjie Pi,Jipeng Zhang,Qifeng Chen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.

</details>
