{"id": "2601.19955", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.19955", "abs": "https://arxiv.org/abs/2601.19955", "authors": ["Jean-Marc Fellous", "Gert Cauwenberghs", "Cornelia Ferm\u00fcller", "Yulia Sandamisrkaya", "Terrence Sejnowski"], "title": "NeuroAI and Beyond", "comment": "53 pages, 5 figures, extended appendix", "summary": "Neuroscience and Artificial Intelligence (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed Artificial Intelligence that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7efc\u8ff0\u57fa\u4e8e2025\u5e748\u6708\u4e3e\u884c\u7684\u7814\u8ba8\u4f1a\uff0c\u63a2\u8ba8\u4e86\u795e\u7ecf\u79d1\u5b66\u4e0e\u4eba\u5de5\u667a\u80fd\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6f5c\u5728\u5408\u4f5c\u9886\u57df\u3002", "motivation": "\u63a2\u8ba8\u795e\u7ecf\u79d1\u5b66\u4e0e\u4eba\u5de5\u667a\u80fd\u4e4b\u95f4\u7684\u7d27\u5bc6\u8054\u7cfb\u53ca\u5176\u672a\u6765\u53d1\u5c55\u7684\u53ef\u80fd\u6027\uff0c\u65e8\u5728\u63a8\u52a8NeuroAI\u7684\u53d1\u5c55\u3002", "method": "\u57fa\u4e8e\u7814\u8ba8\u4f1a\u548c\u4e2a\u4eba\u8bbf\u8c08\u6536\u96c6\u6570\u636e\uff0c\u5206\u6790\u795e\u7ecf\u79d1\u5b66\u548c\u4eba\u5de5\u667a\u80fd\u7684\u76f8\u5173\u8fdb\u5c55\u3002", "result": "\u8bc6\u522b\u51fa\u5f53\u524d\u548c\u672a\u6765\u5728\u795e\u7ecf\u4f53\u611f\u3001\u8bed\u8a00\u548c\u4ea4\u6d41\u3001\u673a\u5668\u4eba\u5b66\u3001\u4eba\u7c7b\u548c\u673a\u5668\u5b66\u4e60\u3001\u795e\u7ecf\u5f62\u6001\u5de5\u7a0b\u7b49\u65b9\u9762\u7684\u6f5c\u5728\u534f\u540c\u9886\u57df\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u591a\u79cd\u89c2\u70b9\u548c\u4e2a\u4eba\u58f0\u660e\u3002", "conclusion": "\u5f3a\u8c03NeuroAI\u80fd\u591f\u663e\u8457\u63d0\u9ad8AI\u7b97\u6cd5\u7684\u8303\u56f4\u548c\u6548\u7387\uff0c\u540c\u65f6\u4e5f\u63d0\u51fa\u4e86NeuroAI\u7684\u4f18\u52bf\u548c\u98ce\u9669\u3002"}}
{"id": "2601.20014", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20014", "abs": "https://arxiv.org/abs/2601.20014", "authors": ["Shuhui Qu"], "title": "Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning", "comment": null, "summary": "Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \\textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\\texttt{Sat}/\\texttt{Viol}/\\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \\emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \\textbf{14.9\\%} and \\textbf{5.8\\%} (vs.\\ \\textbf{26.0\\%} and \\textbf{15.7\\%} for the best baseline), while maintaining competitive reference quality.", "AI": {"tldr": "SQ-BCP \u662f\u4e00\u79cd\u901a\u8fc7\u663e\u5f0f\u8868\u793a\u9884\u6761\u4ef6\u72b6\u6001\u5e76\u5229\u7528\u81ea\u67e5\u8be2\u6216\u5047\u8bbe\u89e3\u51b3\u672a\u77e5\u6761\u4ef6\u7684\u53cc\u5411\u89c4\u5212\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u8d44\u6e90\u8fdd\u80cc\u7387\u3002", "motivation": "\u7814\u7a76\u73b0\u72b6\u8868\u660e\uff0c\u5e26\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u65ad\u65f6\u95f4\u89c4\u5212\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u4e0b\u5bb9\u6613\u51fa\u73b0\u9519\u8bef\uff0c\u5982\u865a\u6784\u7f3a\u5931\u7684\u4e8b\u5b9e\u6216\u8fdd\u80cc\u786c\u7ea6\u675f\u3002SQ-BCP \u662f\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u800c\u63d0\u51fa\u7684\u3002", "method": "SQ-BCP \u65b9\u6cd5\u5305\u62ec\u53cc\u5411\u641c\u7d22\u548c\u4f7f\u7528\u63a8\u5bfc\u4e3a\u57fa\u7840\u7684\u9a8c\u8bc1\u5668\u9a8c\u8bc1\u76ee\u6807\u51c6\u786e\u6027\uff0c\u4ec5\u901a\u8fc7\u8ddd\u79bb\u5f97\u5206\u8fdb\u884c\u6392\u540d\u548c\u4fee\u526a\u3002\u5b83\u901a\u8fc7\u81ea\u67e5\u8be2\u548c\u5047\u8bbe\u586b\u8865\u7f3a\u5931\u6761\u4ef6\u3002", "result": "\u5728 WikiHow \u548c RecipeNLG \u4efb\u52a1\u4e2d\uff0cSQ-BCP \u7684\u8d44\u6e90\u8fdd\u80cc\u7387\u663e\u8457\u964d\u4f4e\u5230 14.9% \u548c 5.8%\uff08\u76f8\u6bd4\u800c\u8a00\uff0c\u6700\u597d\u57fa\u7ebf\u4e3a 26.0% \u548c 15.7%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u7684\u5f15\u7528\u8d28\u91cf\u3002", "conclusion": "\u901a\u8fc7 SQ-BCP\uff0c\u4f5c\u8005\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u89c4\u5212\u7ea6\u675f\u7b26\u5408\u76ee\u6807\u8981\u6c42\u4e14\u80fd\u5728\u6709\u9650\u5206\u652f\u548c\u6df1\u5ea6\u4e0b\u627e\u5230\u63a5\u53d7\u7684\u8ba1\u5212\u3002"}}
{"id": "2601.20021", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20021", "abs": "https://arxiv.org/abs/2601.20021", "authors": ["Shuhui Qu"], "title": "Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints", "comment": null, "summary": "Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u7cca\u8303\u7574\u8bba\u89c4\u5212\u65b9\u6cd5\uff08FCP\uff09\uff0c\u901a\u8fc7LLM\u548c\u6b8b\u5dee\u57fa\u4e8e\u7684\u540e\u5411\u9700\u6c42\u652f\u6301\u5177\u6709\u8bed\u4e49\u7684\u6a21\u7cca\u9002\u7528\u6027\u6ce8\u91ca\u53ca\u8d28\u91cf\u95ee\u9898\u7684Lukasiewicz t-\u8303\u5fb7\u7ec4\u5408\uff0c\u4ece\u800c\u5728\u4e0d\u727a\u7272\u53ef\u6267\u884c\u6027\u9a8c\u8bc1\u7684\u540c\u65f6\uff0c\u63d0\u5347\u590d\u6742\u8ba1\u5212\u7684\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u5904\u7406\u7f3a\u4e4f\u66ff\u6362\u6210\u5206\u98df\u8c31\u89c4\u5212\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u8303\u7574\u8bba\u89c4\u5212\u65b9\u6cd5\u5728\u5904\u7406\u6a21\u7cca\u8c13\u8bcd\u65f6\u5b58\u5728\u786c\u6027\u9608\u503c\u8bbe\u5b9a\uff0c\u65e0\u6cd5\u7cbe\u786e\u53cd\u6620\u6267\u884c\u8d28\u91cf\u548c\u8fc7\u7a0b\u4e2d\u7684\u964d\u7ea7\u60c5\u51b5\uff0c\u800c\u9488\u5bf9\u6a21\u7cca\u6027\u9700\u6c42\u7684LLM\u65b9\u6cd5\u5219\u6709\u52a9\u4e8e\u4fdd\u7559\u91cf\u5316\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u63d0\u51faFCP\u65b9\u6cd5\u7ed3\u5408Lukasiewicz t-\u8303\u5fb7\u903b\u8f91\u548cLLM\u6a21\u7cca\u9002\u7528\u6027\u6ce8\u91ca\u4ee5\u589e\u5f3a\u89c4\u5212\u8fc7\u7a0b\u7684\u6709\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "FCP\u65b9\u6cd5\u901a\u8fc7\u7ed9\u6bcf\u4e2a\u52a8\u4f5c\uff08morphism\uff09\u6807\u6ce8\u4e00\u4e2a[0,1]\u4e4b\u95f4\u7684\u5ea6\u91cf\uff0c\u5e76\u5229\u7528Lukasiewicz t-\u8303\u5fb7\u903b\u8f91\u7ec4\u5408\u8ba1\u5212\u8d28\u91cf\uff0c\u4fdd\u6301\u51ed\u5355\u9a8c\u8bc1\u7684\u6e05\u6670\u6027\u3002\u540c\u65f6\u5229\u7528LLM\u7684k\u6837\u672c\u4e2d\u4f4d\u6570\u805a\u5408\u652f\u6301\u6a21\u7cca\u9002\u7528\u6027\u6ce8\u91ca\uff0c\u5e76\u8fd0\u7528\u6b8b\u5dee\u57fa\u4e8e\u7684\u540e\u5411\u9700\u6c42\u8fdb\u884c\u4e2d\u95f4\u76f8\u9047\u641c\u7d22\u3002", "result": "\u5728\u516c\u5171PDDL3\u504f\u597d/\u8d85\u8ba2\u8d2d\u57fa\u51c6\u548c\u57fa\u4e8eRecipeNLG\u7684RecipeNLG-Subs\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFCP\u76f8\u6bd4\u4ec5\u57fa\u4e8eLLM\u548cReAct\u98ce\u683c\u7684\u57fa\u7ebf\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u5e76\u964d\u4f4e\u4e86\u786c\u6027\u7ea6\u675f\u8fdd\u53cd\u60c5\u51b5\uff0c\u5c24\u5176\u5728RecipeNLG-Subs\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u66f4\u4e3a\u51fa\u8272\u3002", "conclusion": "FCP\u65b9\u6cd5\u901a\u8fc7\u4fdd\u7559\u91cf\u5316\u540e\u7684\u6a21\u7cca\u9002\u7528\u6027\u6ce8\u91ca\u5e76\u7ed3\u5408\u51ed\u5355\u9a8c\u8bc1\u7684\u529b\u91cf\uff0c\u5728\u4fdd\u6301\u7075\u6d3b\u591a\u53d8\u7684\u89c4\u5212\u7b56\u7565\u540c\u65f6\u63d0\u9ad8\u4e86\u6574\u4f53\u89c4\u5212\u7684\u8d28\u91cf\u548c\u6267\u884c\u6548\u679c\u3002"}}
{"id": "2601.19970", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19970", "abs": "https://arxiv.org/abs/2601.19970", "authors": ["Nourin Shahin", "Izzat Alsmadi"], "title": "Benchmarking LLAMA Model Security Against OWASP Top 10 For LLM Applications", "comment": null, "summary": "As large language models (LLMs) move from research prototypes to enterprise systems, their security vulnerabilities pose serious risks to data privacy and system integrity. This study benchmarks various Llama model variants against the OWASP Top 10 for LLM Applications framework, evaluating threat detection accuracy, response safety, and computational overhead. Using the FABRIC testbed with NVIDIA A30 GPUs, we tested five standard Llama models and five Llama Guard variants on 100 adversarial prompts covering ten vulnerability categories. Our results reveal significant differences in security performance: the compact Llama-Guard-3-1B model achieved the highest detection rate of 76% with minimal latency (0.165s per test), whereas base models such as Llama-3.1-8B failed to detect threats (0% accuracy) despite longer inference times (0.754s). We observe an inverse relationship between model size and security effectiveness, suggesting that smaller, specialized models often outperform larger general-purpose ones in security tasks. Additionally, we provide an open-source benchmark dataset including adversarial prompts, threat labels, and attack metadata to support reproducible research in AI security, [1].", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u6bd4\u4e86\u5404\u79cdLlama\u6a21\u578b\u53d8\u4f53\u9488\u5bf9OWASP Top 10 LLM\u5e94\u7528\u6846\u67b6\u7684\u5a01\u80c1\u68c0\u6d4b\u51c6\u786e\u5ea6\u3001\u54cd\u5e94\u5b89\u5168\u6027\u53ca\u8ba1\u7b97\u5f00\u9500\uff0c\u53d1\u73b0\u5c0f\u578b\u4e13\u4e3a\u5b89\u5168\u8bbe\u8ba1\u7684Llama-Guard-3-1B\u6a21\u578b\u5728\u68c0\u6d4b\u7387\u548c\u4f4e\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "LLMs\u4ece\u7814\u7a76\u539f\u578b\u8f6c\u53d8\u4e3a\u4f01\u4e1a\u7cfb\u7edf\u65f6\uff0c\u5176\u5b89\u5168\u6f0f\u6d1e\u5bf9\u6570\u636e\u9690\u79c1\u548c\u7cfb\u7edf\u5b8c\u6574\u6027\u6784\u6210\u4e86\u4e25\u91cd\u5a01\u80c1\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u8bc4\u4f30\u548c\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "method": "\u7814\u7a76\u4f7f\u7528FABRIC\u6d4b\u8bd5\u5e8a\u548cNVIDIA A30 GPU\uff0c\u6d4b\u8bd5\u4e86\u4e94\u4e2a\u6807\u51c6Llama\u6a21\u578b\u548c\u4e94\u4e2aLlama Guard\u53d8\u4f53\uff0c\u8bc4\u4f30\u4e86100\u4e2a\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u8986\u76d6\u4e86\u5341\u7c7b\u6f0f\u6d1e\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5c0f\u578b\u7684Llama-Guard-3-1B\u6a21\u578b\u5728\u68c0\u6d4b\u7387\u548c\u4f4e\u5ef6\u8fdf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u68c0\u6d4b\u7387\u4e3a76%\uff0c\u5ef6\u8fdf\u4e3a0.165\u79d2\u3002\u800c\u57fa\u7840\u6a21\u578b\u5982Llama-3.1-8B\u5219\u672a\u80fd\u68c0\u6d4b\u5230\u5a01\u80c1\uff0c\u68c0\u6d4b\u7387\u4e3a0%\uff0c\u4f46\u8017\u65f6\u8f83\u957f\uff0c\u4e3a0.754\u79d2\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u5927\u5c0f\u4e0e\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u53cd\u6bd4\u5173\u7cfb\uff0c\u5c0f\u578b\u3001\u4e13\u4e3a\u5b89\u5168\u8bbe\u8ba1\u7684\u6a21\u578b\u5f80\u5f80\u4f18\u4e8e\u5927\u578b\u901a\u7528\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b\u5bf9\u6297\u6027\u63d0\u793a\u3001\u5a01\u80c1\u6807\u7b7e\u548c\u653b\u51fb\u5143\u6570\u636e\u7684\u5f00\u6e90\u57fa\u51c6\u6570\u636e\u96c6\u4ee5\u652f\u6301AI\u5b89\u5168\u7684\u53ef\u91cd\u590d\u7814\u7a76\u3002"}}
{"id": "2601.20163", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.20163", "abs": "https://arxiv.org/abs/2601.20163", "authors": ["Mahsa Tahghigh", "Hassan Salmani"], "title": "Reference-Free Spectral Analysis of EM Side-Channels for Always-on Hardware Trojan Detection", "comment": "Accepted at GOMACTech 2026", "summary": "Always-on hardware Trojans (HTs) pose a critical risk to trusted microelectronics, yet most side-channel detection methods rely on unavailable golden references. We present a reference-free approach that combines time-frequency EM analysis with Gaussian Mixture Models (GMMs). By applying Short-Time Fourier Transform (STFT) at multiple window sizes, we show that HT-free circuits exhibit fluctuating statistical structure, while always-on HTs leave persistent footprints with fewer, more consistent mixture components. Results on AES-128 demonstrate feasibility without requiring reference models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91d1\u53c2\u8003\u7684(always-on)\u786c\u4ef6\u6076\u610f Trojans (HTs) \u65f6\u9891\u7535\u78c1 (EM) \u5206\u6790\u65b9\u6cd5\uff0c\u4f7f\u7528 Gaussian Mixture Models (GMMs) \u8fdb\u884c\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u4fa7\u4fe1\u9053\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u65e0\u6cd5\u83b7\u53d6\u7684\u91d1\u53c2\u8003\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u91d1\u53c2\u8003\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b always-on HTs\uff0c\u4ee5\u589e\u5f3a\u53ef\u4fe1\u5fae\u7535\u5b50\u7684\u5b89\u5168\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u65f6\u95f4-\u9891\u8c31 EM \u5206\u6790\u4e0e Gaussian Mixture Models (GMMs)\uff0c\u901a\u8fc7\u5728\u591a\u4e2a\u7a97\u53e3\u5927\u5c0f\u4e0a\u5e94\u7528 Short-Time Fourier Transform (STFT) \u5bf9 always-on HT \u548c HT-free \u7535\u8def\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5728\u6ca1\u6709\u53c2\u8003\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9 AES-128 \u7684 always-on HTs \u8fdb\u884c\u6709\u6548\u68c0\u6d4b\u3002", "conclusion": "\u65e0\u9700\u91d1\u53c2\u8003\u7684 always-on HT \u68c0\u6d4b\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\uff0c\u4e3a\u786c\u4ef6\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.20221", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20221", "abs": "https://arxiv.org/abs/2601.20221", "authors": ["Hang Zhang", "Ruheng Wang", "Yuelyu Ji", "Mingu Kwak", "Xizhi Wu", "Chenyu Li", "Li Zhang", "Wenqi Shi", "Yifan Peng", "Yanshan Wang"], "title": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning", "comment": null, "summary": "Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\\method$ demonstrates an $\\mathbf{8\\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a$\textbf{method}$\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u67e5\u8be2\u5916\u90e8\u533b\u5b66\u8d44\u6e90\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728MedQA\u548cMedXpertQA\u57fa\u51c6\u4e0a\u7684\u51c6\u786e\u6027\u6709\u4e86\u5927\u5e45\u63d0\u9ad8\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6837\u672c\u9884\u7b97\u9700\u6c42\u3002", "motivation": "\u5728\u4e34\u5e8a\u533b\u7597\u80cc\u666f\u4e0b\uff0c\u4f7f\u7528\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u533b\u7597\u63a8\u7406\u65f6\uff0c\u9700\u8981\u786e\u4fdd\u5176\u7ed3\u679c\u7684\u51c6\u786e\u6027\u3002\u73b0\u6709\u7684\u5956\u52b1\u6a21\u578b\u65b9\u6cd5\u867d\u7136\u4fbf\u6377\u4f46\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u9a8c\u8bc1\u6846\u67b6\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u79cd\u8fed\u4ee3\u8bad\u7ec3\u7684 reinforcement learning \u65b9\u6cd5\uff0c\u7ed3\u5408\u5de5\u5177\u8f85\u52a9\u7684\u9a8c\u8bc1\u673a\u5236\u3002\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u6570\u636e\u5206\u5e03\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u968f\u9a8c\u8bc1\u8fc7\u7a0b\u8fdb\u884c\u9010\u6b65\u67e5\u8be2\u5916\u90e8\u533b\u5b66\u8d44\u6599\u4ee5\u6536\u96c6\u8bc1\u636e\u3002\u8fd9\u79cd\u7b56\u7565\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u590d\u6742\u6807\u8bb0\u6570\u636e\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u66f4\u6709\u6548\u7684\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c$\textbf{method}$\u5728\u56db\u4e2a\u533b\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728 MedQA \u548c MedXpertQA \u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u6027\u5206\u522b\u63d0\u9ad8\u4e8623.5% \u548c 32.0%\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u5927\u5927\u51cf\u5c11\u4e86\u6240\u9700\u6837\u672c\u7684\u6570\u91cf\uff0c\u6bd4\u524d\u4eba\u7684\u5956\u52b1\u6a21\u578b\u57fa\u7840\u7ebf\u51cf\u5c11\u4e868\u500d\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5c06\u9a8c\u8bc1\u8fc7\u7a0b\u4e0e\u52a8\u6001\u68c0\u7d22\u5230\u7684\u8bc1\u636e\u7ed3\u5408\u4f7f\u7528\uff0c\u662f\u6784\u5efa\u66f4\u53ef\u9760\u533b\u7597\u63a8\u7406\u7cfb\u7edf\u7684\u4e00\u4e2a\u6709\u539f\u5219\u7684\u65b9\u6cd5\u3002"}}
{"id": "2601.20305", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20305", "abs": "https://arxiv.org/abs/2601.20305", "authors": ["Zhenchen Tang", "Songlin Yang", "Zichuan Wang", "Bo Peng", "Yang Li", "Beibei Dong", "Jing Dong"], "title": "Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models", "comment": null, "summary": "Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model's understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model's latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEndogenous Reprompting\u7684\u673a\u5236\uff0c\u901a\u8fc7SEER\u6846\u67b6\uff0c\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u751f\u6210\u4e0e\u6587\u672c\u5185\u5bb9\u4e00\u81f4\u7684\u63cf\u8ff0\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u867d\u7136\u5177\u6709\u8f83\u5f3a\u7684\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u5728\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u51fa\u4e00\u79cd\u673a\u5236\u6765\u89e3\u51b3\u8fd9\u4e00\u8ba4\u77e5\u5dee\u8ddd\u3002", "method": "\u63d0\u51faEndogenous Reprompting\u673a\u5236\u53caSEER\u6846\u67b6\uff0c\u5229\u7528Reinforcement Learning with Verifiable Rewards (RLVR)\u548cReinforcement Learning with Model-rewarded Thinking (RLMT)\u5bf9\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u5176\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u80fd\u591f\u81ea\u52a8\u751f\u6210\u4e0e\u5176\u751f\u6210\u5185\u5bb9\u76f8\u5339\u914d\u7684\u63cf\u8ff0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSEER\u6846\u67b6\u5728\u8bc4\u4f30\u51c6\u786e\u5ea6\u3001\u590d\u8be2\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u7b49\u65b9\u9762\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u800c\u4e0d\u4f1a\u727a\u7272\u6a21\u578b\u7684\u591a\u6a21\u6001\u901a\u7528\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u589e\u5f3a\u6a21\u578b\u7684\u751f\u6210\u8868\u73b0\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u8bc1\u5b9e\u4e86Endogenous Reprompting\u673a\u5236\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.20270", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20270", "abs": "https://arxiv.org/abs/2601.20270", "authors": ["Holly Trikilis", "Pasindu Marasinghe", "Fariza Rashid", "Suranga Seneviratne"], "title": "Eliciting Least-to-Most Reasoning for Phishing URL Detection", "comment": null, "summary": "Phishing continues to be one of the most prevalent attack vectors, making accurate classification of phishing URLs essential. Recently, large language models (LLMs) have demonstrated promising results in phishing URL detection. However, their reasoning capabilities that enabled such performance remain underexplored. To this end, in this paper, we propose a Least-to-Most prompting framework for phishing URL detection. In particular, we introduce an \"answer sensitivity\" mechanism that guides Least-to-Most's iterative approach to enhance reasoning and yield higher prediction accuracy. We evaluate our framework using three URL datasets and four state-of-the-art LLMs, comparing against a one-shot approach and a supervised model. We demonstrate that our framework outperforms the one-shot baseline while achieving performance comparable to that of the supervised model, despite requiring significantly less training data. Furthermore, our in-depth analysis highlights how the iterative reasoning enabled by Least-to-Most, and reinforced by our answer sensitivity mechanism, drives these performance gains. Overall, we show that this simple yet powerful prompting strategy consistently outperforms both one-shot and supervised approaches, despite requiring minimal training or few-shot guidance. Our experimental setup can be found in our Github repository github.sydney.edu.au/htri0928/least-to-most-phishing-detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5c11\u5230\u6700\u591a\u7684\u63d0\u793a\u6846\u67b6\u8fdb\u884c\u9493\u9c7cURL\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u7b54\u6848\u654f\u611f\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u63a8\u7406\u80fd\u529b\u5e76\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u7531\u4e8e\u9493\u9c7c\u653b\u51fb\u7684\u6301\u7eed\u5b58\u5728\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9493\u9c7cURL\u68c0\u6d4b\u4e2d\u7684\u4f18\u79c0\u8868\u73b0\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6700\u5c11\u5230\u6700\u591a\u7684\u63d0\u793a\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86\u7b54\u6848\u654f\u611f\u673a\u5236\uff0c\u7528\u4e8e\u6307\u5bfc\u6846\u67b6\u7684\u65b9\u6cd5\u3002", "result": "\u8be5\u6846\u67b6\u5728\u4e09\u9879URL\u6570\u636e\u96c6\u548c\u56db\u5927\u6700\u5148\u8fdb\u7684LLM\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8868\u73b0\u4f18\u4e8e\u4e00\u6b21\u53cd\u9988\u65b9\u6cd5\uff0c\u5e76\u63a5\u8fd1\u76d1\u7763\u6a21\u578b\u7684\u6548\u679c\uff0c\u540c\u65f6\u9700\u8981\u8f83\u5c11\u7684\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u8f83\u5c11\u7684\u8bad\u7ec3\u6570\u636e\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u4e00\u6b21\u53cd\u9988\u548c\u76d1\u7763\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u63d0\u793a\u7b56\u7565\u3002"}}
{"id": "2601.20310", "categories": ["cs.CR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20310", "abs": "https://arxiv.org/abs/2601.20310", "authors": ["Xin Zhang", "Zijin Yang", "Kejiang Chen", "Linfeng Ma", "Weiming Zhang", "Nenghai Yu"], "title": "SemBind: Binding Diffusion Watermarks to Semantics Against Black-Box Forgery Attacks", "comment": null, "summary": "Latent-based watermarks, integrated into the generation process of latent diffusion models (LDMs), simplify detection and attribution of generated images. However, recent black-box forgery attacks, where an attacker needs at least one watermarked image and black-box access to the provider's model, can embed the provider's watermark into images not produced by the provider, posing outsized risk to provenance and trust. We propose SemBind, the first defense framework for latent-based watermarks that resists black-box forgery by binding latent signals to image semantics via a learned semantic masker. Trained with contrastive learning, the masker yields near-invariant codes for the same prompt and near-orthogonal codes across prompts; these codes are reshaped and permuted to modulate the target latent before any standard latent-based watermark. SemBind is generally compatible with existing latent-based watermarking schemes and keeps image quality essentially unchanged, while a simple mask-ratio parameter offers a tunable trade-off between anti-forgery strength and robustness. Across four mainstream latent-based watermark methods, our SemBind-enabled anti-forgery variants markedly reduce false acceptance under black-box forgery while providing a controllable robustness-security balance.", "AI": {"tldr": "\u63d0\u51fa\u4e86SemBind\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u9488\u5bf9\u57fa\u4e8e\u6f5c\u5728\u7a7a\u95f4\u7684\u6c34\u5370\u7684\u65b0\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u8bed\u4e49\u9762\u7f69\u5c06\u6f5c\u5728\u4fe1\u53f7\u7ed1\u5b9a\u5230\u56fe\u50cf\u8bed\u4e49\uff0c\u4ece\u800c\u62b5\u6297\u9ed1\u76d2\u4f2a\u9020\u653b\u51fb\u3002\u8be5\u65b9\u6cd5\u4fdd\u6301\u4e86\u56fe\u50cf\u8d28\u91cf\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u53ef\u8c03\u8282\u7684\u6297\u4f2a\u9020\u5f3a\u5ea6\u548c\u7a33\u5065\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "motivation": "\u5bf9\u6297\u9ed1\u76d2\u4f2a\u9020\u653b\u51fb\u5bf9\u751f\u6210\u6a21\u578b\u7684\u6c34\u5370\u68c0\u6d4b\u4e0e\u6eaf\u6e90\u9020\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u9632\u5fa1\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u7684\u9632\u5fa1\u7b56\u7565\u3002", "method": "SemBind\u6846\u67b6\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u8bed\u4e49\u9762\u7f69\uff0c\u4ea7\u751f\u5bf9\u63d0\u793a\u4e0d\u53d8\u4f46\u8de8\u63d0\u793a\u63a5\u8fd1\u6b63\u4ea4\u7684\u4ee3\u7801\uff0c\u8fd9\u4e9b\u4ee3\u7801\u7528\u4e8e\u8c03\u8282\u76ee\u6807\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u6c34\u5370\u3002", "result": "SemBind\u5728\u56db\u79cd\u4e3b\u6d41\u7684\u57fa\u4e8e\u6f5c\u5728\u7a7a\u95f4\u7684\u6c34\u5370\u65b9\u6cd5\u4e0a\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9ed1\u76d2\u4f2a\u9020\u653b\u51fb\u4e0b\u7684\u8bef\u63a5\u53d7\u7387\uff0c\u63d0\u4f9b\u4e86\u53ef\u63a7\u7684\u7a33\u5065\u6027-\u5b89\u5168\u6027\u5e73\u8861\u3002", "conclusion": "SemBind\u4e3a\u57fa\u4e8e\u6f5c\u5728\u7a7a\u95f4\u7684\u6c34\u5370\u63d0\u4f9b\u4e86\u65b0\u7684\u9632\u5fa1\u673a\u5236\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u5bf9\u9ed1\u76d2\u4f2a\u9020\u653b\u51fb\u7684\u62b5\u6297\u529b\u3002"}}
{"id": "2601.20352", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20352", "abs": "https://arxiv.org/abs/2601.20352", "authors": ["Weiquan Huang", "Zixuan Wang", "Hehai Lin", "Sudong Wang", "Bo Xu", "Qian Li", "Beier Zhu", "Linyi Yang", "Chengwei Qin"], "title": "AMA: Adaptive Memory via Multi-Agent Collaboration", "comment": "8 pages", "summary": "The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6AMA\uff0c\u5229\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7ba1\u7406\u8de8\u591a\u79cd\u7c92\u5ea6\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u5339\u914d\u4e0d\u4e00\u81f4\u548c\u79ef\u7d2f\u903b\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u8bb0\u5fc6\u7cfb\u7edf\u5b58\u5728\u8bbe\u8ba1\u4e0d\u8db3\uff0c\u5bfc\u81f4\u8bb0\u5fc6\u5b58\u50a8\u7684\u4fe1\u606f\u4e0e\u4efb\u52a1\u7279\u5b9a\u63a8\u7406\u9700\u6c42\u4e4b\u95f4\u6301\u7eed\u4e0d\u5339\u914d\uff0c\u79ef\u7d2f\u4e86\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\u3002", "method": "AMA\u6846\u67b6\u91c7\u7528\u5206\u5c42\u8bb0\u5fc6\u8bbe\u8ba1\uff0c\u52a8\u6001\u8c03\u6574\u68c0\u7d22\u7c92\u5ea6\u4ee5\u5339\u914d\u4efb\u52a1\u590d\u6742\u6027\u3002\u901a\u8fc7\u534f\u4f5c\u7684\u6784\u9020\u8005\u548c\u68c0\u7d22\u8005\u5b9e\u73b0\u591a\u7c92\u5ea6\u8bb0\u5fc6\u6784\u5efa\u548c\u81ea\u9002\u5e94\u67e5\u8be2\u8def\u7531\uff0c\u6cd5\u5b98\u9a8c\u8bc1\u68c0\u7d22\u5185\u5bb9\u7684\u76f8\u5173\u6027\u548c\u4e00\u81f4\u6027\uff0c\u5fc5\u8981\u65f6\u89e6\u53d1\u8fed\u4ee3\u68c0\u7d22\u6216\u7531\u5237\u65b0\u8005\u8fdb\u884c\u9488\u5bf9\u6027\u66f4\u65b0\u6216\u5220\u9664\u8fc7\u65f6\u6761\u76ee\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAMA\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u5728\u51cf\u5c11\u5927\u7ea680%\u7684token\u6d88\u8d39\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u68c0\u7d22\u7cbe\u5ea6\u548c\u957f\u671f\u8bb0\u5fc6\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "AMA\u6846\u67b6\u5728\u63d0\u9ad8\u957f\u671f\u4ea4\u4e92\u548c\u590d\u6742\u63a8\u7406\u7684\u652f\u6301\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u6709\u6548\u6027\uff0c\u662f\u5904\u7406\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bb0\u5fc6\u7cfb\u7edf\u7684\u4e00\u9879\u521b\u65b0\u8d21\u732e\u3002"}}
{"id": "2601.20325", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.20325", "abs": "https://arxiv.org/abs/2601.20325", "authors": ["Lulu Xue", "Shengshan Hu", "Wei Lu", "Ziqi Zhou", "Yufei Song", "Jianhong Cheng", "Minghui Li", "Yanjun Zhang", "Leo Yu Zhang"], "title": "UnlearnShield: Shielding Forgotten Privacy against Unlearning Inversion", "comment": "This work has been accepted by ICASSP 2026", "summary": "Machine unlearning is an emerging technique that aims to remove the influence of specific data from trained models, thereby enhancing privacy protection. However, recent research has uncovered critical privacy vulnerabilities, showing that adversaries can exploit unlearning inversion to reconstruct data that was intended to be erased. Despite the severity of this threat, dedicated defenses remain lacking. To address this gap, we propose UnlearnShield, the first defense specifically tailored to counter unlearning inversion. UnlearnShield introduces directional perturbations in the cosine representation space and regulates them through a constraint module to jointly preserve model accuracy and forgetting efficacy, thereby reducing inversion risk while maintaining utility. Experiments demonstrate that it achieves a good trade-off among privacy protection, accuracy, and forgetting.", "AI": {"tldr": "UnlearnShield \u662f\u4e00\u79cd\u9488\u5bf9\u53cd\u5411\u5220\u9664\u653b\u51fb\u7684\u9632\u5907\u63aa\u65bd\uff0c\u901a\u8fc7\u65b9\u5411\u6027\u6270\u52a8\u5728\u4f59\u5f26\u8868\u793a\u7a7a\u95f4\u5e76\u4f7f\u7528\u7ea6\u675f\u6a21\u5757\u6765\u8c03\u63a7\uff0c\u6765\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u548c\u9057\u5fd8\u6548\u679c\uff0c\u4ece\u800c\u964d\u4f4e\u53cd\u5411\u5220\u9664\u7684\u98ce\u9669\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u9632\u9690\u79c1\u7b56\u7565\u5b58\u5728\u6f0f\u6d1e\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5220\u9664\u7279\u5b9a\u6570\u636e\u540e\uff0c\u653b\u51fb\u8005\u4ecd\u80fd\u6062\u590d\u8fd9\u4e9b\u6570\u636e\u3002\u7814\u7a76\u8005\u8ba4\u4e3a\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u9632\u5fa1\u63aa\u65bd\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "UnlearnShield \u91c7\u7528\u65b9\u5411\u6027\u6270\u52a8\u5728\u4f59\u5f26\u8868\u793a\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u7ea6\u675f\u6a21\u5757\u6765\u8c03\u6574\u8fd9\u79cd\u6270\u52a8\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u548c\u9057\u5fd8\u65e7\u6570\u636e\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUnlearnShield \u80fd\u591f\u5728\u4fdd\u62a4\u9690\u79c1\u3001\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u548c\u9057\u5fd8\u65e7\u6570\u636e\u7684\u80fd\u529b\u4e4b\u95f4\u627e\u5230\u4e00\u4e2a\u826f\u597d\u7684\u5e73\u8861\u3002", "conclusion": "\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0cUnlearnShield \u662f\u4e00\u79cd\u6709\u6548\u7684\u9632\u5fa1\u53cd\u5411\u5220\u9664\u653b\u51fb\u7684\u65b9\u6cd5\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.20379", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20379", "abs": "https://arxiv.org/abs/2601.20379", "authors": ["Zhengbo Jiao", "Hongyu Xian", "Qinglong Wang", "Yunpu Ma", "Zhebo Wang", "Zifan Zhang", "Dezhang Kong", "Meng Han"], "title": "Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution", "comment": "19 pages, 5 figures", "summary": "Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper's epistemology of \"conjectures and refutations,\" we argue that intelligence requires real-time evolution of the model's policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model's reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPolicy of Thoughts (PoT) \u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5b9e\u4f8b\u5185\u90e8\u4f18\u5316\u6765\u63d0\u5347\u6a21\u578b\u7684\u957f\u5468\u671f\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728LiveCodeBench\u6d4b\u8bd5\u4e2d\uff0c\u4e00\u4e2a\u5c0f\u5f97\u591a\u7684\u6a21\u578b\uff084B\uff09\u8868\u73b0\u51fa\u663e\u8457\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6267\u884c\u53cd\u9988\u6765\u8fc7\u6ee4\u6216\u91cd\u5199\u8f68\u8ff9\uff0c\u800c\u8fd9\u79cd\u53cd\u9988\u5e76\u672a\u88ab\u79ef\u6781\u5730\u96c6\u6210\u5230\u6a21\u578b\u7684\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u7684\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u53d7Popper\u7684\u77e5\u8bc6\u7406\u8bba\u542f\u53d1\uff0c\u7814\u7a76\u8ba4\u4e3a\u667a\u80fd\u9700\u8981\u901a\u8fc7\u4ece\u5931\u8d25\u4e2d\u5b66\u4e60\u4e0d\u65ad\u8c03\u6574\u6a21\u578b\u7684\u7b56\u7565\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aPoT\u7684\u6846\u67b6\uff0c\u57fa\u4e8e\u5728\u7ebf\u4f18\u5316\u8fc7\u7a0b\u3002\u5b83\u9996\u5148\u901a\u8fc7\u6709\u6548\u7684\u63a2\u7d22\u673a\u5236\u751f\u6210\u591a\u6837\u5316\u7684\u5019\u9009\u89e3\u51b3\u65b9\u6848\uff0c\u7136\u540e\u5229\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u6839\u636e\u6267\u884c\u53cd\u9988\u66f4\u65b0\u4e00\u4e2a\u4e34\u65f6\u7684LoRA\u9002\u914d\u5668\u3002\u8be5\u95ed\u73af\u8bbe\u8ba1\u80fd\u591f\u52a8\u6001\u5730\u9488\u5bf9\u5177\u4f53\u7684\u5b9e\u4f8b\u4f18\u5316\u6a21\u578b\u7684\u63a8\u7406\u5047\u8bbe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPoT\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002\u4e00\u4e2a4B\u7684\u6a21\u578b\u5728LiveCodeBench\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa49.71%\u7684\u51c6\u786e\u6027\uff0c\u8d85\u8d8a\u4e86GPT-4o\u548cDeepSeek-V3\uff0c\u5c3d\u7ba1\u540e\u8005\u6a21\u578b\u66f4\u5927\u3002", "conclusion": "PoT\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u957f\u5468\u671f\u63a8\u7406\u7684\u7a33\u5b9a\u6027\u4e0e\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u5c0f\u578b\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u66f4\u5927\u6f5c\u529b\u3002"}}
{"id": "2601.20467", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20467", "abs": "https://arxiv.org/abs/2601.20467", "authors": ["Zhenxuan Fan", "Jie Cao", "Yang Dai", "Zheqi Lv", "Wenqiao Zhang", "Zhongle Xie", "Peng LU", "Beng Chin Ooi"], "title": "CtrlCoT: Dual-Granularity Chain-of-Thought Compression for Controllable Reasoning", "comment": "16 pages, 9 figures, 11 tables", "summary": "Chain-of-thought (CoT) prompting improves LLM reasoning but incurs high latency and memory cost due to verbose traces, motivating CoT compression with preserved correctness. Existing methods either shorten CoTs at the semantic level, which is often conservative, or prune tokens aggressively, which can miss task-critical cues and degrade accuracy. Moreover, combining the two is non-trivial due to sequential dependency, task-agnostic pruning, and distribution mismatch. We propose \\textbf{CtrlCoT}, a dual-granularity CoT compression framework that harmonizes semantic abstraction and token-level pruning through three components: Hierarchical Reasoning Abstraction produces CoTs at multiple semantic granularities; Logic-Preserving Distillation trains a logic-aware pruner to retain indispensable reasoning cues (e.g., numbers and operators) across pruning ratios; and Distribution-Alignment Generation aligns compressed traces with fluent inference-time reasoning styles to avoid fragmentation. On MATH-500 with Qwen2.5-7B-Instruct, CtrlCoT uses 30.7\\% fewer tokens while achieving 7.6 percentage points higher than the strongest baseline, demonstrating more efficient and reliable reasoning. Our code will be publicly available at https://github.com/fanzhenxuan/Ctrl-CoT.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.20374", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.20374", "abs": "https://arxiv.org/abs/2601.20374", "authors": ["Sura Khalid Salsal", "Eman Shaker Mahmood", "Farah Tawfiq Abdul Hussien", "Maryam Mahdi Alhusseini", "Azhar Naji Alyahya", "Nikolai Safiullin"], "title": "A High-Performance Fractal Encryption Framework and Modern Innovations for Secure Image Transmission", "comment": null, "summary": "The current digital era, driven by growing threats to data security, requires a robust image encryption technique. Classical encryption algorithms suffer from a trade-off among security, image fidelity, and computational efficiency. This paper aims to enhance the performance and efficiency of image encryption. This is done by proposing Fractal encryption based on Fourier transforms as a new method of image encryption, leveraging state-of-the-art technology. The new approach considered here intends to enhance both security and efficiency in image encryption by comparing Fractal Encryption with basic methods. The suggested system also aims to optimise encryption/ decryption times and preserve image quality. This paper provides an introduction to Image Encryption using the fractal-based method, its mathematical formulation, and its comparative efficiency against publicly known traditional encryption methods. As a result, after filling the gaps identified in previous research, it has significantly improved both its encryption/decryption time and image fidelity compared to other techniques. In this paper, directions for future research and possible improvements are outlined for attention.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.20487", "categories": ["cs.AI", "cs.GT", "cs.HC", "econ.GN"], "pdf": "https://arxiv.org/pdf/2601.20487", "abs": "https://arxiv.org/abs/2601.20487", "authors": ["Nico Mutzner", "Taha Yasseri", "Heiko Rauhut"], "title": "Normative Equivalence in human-AI Cooperation: Behaviour, Not Identity, Drives Cooperation in Mixed-Agent Groups", "comment": null, "summary": "The introduction of artificial intelligence (AI) agents into human group settings raises essential questions about how these novel participants influence cooperative social norms. While previous studies on human-AI cooperation have primarily focused on dyadic interactions, little is known about how integrating AI agents affects the emergence and maintenance of cooperative norms in small groups. This study addresses this gap through an online experiment using a repeated four-player Public Goods Game (PGG). Each group consisted of three human participants and one bot, which was framed either as human or AI and followed one of three predefined decision strategies: unconditional cooperation, conditional cooperation, or free-riding. In our sample of 236 participants, we found that reciprocal group dynamics and behavioural inertia primarily drove cooperation. These normative mechanisms operated identically across conditions, resulting in cooperation levels that did not differ significantly between human and AI labels. Furthermore, we found no evidence of differences in norm persistence in a follow-up Prisoner's Dilemma, or in participants' normative perceptions. Participants' behaviour followed the same normative logic across human and AI conditions, indicating that cooperation depended on group behaviour rather than partner identity. This supports a pattern of normative equivalence, in which the mechanisms that sustain cooperation function similarly in mixed human-AI and all human groups. These findings suggest that cooperative norms are flexible enough to extend to artificial agents, blurring the boundary between humans and AI in collective decision-making.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.20378", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.20378", "abs": "https://arxiv.org/abs/2601.20378", "authors": ["Mario Perera", "Michael Mackay", "Max Hashem Eiza", "Alessandro Raschell\u00e0", "Nathan Shone", "Mukesh Kumar Maheshwari"], "title": "Towards Quantum-Safe O-RAN -- Experimental Evaluation of ML-KEM-Based IPsec on the E2 Interface", "comment": "Please note this is a draft and will be revised soon. However, all the simulations, experiments, and findings are final", "summary": "As Open Radio Access Network (O-RAN) deployments expand and adversaries adopt 'store-now, decrypt-later' strategies, operators need empirical data on the cost of migrating critical control interfaces to post-quantum cryptography (PQC). This paper experimentally evaluates the impact of integrating a NIST-aligned module-lattice KEM (ML-KEM, CRYSTALS-Kyber) into IKEv2/IPsec protecting the E2 interface between the 5G Node B (gNB) and the Near-Real-Time RAN Intelligent Controller (Near-RT RIC). Using an open-source testbed built from srsRAN, Open5GS, FlexRIC and strongSwan (with liboqs), we compare three configurations: no IPsec, classical ECDH-based IPsec, and ML-KEM-based IPsec. The study focuses on IPsec tunnel-setup latency and the runtime behaviour of Near-RT RIC xApps under realistic signalling workloads. Results from repeated, automated runs show that ML-KEM integration adds a small overhead to tunnel establishment, which is approximately 3~5 ms in comparison to classical IPsec, while xApp operation and RIC control loops remain stable in our experiments. These findings indicate that ML-KEM based IPsec on the E2 interface is practically feasible and inform quantum-safe migration strategies for O-RAN deployments.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.20539", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20539", "abs": "https://arxiv.org/abs/2601.20539", "authors": ["Oguzhan Gungordu", "Siheng Xiong", "Faramarz Fekri"], "title": "PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs", "comment": null, "summary": "Large Language Models (LLMs) have enabled automated heuristic design (AHD) for combinatorial optimization problems (COPs), but existing frameworks' reliance on fixed evolutionary rules and static prompt templates often leads to myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory. This approach allows the system to carry forward past decisions and reuse or avoid derivation information across generations. A policy agent plans evolutionary actions, a world model agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps, shifting LLM-based AHD from trial-and-error evolution toward state-aware planning through reasoning. Experiments across diverse COPs show that PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.20400", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.20400", "abs": "https://arxiv.org/abs/2601.20400", "authors": ["Jean-Guillaume Dumas", "Aude Maignan", "Luiza Soezima"], "title": "Fuzzy Private Set Union via Oblivious Key Homomorphic Encryption Retrieval", "comment": null, "summary": "Private Set Multi-Party Computations are protocols that allow parties to jointly and securely compute functions: apart from what is deducible from the output of the function, the input sets are kept private. Then, a Private Set Union (PSU), resp. Intersection (PSI), is a protocol that allows parties to jointly compute the union, resp. the intersection, between their private sets. Now a structured PSI, is a PSI where some structure of the sets can allow for more efficient protocols. For instance in Fuzzy PSI, elements only need to be close enough, instead of equal, to be part of the intersection. We present in this paper, Fuzzy PSU protocols (FPSU), able to efficiently take into account approximations in the union. For this, we introduce a new efficient sub-protocol, called Oblivious Key Homomorphic Encryption Retrieval (OKHER), improving on Oblivious Key-Value Retrieval (OKVR) techniques in our setting. In the fuzzy context, the receiver set $X=\\{x_i\\}_{1..n}$ is replaced by ${\\mathcal B}_\u03b4(X)$, the union of $n$ balls of dimension $d$ with radius $\u03b4$, centered at the $x_i$. The sender set is just its $m$ points of dimension $d$. Then the FPSU functionality corresponds to $X \\sqcup \\{y \\in Y, y \\notin {\\mathcal B}_\u03b4(X)\\}$. Thus, we formally define the FPSU functionality and security properties, and propose several protocols tuned to the patterns of the balls using the $l_\\infty$ distance. Using our OKHER routine and homomorphic encryption, we are for instance able to obtain a FPSU protocols with an asymptotic communication volume bound ranging from $O(dm\\log(\u03b4{n}))$ to $O(d^2m\\log(\u03b4^2n))$, depending on the receiver data set structure.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.20554", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20554", "abs": "https://arxiv.org/abs/2601.20554", "authors": ["Yaacov Pariente", "Vadim Indelman"], "title": "Online Risk-Averse Planning in POMDPs Using Iterated CVaR Value Function", "comment": null, "summary": "We study risk-sensitive planning under partial observability using the dynamic risk measure Iterated Conditional Value-at-Risk (ICVaR). A policy evaluation algorithm for ICVaR is developed with finite-time performance guarantees that do not depend on the cardinality of the action space. Building on this foundation, three widely used online planning algorithms--Sparse Sampling, Particle Filter Trees with Double Progressive Widening (PFT-DPW), and Partially Observable Monte Carlo Planning with Observation Widening (POMCPOW)--are extended to optimize the ICVaR value function rather than the expectation of the return. Our formulations introduce a risk parameter $\u03b1$, where $\u03b1= 1$ recovers standard expectation-based planning and $\u03b1< 1$ induces increasing risk aversion. For ICVaR Sparse Sampling, we establish finite-time performance guarantees under the risk-sensitive objective, which further enable a novel exploration strategy tailored to ICVaR. Experiments on benchmark POMDP domains demonstrate that the proposed ICVaR planners achieve lower tail risk compared to their risk-neutral counterparts.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.20507", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.20507", "abs": "https://arxiv.org/abs/2601.20507", "authors": ["Philipp Mao", "Li Shi", "Marcel Busch", "Mathias Payer"], "title": "T\u00c4MU: Emulating Trusted Applications at the (GlobalPlatform)-API Layer", "comment": null, "summary": "Mobile devices rely on Trusted Execution Environments (TEEs) to execute security-critical code and protect sensitive assets. This security-critical code is modularized in components known as Trusted Applications (TAs). Vulnerabilities in TAs can compromise the TEE and, thus, the entire system. However, the closed-source nature and fragmentation of mobile TEEs severely hinder dynamic analysis of TAs, limiting testing efforts to mostly static analyses. This paper presents T\u00c4MU, a rehosting platform enabling dynamic analysis of TAs, specifically fuzzing and debugging, by interposing their execution at the API layer. To scale to many TAs across different TEEs, T\u00c4MU leverages the standardization of TEE APIs, driven by the GlobalPlatform specifications. For the remaining TEE-specific APIs not shared across different TEEs, T\u00c4MU introduces the notion of greedy high-level emulation, a technique that allows prioritizing manual rehosting efforts based on the potential coverage gain during fuzzing. We implement T\u00c4MU and use it to emulate 67 TAs across four TEEs. Our fuzzing campaigns yielded 17 zero-day vulnerabilities across 11 TAs. These results indicate a deficit of dynamic analysis capabilities across the TEE ecosystem, where not even vendors with source code unlocked these capabilities for themselves. T\u00c4MU promises to close this gap by bringing effective and practical dynamic analysis to the mobile TEE domain.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.20604", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20604", "abs": "https://arxiv.org/abs/2601.20604", "authors": ["Gray Cox"], "title": "Dialogical Reasoning Across AI Architectures: A Multi-Model Framework for Testing AI Alignment Strategies", "comment": "23 pages, 5 tables, 5 appendices. Code and data: https://github.com/jgraycox-coa/vcw-multi-ai-dialogue", "summary": "This paper introduces a methodological framework for empirically testing AI alignment strategies through structured multi-model dialogue. Drawing on Peace Studies traditions - particularly interest-based negotiation, conflict transformation, and commons governance - we operationalize Viral Collaborative Wisdom (VCW), an approach that reframes alignment from a control problem to a relationship problem developed through dialogical reasoning.\n  Our experimental design assigns four distinct roles (Proposer, Responder, Monitor, Translator) to different AI systems across six conditions, testing whether current large language models can engage substantively with complex alignment frameworks. Using Claude, Gemini, and GPT-4o, we conducted 72 dialogue turns totaling 576,822 characters of structured exchange.\n  Results demonstrate that AI systems can engage meaningfully with Peace Studies concepts, surface complementary objections from different architectural perspectives, and generate emergent insights not present in initial framings - including the novel synthesis of \"VCW as transitional framework.\" Cross-architecture patterns reveal that different models foreground different concerns: Claude emphasized verification challenges, Gemini focused on bias and scalability, and GPT-4o highlighted implementation barriers.\n  The framework provides researchers with replicable methods for stress-testing alignment proposals before implementation, while the findings offer preliminary evidence about AI capacity for the kind of dialogical reasoning VCW proposes. We discuss limitations, including the observation that dialogues engaged more with process elements than with foundational claims about AI nature, and outline directions for future research including human-AI hybrid protocols and extended dialogue studies.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.20548", "categories": ["cs.CR", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.20548", "abs": "https://arxiv.org/abs/2601.20548", "authors": ["Kahraman Kostas", "Rabia Yasa Kostas"], "title": "IoT Device Identification with Machine Learning: Common Pitfalls and Best Practices", "comment": "4 pages", "summary": "This paper critically examines the device identification process using machine learning, addressing common pitfalls in existing literature. We analyze the trade-offs between identification methods (unique vs. class based), data heterogeneity, feature extraction challenges, and evaluation metrics. By highlighting specific errors, such as improper data augmentation and misleading session identifiers, we provide a robust guideline for researchers to enhance the reproducibility and generalizability of IoT security models.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.20614", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20614", "abs": "https://arxiv.org/abs/2601.20614", "authors": ["Yanqi Dai", "Yuxiang Ji", "Xiao Zhang", "Yong Wang", "Xiangxiang Chu", "Zhiwu Lu"], "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation", "comment": "Accepted for ICLR 2026", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.20629", "categories": ["cs.CR", "cs.ET", "cs.OS"], "pdf": "https://arxiv.org/pdf/2601.20629", "abs": "https://arxiv.org/abs/2601.20629", "authors": ["Aditya Mitra", "Hamza Haroon", "Amaan Rais Shah", "Mohammad Elham Rasooli", "Bogdan Itsam Dorantes Nikolaev", "Tu\u011f\u00e7e Ball\u0131"], "title": "/dev/SDB: Software Defined Boot -- A novel standard for diskless booting anywhere and everywhere", "comment": null, "summary": "A computer is nothing but a device that processes the instructions supplied to it. However, as computers evolved, the instructions or codes started to be more complicated. As computers started to be used by non-technical people, it became imperative that the users be able to use the machine without having underlying knowledge of the code or the hardware. And operating system became the backbone for translating the inputs from the user to actual operation on the hardware. With the increasing complexity and the choices of operating system, it became clear that different groups of people, especially in an enterprise scenario, required different operating systems. Installing them all on a single machine, for shared computers became a difficult task, giving rise to network-based booting. But network-based booting was confined to only wired connectivity, keeping it restricted to very small geographical areas. The proposed system, /dev/SDB, is aimed at creating a standard where any user, anyone on the globe, can access the operating system authorized to them without having to be on the corporate network. It aims to offer the same over Wi-Fi as well as cellular connectivity, ensuring employees can truly work from anywhere, while following the policies for operating systems and without redundant hardware.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.20641", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20641", "abs": "https://arxiv.org/abs/2601.20641", "authors": ["Boaz Carmeli", "Orr Paradise", "Shafi Goldwasser", "Yonatan Belinkov", "Ron Meir"], "title": "Investigating the Development of Task-Oriented Communication in Vision-Language Models", "comment": null, "summary": "We investigate whether \\emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.20638", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.20638", "abs": "https://arxiv.org/abs/2601.20638", "authors": ["David Schmidt", "Sebastian Schrittwieser", "Edgar Weippl"], "title": "Supply Chain Insecurity: Exposing Vulnerabilities in iOS Dependency Management Systems", "comment": null, "summary": "Dependency management systems are a critical component in software development, enabling projects to incorporate existing functionality efficiently. However, misconfigurations and malicious actors in these systems pose severe security risks, leading to supply chain attacks. Despite the widespread use of smartphone apps, the security of dependency management systems in the iOS software supply chain has received limited attention. In this paper, we focus on CocoaPods, one of the most widely used dependency management systems for iOS app development, but also examine the security of Carthage and Swift Package Manager (SwiftPM). We demonstrate that iOS apps expose internal package names and versions. Attackers can exploit this leakage to register previously unclaimed dependencies in CocoaPods, enabling remote code execution (RCE) on developer machines and build servers. Additionally, we show that attackers can compromise dependencies by reclaiming abandoned domains and GitHub URLs. Analyzing a dataset of 9,212 apps, we quantify how many apps are susceptible to these vulnerabilities. Further, we inspect the use of vulnerable dependencies within public GitHub repositories. Our findings reveal that popular apps disclose internal dependency information, enabling dependency confusion attacks. Furthermore, we show that hijacking a single CocoaPod library through an abandoned domain could compromise 63 iOS apps, affecting millions of users. Finally, we compare iOS dependency management systems with Cargo, Go modules, Maven, npm, and pip to discuss mitigation strategies for the identified threats.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.20716", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.20716", "abs": "https://arxiv.org/abs/2601.20716", "authors": ["Abylay Satybaldy", "Kamil Tylinski", "Jiahua Xu"], "title": "Decentralized Identity in Practice: Benchmarking Latency, Cost, and Privacy", "comment": null, "summary": "Decentralized Identifiers (DIDs) are increasingly deployed on distributed ledgers, yet systematic cross-platform evidence on their operational behavior remains limited. We present an empirical benchmarking study of three prominent ledger-based DID methods - Ethereum, Hedera, and XRP Ledger - using reference Software Development Kits (SDKs) under a unified experimental setup. We measure latency, transaction cost, and on-chain metadata exposure, normalizing latency by each platform's block or consensus interval and cost by its native value transfer fee. Privacy leakage is quantified using a Metadata-Leakage Score (MLS), an entropy-based measure expressed in bits per operation.\n  Our results reveal distinct architectural trade-offs. Ethereum enables near-instant, off-chain DID creation, but incurs the highest latency and cost for on-chain lifecycle operations. XRPL delivers deterministic and stable latency with fixed, low fees, yet exhibits higher metadata leakage due to more verbose transaction payloads. Hedera achieves the lowest on-chain latency and low fees with minimal metadata leakage, while occasional variance arises from SDK-side processing and confirmation pipelines.\n  Overall, the findings show that ledger architecture and SDK workflows play a major role in shaping DID latency, cost, and metadata exposure, complementing the effects of the underlying consensus mechanism. These results provide evidence-based insights to support informed selection and configuration of DID systems under performance and privacy constraints.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.20735", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.20735", "abs": "https://arxiv.org/abs/2601.20735", "authors": ["Arvid Becker", "Pedro Cabalar", "Martin Di\u00e9guez", "Susana Hahn", "Javier Romero", "Torsten Schaub"], "title": "Implementing Metric Temporal Answer Set Programming", "comment": null, "summary": "We develop a computational approach to Metric Answer Set Programming (ASP) to allow for expressing quantitative temporal constraints, like durations and deadlines. A central challenge is to maintain scalability when dealing with fine-grained timing constraints, which can significantly exacerbate ASP's grounding bottleneck. To address this issue, we leverage extensions of ASP with difference constraints, a simplified form of linear constraints, to handle time-related aspects externally. Our approach effectively decouples metric ASP from the granularity of time, resulting in a solution that is unaffected by time precision.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.20856", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20856", "abs": "https://arxiv.org/abs/2601.20856", "authors": ["Sebastiano Monti", "Carlo Nicolini", "Gianni Pellegrini", "Jacopo Staiano", "Bruno Lepri"], "title": "SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models", "comment": null, "summary": "Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
