<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 6]
- [cs.CR](#cs.CR) [Total: 9]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge](https://arxiv.org/abs/2602.17826)
*Marcelo Labre*

Main category: cs.AI

TL;DR: 通过结合形式领域本体，研究者提出了一种神经符号管道，旨在提高语言模型在需要可验证推理的专业领域中的可靠性。虽然相关领域的知识能够改善模型性能，但不相关的背景信息会损害其表现。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型存在幻觉、脆弱性和缺乏形式基础等根本限制，在高风险的专业领域中尤为突出，这些领域需要可验证的推理。研究者旨在探讨是否可以通过引入正式领域的本体来增强语言模型的可靠性。

Method: 研究者利用数学领域作为概念验证场景，设计了一种结合检索增强生成和跨编码器重排序的神经符号管道，该管道利用了OpenMath本体，以注入与之相关的定义至模型提示中。

Result: 在MATH基准测试中，使用这项方法改进上下文性能在高质量检索时有所提升，但在无关背景下则效果明显下降。

Conclusion: 研究结果表明，本体导向的上下文有助于提高神经符号模型的性能，但也揭示了这种方法面临的挑战。

Abstract: Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.

</details>


### [2] [The Token Games: Evaluating Language Model Reasoning with Puzzle Duels](https://arxiv.org/abs/2602.17831)
*Simon Henniger,Gabriel Poesia*

Main category: cs.AI

TL;DR: 该研究提出了The Token Games (TTG) 框架，让大语言模型通过相互挑战来创建和解决问题，以此评估它们的推理能力。研究展示了10种前沿模型的表现，并发现即使是这些模型，创建好的谜题仍然是一个极具挑战的任务。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试使用复杂的领域知识来挑战模型的推理能力，但这些方法可能存在训练中遇到过类似问题的偏见。研究者借鉴历史上的数学对决，创造了一个新的评价框架，旨在更全面地测试模型的推理能力。

Method: 研究者设计了一个基于编程谜题的挑战机制，模型通过创建和解决这些谜题来进行相互挑战。谜题的形式为给定一个接收布尔值的Python函数，找到使其返回True的输入。研究使用Elo评级体系来比较模型之间的相对表现。

Result: 研究在TTG框架下测试了10种前沿模型，结果显示这些模型的相对排名与现有基准一致。同时，研究还表明，即使是这些先进模型，创造高质量谜题也仍然是一项挑战。

Conclusion: 研究提出的新框架TTG提供了一个新的评估模型推理能力的方法，不仅测量了解决问题的能力，还考察了模型的创造力和任务生成的能力。这种方法有望减少对人工设计谜题的依赖。

Abstract: Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.

</details>


### [3] [WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics](https://arxiv.org/abs/2602.17990)
*Madhav Kanda,Pedro Las-Casas,Alok Gautam Kumbhare,Rodrigo Fonseca,Sharad Agarwal*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.

</details>


### [4] [Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets](https://arxiv.org/abs/2602.18025)
*Haruki Abe,Takayuki Osa,Yusuke Mukuta,Tatsuya Harada*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.

</details>


### [5] [SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps](https://arxiv.org/abs/2602.18201)
*Joseph Bingham,Netanel Arussy,Dvir Aran*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime

</details>


### [6] [Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies](https://arxiv.org/abs/2602.18291)
*Zhuoran Li,Hai Zhong,Xun Wang,Qingxin Xia,Lihua Zhang,Longbo Huang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \underline{O}nline off-policy \underline{MA}RL framework using \underline{D}iffusion policies (\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\times$ to $5\times$ improvement in sample efficiency.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [7] [Exploiting Liquidity Exhaustion Attacks in Intent-Based Cross-Chain Bridges](https://arxiv.org/abs/2602.17805)
*André Augusto,Christof Ferreira Torres,André Vasconcelos,Miguel Correia*

Main category: cs.CR

TL;DR: 该研究分析了跨链订单填充协议中的流动性耗尽攻击，并提出了一种基于参数的重播攻击模拟框架，揭示了不同协议在不同攻击条件下的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 为了揭示增量链桥带来的新型系统性风险，如流动性集中和结算延迟，并评估它们在攻击下的表现。

Method: 通过模拟各种攻击，分析大规模的真实订单数据，包括计算攻击成本和评估协议安全性。

Result: 研究发现，具有较高解算商利润的协议在当前参数下容易遭受流动性耗尽攻击；Across协议表现出较高安全性，而Mayan Swift在压力测试中变得脆弱。同时，研究展示了在拜占庭攻击下，所有协议都可能遭受广泛可用性损失，导致大量失败的跨链订单。

Conclusion: 提出了优化的攻击策略，利用数据模式降低成本，降低流动性耗尽攻击的门槛。

Abstract: Intent-based cross-chain bridges have emerged as an alternative to traditional interoperability protocols by allowing off-chain entities (\emph{solvers}) to immediately fulfill users' orders by fronting their own liquidity. While improving user experience, this approach introduces new systemic risks, such as solver liquidity concentration and delayed settlement. In this paper, we propose a new class of attacks called \emph{liquidity exhaustion attacks} and a replay-based parameterized attack simulation framework. We analyze 3.5 million cross-chain intents that moved \$9.24B worth of tokens between June and November 2025 across three major protocols (Mayan Swift, Across, and deBridge), spanning nine blockchains.
  For rational attackers, our results show that protocols with higher solver profitability, such as deBridge, are vulnerable under current parameters: 210 historical attack instances yield a mean net profit of \$286.14, with 80.5\% of attacks profitable. In contrast, Across remains robust in all tested configurations due to low solver margins and very high liquidity, while Mayan Swift is generally secure but becomes vulnerable under stress-test conditions. Under byzantine attacks, we show that it is possible to suppress availability across all protocols, causing dozens of failed intents and solver profit losses of up to \$978 roughly every 16 minutes. Finally, we propose an optimized attack strategy that exploits patterns in the data to reduce attack costs by up to 90.5\% compared to the baseline, lowering the barrier to liquidity exhaustion attacks.

</details>


### [8] [Symfrog-512: High-Capacity Sponge-Based AEAD Cipher (1024-bit State)](https://arxiv.org/abs/2602.17900)
*Victor Duarte Melo*

Main category: cs.CR

TL;DR: 本提交附带了完整的参考实现、确定性测试向量和可重复的基准测试套件，所有源代码和构建说明均已在项目仓库中公开。性能数据在特定硬件和编译器设置下由内置基准工具生成。项目遵循MIT许可证发布，并欢迎外部加密分析、反馈及重现已行性检查。


<details>
  <summary>Details</summary>
Motivation: 本文的主要动机是提供一个完整的参考实现，以及确保结果的可验证性和重现实验性，同时明确声明安全断言仅限于理想置换模型，并强调在未提供正式防侧信道证明的情况下，旨在实现秘密依赖操作的常量时间行为。

Method: 该方法包括完整的代码实现、测试向量和基准测试工具。源代码、构建指南和回归数据公开在项目仓库中。性能数据通过内建的基准工具在特定硬件和编译器设置下生成。

Result: 结果展示了AEAD构造的完整规范，包括域分离、速率和容量选择、标签生成和参考命令行界面的文件格式。性能数据反映了在给定硬件和编译器配置下的具体表现。安全性声明严格限定在理想置换模型范围内，并未做出更强的理论保证。

Conclusion: 结论是该项目的实现目标是秘密依赖操作的时间常量行为，尽管未提供正式的侧信道安全证明。项目以MIT许可证开放，并鼓励外部的加密分析和反馈。

Abstract: This submission includes a complete reference implementation together with deterministic test vectors and a reproducible benchmark suite. All source code, build instructions, and regression artifacts are publicly available in the project repository, enabling independent verification and reimplementation of the scheme. The AEAD construction is fully specified, including domain separation, rate and capacity choices, tag generation, and the exact file format used by the reference CLI. Reported performance numbers are produced by the built in benchmark tool under documented hardware and compiler settings. All security claims are made strictly within the ideal permutation model following standard sponge and duplex bounds, and no stronger guarantees are asserted for the concrete permutation beyond the documented analysis and empirical behavior. The implementation aims for constant time behavior with respect to secret dependent operations, although no formal side channel proof is provided. The project is released under the MIT license, and external cryptanalysis, feedback, and reproducibility checks are explicitly encouraged.

</details>


### [9] [Dynamic Deception: When Pedestrians Team Up to Fool Autonomous Cars](https://arxiv.org/abs/2602.18079)
*Masoud Jamshidiyan Tehrani,Marco Gabriel,Jinhan Kim,Paolo Tonella*

Main category: cs.CR

TL;DR: 此研究通过在模拟器中使用动态协作的多个目标（例如两名行人）来提升针对自动驾驶感知模型的攻击效果，从而首次揭示了模型级鲁棒性与端到端安全性之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶感知模型的许多对抗攻击在实际应用中无法导致整体系统崩溃，主要是因为这些攻击在现实系统中通常会因为车辆的动力学效应而变得空间或时间上很短暂，因此很少影响车辆行为。

Method: 研究通过在CARLA模拟器中使用一个包含两个行人携带含有对抗模式的衣物并协同行动的新方法，以验证针对自动驾驶系统的攻击效果。

Result: 单一行人攻击在所有试验（10次）中均未成功，而两名行人的动态协作攻击导致车辆完全停止的比例达到50%，静止协作的目标则没有成功攻击，研究成果表明了持久的对抗信号通过协调行为得到增强后才能引发整个系统的失败。

Conclusion: 研究揭示了自动驾驶在模型层面对抗攻击的稳健性与端到端安全性能之间的差距，强调了针对动态协作攻击的安全性问题需要进一步研究。

Abstract: Many adversarial attacks on autonomous-driving perception models fail to cause system-level failures once deployed in a full driving stack. The main reason for such ineffectiveness is that once deployed in a system (e.g., within a simulator), attacks tend to be spatially or temporally short-lived, due to the vehicle's dynamics, hence rarely influencing the vehicle behaviour. In this paper, we address both limitations by introducing a system-level attack in which multiple dynamic elements (e.g., two pedestrians) carry adversarial patches (e.g., on cloths) and jointly amplify their effect through coordination and motion. We evaluate our attacks in the CARLA simulator using a state-of-the-art autonomous driving agent. At the system level, single-pedestrian attacks fail in all runs (out of 10), while dynamic collusion by two pedestrians induces full vehicle stops in up to 50\% of runs, with static collusion yielding no successful attack at all. These results show that system-level failures arise only when adversarial signals persist over time and are amplified through coordinated actors, exposing a gap between model-level robustness and end-to-end safety.

</details>


### [10] [AndroWasm: an Empirical Study on Android Malware Obfuscation through WebAssembly](https://arxiv.org/abs/2602.18082)
*Diego Soi,Silvia Lucia Sanna,Lorenzo Pisu,Leonardo Regano,Giorgio Giacinto*

Main category: cs.CR

TL;DR: 本文研究了WebAssembly作为一种新型技术，用于隐藏恶意载荷和逃避传统静态分析和签名匹配机制，提供PoC展示攻击者如何通过将恶意功能嵌入并执行来绕过工业级最先进的工具，如VirusTotal和MobSF。


<details>
  <summary>Details</summary>
Motivation: 虽然WebAssembly（Wasm）通常用于渲染特定的娱乐活动并在浏览器中与原生组件交互，但本文深入分析了其被Android恶意软件利用以包括Wasm模块在内的执行管道中的机制。鉴于当前反病毒工具在恶意软件检测方面的不足，现有技术无法完全识别Wasm包装的恶意代码，因此提出新的威胁模型以展示如何隐藏并执行恶意功能。

Method: 通过对WebAssembly和Android恶意软件技术的深入分析，结合实例证明攻击者如何利用WebAssembly的特性进行恶意行为。

Result: 本文揭示了WebAssembly如何被用于隐藏恶意代码执行，提出了一个威胁模型以表明现有检测技术可能无法识别这些隐藏的恶意代码。

Conclusion: 研究结果强调了研究新的分析技术来检测WebAssembly中潜在恶意行为的重要性，为抵御高级持续的威胁提供了新的见解。

Abstract: In recent years, stealthy Android malware has increasingly adopted sophisticated techniques to bypass automatic detection mechanisms and harden manual analysis. Adversaries typically rely on obfuscation, anti-repacking, steganography, poisoning, and evasion techniques to AI-based tools, and in-memory execution to conceal malicious functionality.
  In this paper, we investigate WebAssembly (Wasm) as a novel technique for hiding malicious payloads and evading traditional static analysis and signature-matching mechanisms. While Wasm is typically employed to render specific gaming activities and interact with the native components in web browsers, we provide an in-depth analysis on the mechanisms Android may employ to include Wasm modules in its execution pipeline. Additionally, we provide Proofs-of-Concept to demonstrate a threat model in which an attacker embeds and executes malicious routines, effectively bypassing IoC detection by industrial state-of-the-art tools, like VirusTotal and MobSF.

</details>


### [11] [Many Tools, Few Exploitable Vulnerabilities: A Survey of 246 Static Code Analyzers for Security](https://arxiv.org/abs/2602.18270)
*Kevin Hermann,Sven Peldszus,Thorsten Berger*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Static security analysis is a widely used technique for detecting software vulnerabilities across a wide range of weaknesses, application domains, and programming languages. While prior work surveyed static analyzes for specific weaknesses or application domains, no overview of the entire security landscape exists. We present a systematic literature review of 246 static security analyzers concerning their targeted vulnerabilities, application domains, analysis techniques, evaluation methods, and limitations. We observe that most analyzers focus on a limited set of weaknesses, that the vulnerabilities they detect are rarely exploitable, and that evaluations use custom benchmarks that are too small to enable robust assessment.

</details>


### [12] [Detecting PowerShell-based Fileless Cryptojacking Attacks Using Machine Learning](https://arxiv.org/abs/2602.18285)
*Said Varlioglu,Nelly Elsayed,Murat Ozer,Zag ElSayed,John M. Emmert*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the emergence of remote code execution (RCE) vulnerabilities in ubiquitous libraries and advanced social engineering techniques, threat actors have started conducting widespread fileless cryptojacking attacks. These attacks have become effective with stealthy techniques based on PowerShell-based exploitation in Windows OS environments. Even if attacks are detected and malicious scripts removed, processes may remain operational on victim endpoints, creating a significant challenge for detection mechanisms. In this paper, we conducted an experimental study with a collected dataset on detecting PowerShell-based fileless cryptojacking scripts. The results showed that Abstract Syntax Tree (AST)-based fine-tuned CodeBERT achieved a high recall rate, proving the importance of the use of AST integration and fine-tuned pre-trained models for programming language.

</details>


### [13] [FeatureBleed: Inferring Private Enriched Attributes From Sparsity-Optimized AI Accelerators](https://arxiv.org/abs/2602.18304)
*Darsh Asher,Farshad Dizani,Joshua Kalyanapu,Rosario Cammarota,Aydin Aysu,Samira Mirbagher Ajorpaz*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Backend enrichment is now widely deployed in sensitive domains such as product recommendation pipelines, healthcare, and finance, where models are trained on confidential data and retrieve private features whose values influence inference behavior while remaining hidden from the API caller. This paper presents the first hardware-level backend retrieval data-stealing attack, showing that accelerator optimizations designed for performance can directly undermine data confidentiality and bypass state-of-the-art privacy defenses.
  Our attack, FEATUREBLEED, exploits zero-skipping in AI accelerators to infer private backend-retrieved features solely through end-to-end timing, without relying on power analysis, DVFS manipulation, or shared-cache side channels. We evaluate FEATUREBLEED on three datasets spanning medical and non-medical domains: Texas-100X (clinical records), OrganAMNIST (medical imaging), and Census-19 (socioeconomic data). We further evaluate FEATUREBLEED across three hardware backends (Intel AVX, Intel AMX, and NVIDIA A100) and three model architectures (DNNs, CNNs, and hybrid CNN-MLP pipelines), demonstrating that the leakage generalizes across CPU and GPU accelerators, data modalities, and application domains, with an adversarial advantage of up to 98.87 percentage points.
  Finally, we identify the root cause of the leakage as sparsity-driven zero-skipping in modern hardware. We quantify the privacy-performance-power trade-off: disabling zero-skipping increases Intel AMX per-operation energy by up to 25 percent and incurs 100 percent performance overhead. We propose a padding-based defense that masks timing leakage by equalizing responses to the worst-case execution time, achieving protection with only 7.24 percent average performance overhead and no additional power cost.

</details>


### [14] [Drawing the LINE: Cryptographic Analysis and Security Improvements for the LINE E2EE Protocol](https://arxiv.org/abs/2602.18370)
*Benjamin Dowling,Prosanta Gope,Mehr U Nisa,Bhagya Wimalasiri*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: LINE has emerged as one of the most popular communication platforms in many East Asian countries, including Thailand and Japan, with millions of active users. Therefore, it is essential to understand its security guarantees. In this work, we present the first provable security analysis of the LINE version two (LINEv2) messaging protocol, focusing on its cryptographic guarantees in a real-world setting. We capture the architecture and security of the LINE messaging protocol by modifying the Multi-Stage Key Exchange (MSKE) model, a framework for analysing cryptographic protocols under adversarial conditions. While LINEv2 achieves basic security properties such as key indistinguishability and message authentication, we highlight the lack of forward secrecy (FS) and post-compromise security (PCS). To address this, we introduce a stronger version of the LINE protocol, introducing FS and PCS to LINE, analysing and benchmarking our results.

</details>


### [15] [Trojans in Artificial Intelligence (TrojAI) Final Report](https://arxiv.org/abs/2602.07152)
*Kristopher W. Reese,Taylor Kulp-McDowall,Michael Majurski,Tim Blattner,Derek Juba,Peter Bajcsy,Antonio Cardone,Philippe Dessauw,Alden Dima,Anthony J. Kearsley,Melinda Kleczynski,Joel Vasanth,Walid Keyrouz,Chace Ashcraft,Neil Fendley,Ted Staley,Trevor Stout,Josh Carney,Greg Canal,Will Redman,Aurora Schmidt,Cameron Hickert,William Paul,Jared Markowitz,Nathan Drenkow,David Shriver,Marissa Connor,Keltin Grimes,Marco Christiani,Hayden Moore,Jordan Widjaja,Kasimir Gabert,Uma Balakrishnan,Satyanadh Gundimada,John Jacobellis,Sandya Lakkur,Vitus Leung,Jon Roose,Casey Battaglino,Farinaz Koushanfar,Greg Fields,Xihe Gu,Yaman Jandali,Xinqiao Zhang,Akash Vartak,Tim Oates,Ben Erichson,Michael Mahoney,Rauf Izmailov,Xiangyu Zhang,Guangyu Shen,Siyuan Cheng,Shiqing Ma,XiaoFeng Wang,Haixu Tang,Di Tang,Xiaoyi Chen,Zihao Wang,Rui Zhu,Susmit Jha,Xiao Lin,Manoj Acharya,Wenchao Li,Chao Chen*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Intelligence Advanced Research Projects Activity (IARPA) launched the TrojAI program to confront an emerging vulnerability in modern artificial intelligence: the threat of AI Trojans. These AI trojans are malicious, hidden backdoors intentionally embedded within an AI model that can cause a system to fail in unexpected ways, or allow a malicious actor to hijack the AI model at will. This multi-year initiative helped to map out the complex nature of the threat, pioneered foundational detection methods, and identified unsolved challenges that require ongoing attention by the burgeoning AI security field. This report synthesizes the program's key findings, including methodologies for detection through weight analysis and trigger inversion, as well as approaches for mitigating Trojan risks in deployed models. Comprehensive test and evaluation results highlight detector performance, sensitivity, and the prevalence of "natural" Trojans. The report concludes with lessons learned and recommendations for advancing AI security research.

</details>
